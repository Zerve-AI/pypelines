{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Zerve-AI/pypelines.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the pypeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = ''\n",
    "os.chdir(f'{folder}/pypelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST OF MODELS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR CLASSIFICATION PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils\n",
    "\n",
    "\n",
    "utils.list_supported_models(model_type='classification')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE CLASSIFICATION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"pypelines/datasets/classification/titanic.csv\")\n",
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Decision Tree Classifier']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_pypelines_all.model_grid_search_settings(model_name='Decision Tree Classifier'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "    'numerical': [\n",
    "        {'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
    "        {'search': True, 'name': 'min_samples_split', 'min': 2, 'max': 10, 'step': 2},\n",
    "        {'search': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 5},\n",
    "        {'search': True, 'name': 'min_weight_fraction_leaf', 'min': 0.0, 'max': 0.5, 'step': 0.25},\n",
    "        {'search': True, 'name': 'max_leaf_nodes', 'min': 1, 'max': 10, 'step': 5},\n",
    "        {'search': True, 'name': 'min_impurity_decrease', 'min': 0.0, 'max': 0.5, 'step': 0.25}\n",
    "    ],\n",
    "    'categorical': [\n",
    "        {'search': False, 'name': 'criterion', 'selected': ['gini'], 'values': ['gini', 'entropy']},\n",
    "        {'search': False, 'name': 'splitter', 'selected': ['best'], 'values': ['best', 'random']},\n",
    "        {'search': False, 'name': 'max_features', 'selected': ['auto'], 'values': ['auto', 'sqrt', 'log2']},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(clf_pypelines_all.set_model_grid_search_settings(hyperparam_dict=hyperparameter, model_name='Decision Tree Classifier'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Decision Tree Classifier #####\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "dt_classifier_param_grid = {\n",
    "\"dt_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_split\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_leaf\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_weight_fraction_leaf\": np.arange(0.0, 0.5, 0.25),\n",
    "\"dt_classifier__max_leaf_nodes\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_impurity_decrease\": np.arange(0.0, 0.5, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "dt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "dt_classifier_grid_search = GridSearchCV(estimator=dt_classifier_pipe, param_grid=dt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "dt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "dt_classifier_best_estimator = dt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "dt_classifier_search_results = pd.DataFrame(dt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "dt_classifier_predictions = pd.DataFrame(dt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "dt_classifier_predictions_prob = dt_classifier_best_estimator.predict_proba(X_test)\n",
    "dt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[0]] = dt_classifier_predictions_prob[:,0]\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]] = dt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "dt_classifier_accuracy = accuracy_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_f1_score = f1_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_precision = precision_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_recall = recall_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_roc_auc_score = roc_auc_score(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "dt_classifier_performance_metrics = [['dt_classifier','accuracy',dt_classifier_accuracy], \n",
    "                                  ['dt_classifier','f1_score',dt_classifier_f1_score],\n",
    "                                  ['dt_classifier','precision', dt_classifier_precision],\n",
    "                                  ['dt_classifier','recall', dt_classifier_recall],\n",
    "                                  ['dt_classifier','roc_auc_score', dt_classifier_roc_auc_score]]\n",
    "dt_classifier_performance_metrics = pd.DataFrame(dt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "dt_classifier_roc_auc_plot, dt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "dt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "dt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "dt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_title(f'dt_classifier ROC Curve')\n",
    "# Add legend\n",
    "dt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(dt_classifier_performance_metrics[dt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = dt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "dt_classifier_lift_plot, dt_classifier_lift_plot_ax = plt.subplots()\n",
    "dt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "dt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "dt_classifier_lift_plot_ax.set_title(f'dt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "dt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "dt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(dt_classifier_performance_metrics)##### End of Model Pipeline for Decision Tree Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE CLASSIFICATION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output\n",
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Logistic Regression', 'SVC Classifier']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Logistic Regression #####\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "log_reg_param_grid = {\n",
    "\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
    "\n",
    "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
    "log_reg_predictions_prob_df = pd.DataFrame()\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
    "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
    "                                  ['log_reg','precision', log_reg_precision],\n",
    "                                  ['log_reg','recall', log_reg_recall],\n",
    "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
    "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "log_reg_roc_auc_plot, log_reg_roc_auc_plot_ax = plt.subplots()\n",
    "log_reg_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "log_reg_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "log_reg_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_title(f'log_reg ROC Curve')\n",
    "# Add legend\n",
    "log_reg_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(log_reg_performance_metrics[log_reg_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = log_reg_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "log_reg_lift_plot, log_reg_lift_plot_ax = plt.subplots()\n",
    "log_reg_lift_plot_ax.set_xlabel('Proportion')\n",
    "log_reg_lift_plot_ax.set_ylabel('Lift')\n",
    "log_reg_lift_plot_ax.set_title(f'log_reg Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "log_reg_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "log_reg_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(log_reg_performance_metrics)##### End of Model Pipeline for Logistic Regression #####\n",
    "##### Model Pipeline for SVC Classifier #####\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "svc_classifier_param_grid = {\n",
    "\"svc_classifier__C\": np.arange(0.1, 1.0, 0.2),\n",
    "\"svc_classifier__degree\": np.arange(2, 5, 1),\n",
    "\"svc_classifier__kernel\": ['linear'],\n",
    "\"svc_classifier__probability\": [True],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "svc_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc_classifier', SVC())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "svc_classifier_grid_search = GridSearchCV(estimator=svc_classifier_pipe, param_grid=svc_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "svc_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "svc_classifier_best_estimator = svc_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "svc_classifier_search_results = pd.DataFrame(svc_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "svc_classifier_predictions = pd.DataFrame(svc_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "svc_classifier_predictions_prob = svc_classifier_best_estimator.predict_proba(X_test)\n",
    "svc_classifier_predictions_prob_df = pd.DataFrame()\n",
    "svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[0]] = svc_classifier_predictions_prob[:,0]\n",
    "svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]] = svc_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "svc_classifier_accuracy = accuracy_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_f1_score = f1_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_precision = precision_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_recall = recall_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_roc_auc_score = roc_auc_score(y_test, svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]])\n",
    "svc_classifier_performance_metrics = [['svc_classifier','accuracy',svc_classifier_accuracy], \n",
    "                                  ['svc_classifier','f1_score',svc_classifier_f1_score],\n",
    "                                  ['svc_classifier','precision', svc_classifier_precision],\n",
    "                                  ['svc_classifier','recall', svc_classifier_recall],\n",
    "                                  ['svc_classifier','roc_auc_score', svc_classifier_roc_auc_score]]\n",
    "svc_classifier_performance_metrics = pd.DataFrame(svc_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "svc_classifier_roc_auc_plot, svc_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "svc_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "svc_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "svc_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "svc_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "svc_classifier_roc_auc_plot_ax.set_title(f'svc_classifier ROC Curve')\n",
    "# Add legend\n",
    "svc_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(svc_classifier_performance_metrics[svc_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = svc_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "svc_classifier_lift_plot, svc_classifier_lift_plot_ax = plt.subplots()\n",
    "svc_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "svc_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "svc_classifier_lift_plot_ax.set_title(f'svc_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "svc_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "svc_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(svc_classifier_performance_metrics)##### End of Model Pipeline for SVC Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION MODEL - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.get_hyperparameters()\n",
    "clf_pypelines_all.model_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for classification default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Decision Tree Classifier #####\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "dt_classifier_param_grid = {\n",
    "\"dt_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_split\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_leaf\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_weight_fraction_leaf\": np.arange(0.0, 0.5, 0.25),\n",
    "\"dt_classifier__max_leaf_nodes\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_impurity_decrease\": np.arange(0.0, 0.5, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "dt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "dt_classifier_grid_search = GridSearchCV(estimator=dt_classifier_pipe, param_grid=dt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "dt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "dt_classifier_best_estimator = dt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "dt_classifier_search_results = pd.DataFrame(dt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "dt_classifier_predictions = pd.DataFrame(dt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "dt_classifier_predictions_prob = dt_classifier_best_estimator.predict_proba(X_test)\n",
    "dt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[0]] = dt_classifier_predictions_prob[:,0]\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]] = dt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "dt_classifier_accuracy = accuracy_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_f1_score = f1_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_precision = precision_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_recall = recall_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_roc_auc_score = roc_auc_score(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "dt_classifier_performance_metrics = [['dt_classifier','accuracy',dt_classifier_accuracy], \n",
    "                                  ['dt_classifier','f1_score',dt_classifier_f1_score],\n",
    "                                  ['dt_classifier','precision', dt_classifier_precision],\n",
    "                                  ['dt_classifier','recall', dt_classifier_recall],\n",
    "                                  ['dt_classifier','roc_auc_score', dt_classifier_roc_auc_score]]\n",
    "dt_classifier_performance_metrics = pd.DataFrame(dt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "dt_classifier_roc_auc_plot, dt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "dt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "dt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "dt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_title(f'dt_classifier ROC Curve')\n",
    "# Add legend\n",
    "dt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(dt_classifier_performance_metrics[dt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = dt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "dt_classifier_lift_plot, dt_classifier_lift_plot_ax = plt.subplots()\n",
    "dt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "dt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "dt_classifier_lift_plot_ax.set_title(f'dt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "dt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "dt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(dt_classifier_performance_metrics)##### End of Model Pipeline for Decision Tree Classifier #####\n",
    "##### Model Pipeline for Logistic Regression #####\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "log_reg_param_grid = {\n",
    "\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
    "\n",
    "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
    "log_reg_predictions_prob_df = pd.DataFrame()\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
    "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
    "                                  ['log_reg','precision', log_reg_precision],\n",
    "                                  ['log_reg','recall', log_reg_recall],\n",
    "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
    "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "log_reg_roc_auc_plot, log_reg_roc_auc_plot_ax = plt.subplots()\n",
    "log_reg_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "log_reg_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "log_reg_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_title(f'log_reg ROC Curve')\n",
    "# Add legend\n",
    "log_reg_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(log_reg_performance_metrics[log_reg_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = log_reg_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "log_reg_lift_plot, log_reg_lift_plot_ax = plt.subplots()\n",
    "log_reg_lift_plot_ax.set_xlabel('Proportion')\n",
    "log_reg_lift_plot_ax.set_ylabel('Lift')\n",
    "log_reg_lift_plot_ax.set_title(f'log_reg Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "log_reg_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "log_reg_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(log_reg_performance_metrics)##### End of Model Pipeline for Logistic Regression #####\n",
    "##### Model Pipeline for Random Forest Classifier #####\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "random_forest_classifier_param_grid = {\n",
    "\"random_forest_classifier__n_estimators\": np.arange(10, 100, 20),\n",
    "\"random_forest_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"random_forest_classifier__min_samples_split\": np.arange(0.5, 1.0, 0.1),\n",
    "\"random_forest_classifier__min_samples_leaf\": np.arange(1, 10, 2),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "random_forest_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest_classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "random_forest_classifier_grid_search = GridSearchCV(estimator=random_forest_classifier_pipe, param_grid=random_forest_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "random_forest_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "random_forest_classifier_best_estimator = random_forest_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "random_forest_classifier_search_results = pd.DataFrame(random_forest_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "random_forest_classifier_predictions = pd.DataFrame(random_forest_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "random_forest_classifier_predictions_prob = random_forest_classifier_best_estimator.predict_proba(X_test)\n",
    "random_forest_classifier_predictions_prob_df = pd.DataFrame()\n",
    "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[0]] = random_forest_classifier_predictions_prob[:,0]\n",
    "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]] = random_forest_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "random_forest_classifier_accuracy = accuracy_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_f1_score = f1_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_precision = precision_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_recall = recall_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_roc_auc_score = roc_auc_score(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
    "random_forest_classifier_performance_metrics = [['random_forest_classifier','accuracy',random_forest_classifier_accuracy], \n",
    "                                  ['random_forest_classifier','f1_score',random_forest_classifier_f1_score],\n",
    "                                  ['random_forest_classifier','precision', random_forest_classifier_precision],\n",
    "                                  ['random_forest_classifier','recall', random_forest_classifier_recall],\n",
    "                                  ['random_forest_classifier','roc_auc_score', random_forest_classifier_roc_auc_score]]\n",
    "random_forest_classifier_performance_metrics = pd.DataFrame(random_forest_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "random_forest_classifier_roc_auc_plot, random_forest_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "random_forest_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "random_forest_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "random_forest_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "random_forest_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "random_forest_classifier_roc_auc_plot_ax.set_title(f'random_forest_classifier ROC Curve')\n",
    "# Add legend\n",
    "random_forest_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(random_forest_classifier_performance_metrics[random_forest_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = random_forest_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "random_forest_classifier_lift_plot, random_forest_classifier_lift_plot_ax = plt.subplots()\n",
    "random_forest_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "random_forest_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "random_forest_classifier_lift_plot_ax.set_title(f'random_forest_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "random_forest_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "random_forest_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(random_forest_classifier_performance_metrics)##### End of Model Pipeline for Random Forest Classifier #####\n",
    "##### Model Pipeline for XGBoost Classifier #####\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "xgboost_classifier_param_grid = {\n",
    "\"xgboost_classifier__learning_rate\": np.arange(0.1, 1.0, 0.25),\n",
    "\"xgboost_classifier__n_estimators\": np.arange(100, 500, 100),\n",
    "\"xgboost_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"xgboost_classifier__gamma\": np.arange(0.0, 0.5, 0.25),\n",
    "\"xgboost_classifier__subsample\": np.arange(0.1, 1.0, 0.25),\n",
    "\"xgboost_classifier__colsample_bytree\": np.arange(0.5, 1.0, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "xgboost_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgboost_classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "xgboost_classifier_grid_search = GridSearchCV(estimator=xgboost_classifier_pipe, param_grid=xgboost_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "xgboost_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "xgboost_classifier_best_estimator = xgboost_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "xgboost_classifier_search_results = pd.DataFrame(xgboost_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "xgboost_classifier_predictions = pd.DataFrame(xgboost_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "xgboost_classifier_predictions_prob = xgboost_classifier_best_estimator.predict_proba(X_test)\n",
    "xgboost_classifier_predictions_prob_df = pd.DataFrame()\n",
    "xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[0]] = xgboost_classifier_predictions_prob[:,0]\n",
    "xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]] = xgboost_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "xgboost_classifier_accuracy = accuracy_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_f1_score = f1_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_precision = precision_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_recall = recall_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_roc_auc_score = roc_auc_score(y_test, xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]])\n",
    "xgboost_classifier_performance_metrics = [['xgboost_classifier','accuracy',xgboost_classifier_accuracy], \n",
    "                                  ['xgboost_classifier','f1_score',xgboost_classifier_f1_score],\n",
    "                                  ['xgboost_classifier','precision', xgboost_classifier_precision],\n",
    "                                  ['xgboost_classifier','recall', xgboost_classifier_recall],\n",
    "                                  ['xgboost_classifier','roc_auc_score', xgboost_classifier_roc_auc_score]]\n",
    "xgboost_classifier_performance_metrics = pd.DataFrame(xgboost_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "xgboost_classifier_roc_auc_plot, xgboost_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "xgboost_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "xgboost_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "xgboost_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "xgboost_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "xgboost_classifier_roc_auc_plot_ax.set_title(f'xgboost_classifier ROC Curve')\n",
    "# Add legend\n",
    "xgboost_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(xgboost_classifier_performance_metrics[xgboost_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = xgboost_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "xgboost_classifier_lift_plot, xgboost_classifier_lift_plot_ax = plt.subplots()\n",
    "xgboost_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "xgboost_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "xgboost_classifier_lift_plot_ax.set_title(f'xgboost_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "xgboost_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "xgboost_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(xgboost_classifier_performance_metrics)##### End of Model Pipeline for XGBoost Classifier #####\n",
    "##### Model Pipeline for GBT Classifier #####\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "gbt_classifier_param_grid = {\n",
    "\"gbt_classifier__learning_rate\": np.arange(0.0, 1.0, 0.2),\n",
    "\"gbt_classifier__n_estimators\": np.arange(100, 10000, 1000),\n",
    "\"gbt_classifier__subsample\": np.arange(0.1, 1.0, 0.2),\n",
    "\"gbt_classifier__max_depth\": np.arange(1, 10000, 1000),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "gbt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gbt_classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "gbt_classifier_grid_search = GridSearchCV(estimator=gbt_classifier_pipe, param_grid=gbt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "gbt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "gbt_classifier_best_estimator = gbt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "gbt_classifier_search_results = pd.DataFrame(gbt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "gbt_classifier_predictions = pd.DataFrame(gbt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "gbt_classifier_predictions_prob = gbt_classifier_best_estimator.predict_proba(X_test)\n",
    "gbt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[0]] = gbt_classifier_predictions_prob[:,0]\n",
    "gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]] = gbt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "gbt_classifier_accuracy = accuracy_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_f1_score = f1_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_precision = precision_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_recall = recall_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_roc_auc_score = roc_auc_score(y_test, gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]])\n",
    "gbt_classifier_performance_metrics = [['gbt_classifier','accuracy',gbt_classifier_accuracy], \n",
    "                                  ['gbt_classifier','f1_score',gbt_classifier_f1_score],\n",
    "                                  ['gbt_classifier','precision', gbt_classifier_precision],\n",
    "                                  ['gbt_classifier','recall', gbt_classifier_recall],\n",
    "                                  ['gbt_classifier','roc_auc_score', gbt_classifier_roc_auc_score]]\n",
    "gbt_classifier_performance_metrics = pd.DataFrame(gbt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "gbt_classifier_roc_auc_plot, gbt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "gbt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "gbt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "gbt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "gbt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "gbt_classifier_roc_auc_plot_ax.set_title(f'gbt_classifier ROC Curve')\n",
    "# Add legend\n",
    "gbt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(gbt_classifier_performance_metrics[gbt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = gbt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "gbt_classifier_lift_plot, gbt_classifier_lift_plot_ax = plt.subplots()\n",
    "gbt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "gbt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "gbt_classifier_lift_plot_ax.set_title(f'gbt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "gbt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "gbt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(gbt_classifier_performance_metrics)##### End of Model Pipeline for GBT Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canvas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
