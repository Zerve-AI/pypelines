{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Zerve-AI/pypelines.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the pypeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = ''\n",
    "os.chdir(f'{folder}/pypelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST OF MODELS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR TIMESERIES CLASSIFICATION PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils\n",
    "\n",
    "\n",
    "utils.list_supported_models(model_type='timeseries_classification')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIMESERIES CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypelines import utils\n",
    "import pandas as pd\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.datasets import load_unit_test\n",
    "from pypelines import ts_classification_pipeline as pipe\n",
    "\n",
    "df_train, df_train_y= load_unit_test(split=\"train\")\n",
    "df_test, df_test_y = load_unit_test(split=\"test\")\n",
    "df_train['class']=df_train_y\n",
    "df_test['class']=df_test_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE TIMESERIES CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc = pipe.TSClassificationPipeline(data=df_train,\n",
    "                                    target_column='class',\n",
    "                                    models=['CNN'],\n",
    "                                    test_data=df_test,positive_class='2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.get_hyperparameters()\n",
    "tsc.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsc.model_grid_search_settings(model_name='CNN'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "    'numerical': \n",
    "    [{'search': True, 'name': 'n_epochs', 'min': 100, 'max': 2000, 'step': 1000}, \n",
    "     {'search': False, 'name': 'batch_size', 'min': 4, 'max': 16, 'step': 4}, \n",
    "     {'search': False, 'name': 'kernel_size', 'min': 4, 'max': 16, 'step': 4}, \n",
    "     {'search': False, 'name': 'avg_pool_size', 'min': 4, 'max': 16, 'step': 4}, \n",
    "     {'search': False, 'name': 'n_conv_layers', 'min': 4, 'max': 16, 'step': 4}, \n",
    "     {'search': False, 'name': 'n_conv_layers', 'min': 4, 'max': 16, 'step': 4}], \n",
    "     'categorical': []\n",
    "     }\n",
    "\n",
    "print(tsc.set_model_grid_search_settings(hyperparam_dict=hyperparameter, model_name='Random Forest Regression'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single Timeseries Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sktime import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# target dataframe: df_train\n",
    "target = \"class\"\n",
    "features = list(df_train.columns.drop(\"class\"))\n",
    "\n",
    "# train test split\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "\n",
    "##### Model Pipeline for CNN #####\n",
    "\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "CNNClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "}\n",
    "\n",
    "CNNClassifier_model = CNNClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "CNNClassifier_grid_search = GridSearchCV(estimator=CNNClassifier_model, param_grid=CNNClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "CNNClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "CNNClassifier_best_estimator = CNNClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "CNNClassifier_search_results = pd.DataFrame(CNNClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "CNNClassifier_predictions = pd.DataFrame(CNNClassifier_best_estimator.predict(X_test))\n",
    "CNNClassifier_predictions_prob = CNNClassifier_best_estimator.predict_proba(X_test)\n",
    "CNNClassifier_predictions_prob_df = pd.DataFrame()\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[0]] = CNNClassifier_predictions_prob[:,0]\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]] = CNNClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "CNNClassifier_accuracy = accuracy_score(y_test, CNNClassifier_predictions.iloc[:,0])\n",
    "CNNClassifier_f1_score = f1_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_precision = precision_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_recall = recall_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_roc_auc_score = roc_auc_score(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]])\n",
    "CNNClassifier_performance_metrics = [['CNNClassifier','accuracy',CNNClassifier_accuracy], \n",
    "                                  ['CNNClassifier','f1_score',CNNClassifier_f1_score],\n",
    "                                  ['CNNClassifier','precision', CNNClassifier_precision],\n",
    "                                  ['CNNClassifier','recall', CNNClassifier_recall],\n",
    "                                  ['CNNClassifier','roc_auc_score', CNNClassifier_roc_auc_score]]\n",
    "CNNClassifier_performance_metrics = pd.DataFrame(CNNClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "CNNClassifier_roc_auc_plot, CNNClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "CNNClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "CNNClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "CNNClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_title(f'CNNClassifier ROC Curve')\n",
    "# Add legend\n",
    "CNNClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(CNNClassifier_performance_metrics[CNNClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(CNNClassifier_performance_metrics)##### Model Metrics CNN #####\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(X_test)\n",
    "print('Predictions from best model are stored in test_predictions')\n",
    "##### End of Model Pipeline for CNN #####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Timeseries Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc = pipe.TSClassificationPipeline(data=df_train,\n",
    "                                    target_column='class',\n",
    "                                    models=['CNN', 'TAPNET'],\n",
    "                                    test_data=df_test,positive_class='2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.get_hyperparameters()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple Timeseries Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sktime import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# target dataframe: df_train\n",
    "target = \"class\"\n",
    "features = list(df_train.columns.drop(\"class\"))\n",
    "\n",
    "# train test split\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for CNN #####\n",
    "\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "CNNClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "}\n",
    "\n",
    "CNNClassifier_model = CNNClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "CNNClassifier_grid_search = GridSearchCV(estimator=CNNClassifier_model, param_grid=CNNClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "CNNClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "CNNClassifier_best_estimator = CNNClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "CNNClassifier_search_results = pd.DataFrame(CNNClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "CNNClassifier_predictions = pd.DataFrame(CNNClassifier_best_estimator.predict(X_test))\n",
    "CNNClassifier_predictions_prob = CNNClassifier_best_estimator.predict_proba(X_test)\n",
    "CNNClassifier_predictions_prob_df = pd.DataFrame()\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[0]] = CNNClassifier_predictions_prob[:,0]\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]] = CNNClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "CNNClassifier_accuracy = accuracy_score(y_test, CNNClassifier_predictions.iloc[:,0])\n",
    "CNNClassifier_f1_score = f1_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_precision = precision_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_recall = recall_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_roc_auc_score = roc_auc_score(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]])\n",
    "CNNClassifier_performance_metrics = [['CNNClassifier','accuracy',CNNClassifier_accuracy], \n",
    "                                  ['CNNClassifier','f1_score',CNNClassifier_f1_score],\n",
    "                                  ['CNNClassifier','precision', CNNClassifier_precision],\n",
    "                                  ['CNNClassifier','recall', CNNClassifier_recall],\n",
    "                                  ['CNNClassifier','roc_auc_score', CNNClassifier_roc_auc_score]]\n",
    "CNNClassifier_performance_metrics = pd.DataFrame(CNNClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "CNNClassifier_roc_auc_plot, CNNClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "CNNClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "CNNClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "CNNClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_title(f'CNNClassifier ROC Curve')\n",
    "# Add legend\n",
    "CNNClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(CNNClassifier_performance_metrics[CNNClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(CNNClassifier_performance_metrics)##### End of Model Pipeline for CNN #####\n",
    "##### Model Pipeline for TAPNET #####\n",
    "\n",
    "from sktime.classification.deep_learning.tapnet import TapNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "TapNetClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "\"use_lstm\": ['True'],\n",
    "\"use_cnn\": ['True'],\n",
    "\"use_rp\": ['True'],\n",
    "\"use_att\": ['True'],\n",
    "}\n",
    "\n",
    "TapNetClassifier_model = TapNetClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "TapNetClassifier_grid_search = GridSearchCV(estimator=TapNetClassifier_model, param_grid=TapNetClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "TapNetClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "TapNetClassifier_best_estimator = TapNetClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "TapNetClassifier_search_results = pd.DataFrame(TapNetClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "TapNetClassifier_predictions = pd.DataFrame(TapNetClassifier_best_estimator.predict(X_test))\n",
    "TapNetClassifier_predictions_prob = TapNetClassifier_best_estimator.predict_proba(X_test)\n",
    "TapNetClassifier_predictions_prob_df = pd.DataFrame()\n",
    "TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[0]] = TapNetClassifier_predictions_prob[:,0]\n",
    "TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]] = TapNetClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "TapNetClassifier_accuracy = accuracy_score(y_test, TapNetClassifier_predictions.iloc[:,0])\n",
    "TapNetClassifier_f1_score = f1_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_precision = precision_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_recall = recall_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_roc_auc_score = roc_auc_score(y_test, TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]])\n",
    "TapNetClassifier_performance_metrics = [['TapNetClassifier','accuracy',TapNetClassifier_accuracy], \n",
    "                                  ['TapNetClassifier','f1_score',TapNetClassifier_f1_score],\n",
    "                                  ['TapNetClassifier','precision', TapNetClassifier_precision],\n",
    "                                  ['TapNetClassifier','recall', TapNetClassifier_recall],\n",
    "                                  ['TapNetClassifier','roc_auc_score', TapNetClassifier_roc_auc_score]]\n",
    "TapNetClassifier_performance_metrics = pd.DataFrame(TapNetClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "TapNetClassifier_roc_auc_plot, TapNetClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "TapNetClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "TapNetClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "TapNetClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "TapNetClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "TapNetClassifier_roc_auc_plot_ax.set_title(f'TapNetClassifier ROC Curve')\n",
    "# Add legend\n",
    "TapNetClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(TapNetClassifier_performance_metrics[TapNetClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(TapNetClassifier_performance_metrics)##### End of Model Pipeline for TAPNET #####\n",
    "##### Model Comparison #####\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(X_test)\n",
    "print('Predictions from best model are stored in test_predictions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIMESERIES CLASSIFICATION MODEL - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc = pipe.TSClassificationPipeline(data=df_train,\n",
    "                                    target_column='class',\n",
    "                                    test_data=df_test,positive_class='2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.get_hyperparameters()\n",
    "tsc.model_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for regression default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sktime import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# target dataframe: df_train\n",
    "target = \"class\"\n",
    "features = list(df_train.columns.drop(\"class\"))\n",
    "\n",
    "# train test split\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for CNN #####\n",
    "\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "CNNClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "}\n",
    "\n",
    "CNNClassifier_model = CNNClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "CNNClassifier_grid_search = GridSearchCV(estimator=CNNClassifier_model, param_grid=CNNClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "CNNClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "CNNClassifier_best_estimator = CNNClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "CNNClassifier_search_results = pd.DataFrame(CNNClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "CNNClassifier_predictions = pd.DataFrame(CNNClassifier_best_estimator.predict(X_test))\n",
    "CNNClassifier_predictions_prob = CNNClassifier_best_estimator.predict_proba(X_test)\n",
    "CNNClassifier_predictions_prob_df = pd.DataFrame()\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[0]] = CNNClassifier_predictions_prob[:,0]\n",
    "CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]] = CNNClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "CNNClassifier_accuracy = accuracy_score(y_test, CNNClassifier_predictions.iloc[:,0])\n",
    "CNNClassifier_f1_score = f1_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_precision = precision_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_recall = recall_score(y_test, CNNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "CNNClassifier_roc_auc_score = roc_auc_score(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]])\n",
    "CNNClassifier_performance_metrics = [['CNNClassifier','accuracy',CNNClassifier_accuracy], \n",
    "                                  ['CNNClassifier','f1_score',CNNClassifier_f1_score],\n",
    "                                  ['CNNClassifier','precision', CNNClassifier_precision],\n",
    "                                  ['CNNClassifier','recall', CNNClassifier_recall],\n",
    "                                  ['CNNClassifier','roc_auc_score', CNNClassifier_roc_auc_score]]\n",
    "CNNClassifier_performance_metrics = pd.DataFrame(CNNClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, CNNClassifier_predictions_prob_df[CNNClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "CNNClassifier_roc_auc_plot, CNNClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "CNNClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "CNNClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "CNNClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "CNNClassifier_roc_auc_plot_ax.set_title(f'CNNClassifier ROC Curve')\n",
    "# Add legend\n",
    "CNNClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(CNNClassifier_performance_metrics[CNNClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(CNNClassifier_performance_metrics)##### End of Model Pipeline for CNN #####\n",
    "##### Model Pipeline for LSTMFCN #####\n",
    "\n",
    "from sktime.classification.deep_learning.lstmfcn import LSTMFCNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "LSTMFCNClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "}\n",
    "\n",
    "LSTMFCNClassifier_model = LSTMFCNClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "LSTMFCNClassifier_grid_search = GridSearchCV(estimator=LSTMFCNClassifier_model, param_grid=LSTMFCNClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "LSTMFCNClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "LSTMFCNClassifier_best_estimator = LSTMFCNClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "LSTMFCNClassifier_search_results = pd.DataFrame(LSTMFCNClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "LSTMFCNClassifier_predictions = pd.DataFrame(LSTMFCNClassifier_best_estimator.predict(X_test))\n",
    "LSTMFCNClassifier_predictions_prob = LSTMFCNClassifier_best_estimator.predict_proba(X_test)\n",
    "LSTMFCNClassifier_predictions_prob_df = pd.DataFrame()\n",
    "LSTMFCNClassifier_predictions_prob_df[LSTMFCNClassifier_grid_search.classes_[0]] = LSTMFCNClassifier_predictions_prob[:,0]\n",
    "LSTMFCNClassifier_predictions_prob_df[LSTMFCNClassifier_grid_search.classes_[1]] = LSTMFCNClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "LSTMFCNClassifier_accuracy = accuracy_score(y_test, LSTMFCNClassifier_predictions.iloc[:,0])\n",
    "LSTMFCNClassifier_f1_score = f1_score(y_test, LSTMFCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "LSTMFCNClassifier_precision = precision_score(y_test, LSTMFCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "LSTMFCNClassifier_recall = recall_score(y_test, LSTMFCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "LSTMFCNClassifier_roc_auc_score = roc_auc_score(y_test, LSTMFCNClassifier_predictions_prob_df[LSTMFCNClassifier_grid_search.classes_[1]])\n",
    "LSTMFCNClassifier_performance_metrics = [['LSTMFCNClassifier','accuracy',LSTMFCNClassifier_accuracy], \n",
    "                                  ['LSTMFCNClassifier','f1_score',LSTMFCNClassifier_f1_score],\n",
    "                                  ['LSTMFCNClassifier','precision', LSTMFCNClassifier_precision],\n",
    "                                  ['LSTMFCNClassifier','recall', LSTMFCNClassifier_recall],\n",
    "                                  ['LSTMFCNClassifier','roc_auc_score', LSTMFCNClassifier_roc_auc_score]]\n",
    "LSTMFCNClassifier_performance_metrics = pd.DataFrame(LSTMFCNClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, LSTMFCNClassifier_predictions_prob_df[LSTMFCNClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "LSTMFCNClassifier_roc_auc_plot, LSTMFCNClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.set_title(f'LSTMFCNClassifier ROC Curve')\n",
    "# Add legend\n",
    "LSTMFCNClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(LSTMFCNClassifier_performance_metrics[LSTMFCNClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(LSTMFCNClassifier_performance_metrics)##### End of Model Pipeline for LSTMFCN #####\n",
    "##### Model Pipeline for FCN #####\n",
    "\n",
    "from sktime.classification.deep_learning.fcn import FCNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "FCNClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "\"activation\": ['sigmoid'],\n",
    "}\n",
    "\n",
    "FCNClassifier_model = FCNClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "FCNClassifier_grid_search = GridSearchCV(estimator=FCNClassifier_model, param_grid=FCNClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "FCNClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "FCNClassifier_best_estimator = FCNClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "FCNClassifier_search_results = pd.DataFrame(FCNClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "FCNClassifier_predictions = pd.DataFrame(FCNClassifier_best_estimator.predict(X_test))\n",
    "FCNClassifier_predictions_prob = FCNClassifier_best_estimator.predict_proba(X_test)\n",
    "FCNClassifier_predictions_prob_df = pd.DataFrame()\n",
    "FCNClassifier_predictions_prob_df[FCNClassifier_grid_search.classes_[0]] = FCNClassifier_predictions_prob[:,0]\n",
    "FCNClassifier_predictions_prob_df[FCNClassifier_grid_search.classes_[1]] = FCNClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "FCNClassifier_accuracy = accuracy_score(y_test, FCNClassifier_predictions.iloc[:,0])\n",
    "FCNClassifier_f1_score = f1_score(y_test, FCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "FCNClassifier_precision = precision_score(y_test, FCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "FCNClassifier_recall = recall_score(y_test, FCNClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "FCNClassifier_roc_auc_score = roc_auc_score(y_test, FCNClassifier_predictions_prob_df[FCNClassifier_grid_search.classes_[1]])\n",
    "FCNClassifier_performance_metrics = [['FCNClassifier','accuracy',FCNClassifier_accuracy], \n",
    "                                  ['FCNClassifier','f1_score',FCNClassifier_f1_score],\n",
    "                                  ['FCNClassifier','precision', FCNClassifier_precision],\n",
    "                                  ['FCNClassifier','recall', FCNClassifier_recall],\n",
    "                                  ['FCNClassifier','roc_auc_score', FCNClassifier_roc_auc_score]]\n",
    "FCNClassifier_performance_metrics = pd.DataFrame(FCNClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, FCNClassifier_predictions_prob_df[FCNClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "FCNClassifier_roc_auc_plot, FCNClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "FCNClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "FCNClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "FCNClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "FCNClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "FCNClassifier_roc_auc_plot_ax.set_title(f'FCNClassifier ROC Curve')\n",
    "# Add legend\n",
    "FCNClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(FCNClassifier_performance_metrics[FCNClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(FCNClassifier_performance_metrics)##### End of Model Pipeline for FCN #####\n",
    "##### Model Pipeline for INCEPTIONTIME #####\n",
    "\n",
    "from sktime.classification.deep_learning.inceptiontime import InceptionTimeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "InceptionTimeClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "}\n",
    "\n",
    "InceptionTimeClassifier_model = InceptionTimeClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "InceptionTimeClassifier_grid_search = GridSearchCV(estimator=InceptionTimeClassifier_model, param_grid=InceptionTimeClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "InceptionTimeClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "InceptionTimeClassifier_best_estimator = InceptionTimeClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "InceptionTimeClassifier_search_results = pd.DataFrame(InceptionTimeClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "InceptionTimeClassifier_predictions = pd.DataFrame(InceptionTimeClassifier_best_estimator.predict(X_test))\n",
    "InceptionTimeClassifier_predictions_prob = InceptionTimeClassifier_best_estimator.predict_proba(X_test)\n",
    "InceptionTimeClassifier_predictions_prob_df = pd.DataFrame()\n",
    "InceptionTimeClassifier_predictions_prob_df[InceptionTimeClassifier_grid_search.classes_[0]] = InceptionTimeClassifier_predictions_prob[:,0]\n",
    "InceptionTimeClassifier_predictions_prob_df[InceptionTimeClassifier_grid_search.classes_[1]] = InceptionTimeClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "InceptionTimeClassifier_accuracy = accuracy_score(y_test, InceptionTimeClassifier_predictions.iloc[:,0])\n",
    "InceptionTimeClassifier_f1_score = f1_score(y_test, InceptionTimeClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "InceptionTimeClassifier_precision = precision_score(y_test, InceptionTimeClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "InceptionTimeClassifier_recall = recall_score(y_test, InceptionTimeClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "InceptionTimeClassifier_roc_auc_score = roc_auc_score(y_test, InceptionTimeClassifier_predictions_prob_df[InceptionTimeClassifier_grid_search.classes_[1]])\n",
    "InceptionTimeClassifier_performance_metrics = [['InceptionTimeClassifier','accuracy',InceptionTimeClassifier_accuracy], \n",
    "                                  ['InceptionTimeClassifier','f1_score',InceptionTimeClassifier_f1_score],\n",
    "                                  ['InceptionTimeClassifier','precision', InceptionTimeClassifier_precision],\n",
    "                                  ['InceptionTimeClassifier','recall', InceptionTimeClassifier_recall],\n",
    "                                  ['InceptionTimeClassifier','roc_auc_score', InceptionTimeClassifier_roc_auc_score]]\n",
    "InceptionTimeClassifier_performance_metrics = pd.DataFrame(InceptionTimeClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, InceptionTimeClassifier_predictions_prob_df[InceptionTimeClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "InceptionTimeClassifier_roc_auc_plot, InceptionTimeClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.set_title(f'InceptionTimeClassifier ROC Curve')\n",
    "# Add legend\n",
    "InceptionTimeClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(InceptionTimeClassifier_performance_metrics[InceptionTimeClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(InceptionTimeClassifier_performance_metrics)##### End of Model Pipeline for INCEPTIONTIME #####\n",
    "##### Model Pipeline for MLP #####\n",
    "\n",
    "from sktime.classification.deep_learning.mlp import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "MLPClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "\"activation\": ['sigmoid'],\n",
    "}\n",
    "\n",
    "MLPClassifier_model = MLPClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "MLPClassifier_grid_search = GridSearchCV(estimator=MLPClassifier_model, param_grid=MLPClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "MLPClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "MLPClassifier_best_estimator = MLPClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "MLPClassifier_search_results = pd.DataFrame(MLPClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "MLPClassifier_predictions = pd.DataFrame(MLPClassifier_best_estimator.predict(X_test))\n",
    "MLPClassifier_predictions_prob = MLPClassifier_best_estimator.predict_proba(X_test)\n",
    "MLPClassifier_predictions_prob_df = pd.DataFrame()\n",
    "MLPClassifier_predictions_prob_df[MLPClassifier_grid_search.classes_[0]] = MLPClassifier_predictions_prob[:,0]\n",
    "MLPClassifier_predictions_prob_df[MLPClassifier_grid_search.classes_[1]] = MLPClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "MLPClassifier_accuracy = accuracy_score(y_test, MLPClassifier_predictions.iloc[:,0])\n",
    "MLPClassifier_f1_score = f1_score(y_test, MLPClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "MLPClassifier_precision = precision_score(y_test, MLPClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "MLPClassifier_recall = recall_score(y_test, MLPClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "MLPClassifier_roc_auc_score = roc_auc_score(y_test, MLPClassifier_predictions_prob_df[MLPClassifier_grid_search.classes_[1]])\n",
    "MLPClassifier_performance_metrics = [['MLPClassifier','accuracy',MLPClassifier_accuracy], \n",
    "                                  ['MLPClassifier','f1_score',MLPClassifier_f1_score],\n",
    "                                  ['MLPClassifier','precision', MLPClassifier_precision],\n",
    "                                  ['MLPClassifier','recall', MLPClassifier_recall],\n",
    "                                  ['MLPClassifier','roc_auc_score', MLPClassifier_roc_auc_score]]\n",
    "MLPClassifier_performance_metrics = pd.DataFrame(MLPClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, MLPClassifier_predictions_prob_df[MLPClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "MLPClassifier_roc_auc_plot, MLPClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "MLPClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "MLPClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "MLPClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "MLPClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "MLPClassifier_roc_auc_plot_ax.set_title(f'MLPClassifier ROC Curve')\n",
    "# Add legend\n",
    "MLPClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(MLPClassifier_performance_metrics[MLPClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(MLPClassifier_performance_metrics)##### End of Model Pipeline for MLP #####\n",
    "##### Model Pipeline for TAPNET #####\n",
    "\n",
    "from sktime.classification.deep_learning.tapnet import TapNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "TapNetClassifier_param_grid = {\n",
    "\"n_epochs\": np.arange(100, 2000, 1000),\n",
    "\"use_lstm\": ['True'],\n",
    "\"use_cnn\": ['True'],\n",
    "\"use_rp\": ['True'],\n",
    "\"use_att\": ['True'],\n",
    "}\n",
    "\n",
    "TapNetClassifier_model = TapNetClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "TapNetClassifier_grid_search = GridSearchCV(estimator=TapNetClassifier_model, param_grid=TapNetClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "TapNetClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "TapNetClassifier_best_estimator = TapNetClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "TapNetClassifier_search_results = pd.DataFrame(TapNetClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "TapNetClassifier_predictions = pd.DataFrame(TapNetClassifier_best_estimator.predict(X_test))\n",
    "TapNetClassifier_predictions_prob = TapNetClassifier_best_estimator.predict_proba(X_test)\n",
    "TapNetClassifier_predictions_prob_df = pd.DataFrame()\n",
    "TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[0]] = TapNetClassifier_predictions_prob[:,0]\n",
    "TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]] = TapNetClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "TapNetClassifier_accuracy = accuracy_score(y_test, TapNetClassifier_predictions.iloc[:,0])\n",
    "TapNetClassifier_f1_score = f1_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_precision = precision_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_recall = recall_score(y_test, TapNetClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "TapNetClassifier_roc_auc_score = roc_auc_score(y_test, TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]])\n",
    "TapNetClassifier_performance_metrics = [['TapNetClassifier','accuracy',TapNetClassifier_accuracy], \n",
    "                                  ['TapNetClassifier','f1_score',TapNetClassifier_f1_score],\n",
    "                                  ['TapNetClassifier','precision', TapNetClassifier_precision],\n",
    "                                  ['TapNetClassifier','recall', TapNetClassifier_recall],\n",
    "                                  ['TapNetClassifier','roc_auc_score', TapNetClassifier_roc_auc_score]]\n",
    "TapNetClassifier_performance_metrics = pd.DataFrame(TapNetClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, TapNetClassifier_predictions_prob_df[TapNetClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "TapNetClassifier_roc_auc_plot, TapNetClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "TapNetClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "TapNetClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "TapNetClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "TapNetClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "TapNetClassifier_roc_auc_plot_ax.set_title(f'TapNetClassifier ROC Curve')\n",
    "# Add legend\n",
    "TapNetClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(TapNetClassifier_performance_metrics[TapNetClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(TapNetClassifier_performance_metrics)##### End of Model Pipeline for TAPNET #####\n",
    "##### Model Pipeline for BOSSENSEMBLE #####\n",
    "\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "BOSSEnsemble_param_grid = {\n",
    "\"max_ensemble_size\": np.arange(100, 2000, 1000),\n",
    "\"use_boss_distance\": ['True'],\n",
    "\"feature_selection\": ['none'],\n",
    "}\n",
    "\n",
    "BOSSEnsemble_model = BOSSEnsemble()\n",
    "\n",
    "# Create the grid search\n",
    "BOSSEnsemble_grid_search = GridSearchCV(estimator=BOSSEnsemble_model, param_grid=BOSSEnsemble_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "BOSSEnsemble_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "BOSSEnsemble_best_estimator = BOSSEnsemble_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "BOSSEnsemble_search_results = pd.DataFrame(BOSSEnsemble_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "BOSSEnsemble_predictions = pd.DataFrame(BOSSEnsemble_best_estimator.predict(X_test))\n",
    "BOSSEnsemble_predictions_prob = BOSSEnsemble_best_estimator.predict_proba(X_test)\n",
    "BOSSEnsemble_predictions_prob_df = pd.DataFrame()\n",
    "BOSSEnsemble_predictions_prob_df[BOSSEnsemble_grid_search.classes_[0]] = BOSSEnsemble_predictions_prob[:,0]\n",
    "BOSSEnsemble_predictions_prob_df[BOSSEnsemble_grid_search.classes_[1]] = BOSSEnsemble_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "BOSSEnsemble_accuracy = accuracy_score(y_test, BOSSEnsemble_predictions.iloc[:,0])\n",
    "BOSSEnsemble_f1_score = f1_score(y_test, BOSSEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "BOSSEnsemble_precision = precision_score(y_test, BOSSEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "BOSSEnsemble_recall = recall_score(y_test, BOSSEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "BOSSEnsemble_roc_auc_score = roc_auc_score(y_test, BOSSEnsemble_predictions_prob_df[BOSSEnsemble_grid_search.classes_[1]])\n",
    "BOSSEnsemble_performance_metrics = [['BOSSEnsemble','accuracy',BOSSEnsemble_accuracy], \n",
    "                                  ['BOSSEnsemble','f1_score',BOSSEnsemble_f1_score],\n",
    "                                  ['BOSSEnsemble','precision', BOSSEnsemble_precision],\n",
    "                                  ['BOSSEnsemble','recall', BOSSEnsemble_recall],\n",
    "                                  ['BOSSEnsemble','roc_auc_score', BOSSEnsemble_roc_auc_score]]\n",
    "BOSSEnsemble_performance_metrics = pd.DataFrame(BOSSEnsemble_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, BOSSEnsemble_predictions_prob_df[BOSSEnsemble_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "BOSSEnsemble_roc_auc_plot, BOSSEnsemble_roc_auc_plot_ax = plt.subplots()\n",
    "BOSSEnsemble_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "BOSSEnsemble_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "BOSSEnsemble_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "BOSSEnsemble_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "BOSSEnsemble_roc_auc_plot_ax.set_title(f'BOSSEnsemble ROC Curve')\n",
    "# Add legend\n",
    "BOSSEnsemble_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(BOSSEnsemble_performance_metrics[BOSSEnsemble_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(BOSSEnsemble_performance_metrics)##### End of Model Pipeline for BOSSENSEMBLE #####\n",
    "##### Model Pipeline for CONTRACTABLEBOSS #####\n",
    "\n",
    "from sktime.classification.dictionary_based import ContractableBOSS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ContractableBOSS_param_grid = {\n",
    "\"max_ensemble_size\": np.arange(100, 2000, 1000),\n",
    "\"feature_selection\": ['none'],\n",
    "}\n",
    "\n",
    "ContractableBOSS_model = ContractableBOSS()\n",
    "\n",
    "# Create the grid search\n",
    "ContractableBOSS_grid_search = GridSearchCV(estimator=ContractableBOSS_model, param_grid=ContractableBOSS_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ContractableBOSS_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ContractableBOSS_best_estimator = ContractableBOSS_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ContractableBOSS_search_results = pd.DataFrame(ContractableBOSS_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ContractableBOSS_predictions = pd.DataFrame(ContractableBOSS_best_estimator.predict(X_test))\n",
    "ContractableBOSS_predictions_prob = ContractableBOSS_best_estimator.predict_proba(X_test)\n",
    "ContractableBOSS_predictions_prob_df = pd.DataFrame()\n",
    "ContractableBOSS_predictions_prob_df[ContractableBOSS_grid_search.classes_[0]] = ContractableBOSS_predictions_prob[:,0]\n",
    "ContractableBOSS_predictions_prob_df[ContractableBOSS_grid_search.classes_[1]] = ContractableBOSS_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ContractableBOSS_accuracy = accuracy_score(y_test, ContractableBOSS_predictions.iloc[:,0])\n",
    "ContractableBOSS_f1_score = f1_score(y_test, ContractableBOSS_predictions.iloc[:,0],pos_label='2')\n",
    "ContractableBOSS_precision = precision_score(y_test, ContractableBOSS_predictions.iloc[:,0],pos_label='2')\n",
    "ContractableBOSS_recall = recall_score(y_test, ContractableBOSS_predictions.iloc[:,0],pos_label='2')\n",
    "ContractableBOSS_roc_auc_score = roc_auc_score(y_test, ContractableBOSS_predictions_prob_df[ContractableBOSS_grid_search.classes_[1]])\n",
    "ContractableBOSS_performance_metrics = [['ContractableBOSS','accuracy',ContractableBOSS_accuracy], \n",
    "                                  ['ContractableBOSS','f1_score',ContractableBOSS_f1_score],\n",
    "                                  ['ContractableBOSS','precision', ContractableBOSS_precision],\n",
    "                                  ['ContractableBOSS','recall', ContractableBOSS_recall],\n",
    "                                  ['ContractableBOSS','roc_auc_score', ContractableBOSS_roc_auc_score]]\n",
    "ContractableBOSS_performance_metrics = pd.DataFrame(ContractableBOSS_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ContractableBOSS_predictions_prob_df[ContractableBOSS_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ContractableBOSS_roc_auc_plot, ContractableBOSS_roc_auc_plot_ax = plt.subplots()\n",
    "ContractableBOSS_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ContractableBOSS_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ContractableBOSS_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ContractableBOSS_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ContractableBOSS_roc_auc_plot_ax.set_title(f'ContractableBOSS ROC Curve')\n",
    "# Add legend\n",
    "ContractableBOSS_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ContractableBOSS_performance_metrics[ContractableBOSS_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ContractableBOSS_performance_metrics)##### End of Model Pipeline for CONTRACTABLEBOSS #####\n",
    "##### Model Pipeline for ElasticEnsemble #####\n",
    "\n",
    "from sktime.classification.distance_based import ElasticEnsemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ElasticEnsemble_param_grid = {\n",
    "\"proportion_of_param_options\": np.arange(0.1, 1.0, 0.5),\n",
    "\"distance_measures\": ['euclidean'],\n",
    "}\n",
    "\n",
    "ElasticEnsemble_model = ElasticEnsemble()\n",
    "\n",
    "# Create the grid search\n",
    "ElasticEnsemble_grid_search = GridSearchCV(estimator=ElasticEnsemble_model, param_grid=ElasticEnsemble_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ElasticEnsemble_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ElasticEnsemble_best_estimator = ElasticEnsemble_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ElasticEnsemble_search_results = pd.DataFrame(ElasticEnsemble_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ElasticEnsemble_predictions = pd.DataFrame(ElasticEnsemble_best_estimator.predict(X_test))\n",
    "ElasticEnsemble_predictions_prob = ElasticEnsemble_best_estimator.predict_proba(X_test)\n",
    "ElasticEnsemble_predictions_prob_df = pd.DataFrame()\n",
    "ElasticEnsemble_predictions_prob_df[ElasticEnsemble_grid_search.classes_[0]] = ElasticEnsemble_predictions_prob[:,0]\n",
    "ElasticEnsemble_predictions_prob_df[ElasticEnsemble_grid_search.classes_[1]] = ElasticEnsemble_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ElasticEnsemble_accuracy = accuracy_score(y_test, ElasticEnsemble_predictions.iloc[:,0])\n",
    "ElasticEnsemble_f1_score = f1_score(y_test, ElasticEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "ElasticEnsemble_precision = precision_score(y_test, ElasticEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "ElasticEnsemble_recall = recall_score(y_test, ElasticEnsemble_predictions.iloc[:,0],pos_label='2')\n",
    "ElasticEnsemble_roc_auc_score = roc_auc_score(y_test, ElasticEnsemble_predictions_prob_df[ElasticEnsemble_grid_search.classes_[1]])\n",
    "ElasticEnsemble_performance_metrics = [['ElasticEnsemble','accuracy',ElasticEnsemble_accuracy], \n",
    "                                  ['ElasticEnsemble','f1_score',ElasticEnsemble_f1_score],\n",
    "                                  ['ElasticEnsemble','precision', ElasticEnsemble_precision],\n",
    "                                  ['ElasticEnsemble','recall', ElasticEnsemble_recall],\n",
    "                                  ['ElasticEnsemble','roc_auc_score', ElasticEnsemble_roc_auc_score]]\n",
    "ElasticEnsemble_performance_metrics = pd.DataFrame(ElasticEnsemble_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ElasticEnsemble_predictions_prob_df[ElasticEnsemble_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ElasticEnsemble_roc_auc_plot, ElasticEnsemble_roc_auc_plot_ax = plt.subplots()\n",
    "ElasticEnsemble_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ElasticEnsemble_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ElasticEnsemble_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ElasticEnsemble_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ElasticEnsemble_roc_auc_plot_ax.set_title(f'ElasticEnsemble ROC Curve')\n",
    "# Add legend\n",
    "ElasticEnsemble_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ElasticEnsemble_performance_metrics[ElasticEnsemble_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ElasticEnsemble_performance_metrics)##### End of Model Pipeline for ElasticEnsemble #####\n",
    "##### Model Pipeline for KNN #####\n",
    "\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "KNeighborsTimeSeriesClassifier_param_grid = {\n",
    "\"n_neighbors\": np.arange(1, 10, 2),\n",
    "\"distance\": ['euclidean'],\n",
    "\"weights\": ['uniform'],\n",
    "\"algorithm\": ['brute'],\n",
    "}\n",
    "\n",
    "KNeighborsTimeSeriesClassifier_model = KNeighborsTimeSeriesClassifier()\n",
    "\n",
    "# Create the grid search\n",
    "KNeighborsTimeSeriesClassifier_grid_search = GridSearchCV(estimator=KNeighborsTimeSeriesClassifier_model, param_grid=KNeighborsTimeSeriesClassifier_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "KNeighborsTimeSeriesClassifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "KNeighborsTimeSeriesClassifier_best_estimator = KNeighborsTimeSeriesClassifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "KNeighborsTimeSeriesClassifier_search_results = pd.DataFrame(KNeighborsTimeSeriesClassifier_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "KNeighborsTimeSeriesClassifier_predictions = pd.DataFrame(KNeighborsTimeSeriesClassifier_best_estimator.predict(X_test))\n",
    "KNeighborsTimeSeriesClassifier_predictions_prob = KNeighborsTimeSeriesClassifier_best_estimator.predict_proba(X_test)\n",
    "KNeighborsTimeSeriesClassifier_predictions_prob_df = pd.DataFrame()\n",
    "KNeighborsTimeSeriesClassifier_predictions_prob_df[KNeighborsTimeSeriesClassifier_grid_search.classes_[0]] = KNeighborsTimeSeriesClassifier_predictions_prob[:,0]\n",
    "KNeighborsTimeSeriesClassifier_predictions_prob_df[KNeighborsTimeSeriesClassifier_grid_search.classes_[1]] = KNeighborsTimeSeriesClassifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "KNeighborsTimeSeriesClassifier_accuracy = accuracy_score(y_test, KNeighborsTimeSeriesClassifier_predictions.iloc[:,0])\n",
    "KNeighborsTimeSeriesClassifier_f1_score = f1_score(y_test, KNeighborsTimeSeriesClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "KNeighborsTimeSeriesClassifier_precision = precision_score(y_test, KNeighborsTimeSeriesClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "KNeighborsTimeSeriesClassifier_recall = recall_score(y_test, KNeighborsTimeSeriesClassifier_predictions.iloc[:,0],pos_label='2')\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_score = roc_auc_score(y_test, KNeighborsTimeSeriesClassifier_predictions_prob_df[KNeighborsTimeSeriesClassifier_grid_search.classes_[1]])\n",
    "KNeighborsTimeSeriesClassifier_performance_metrics = [['KNeighborsTimeSeriesClassifier','accuracy',KNeighborsTimeSeriesClassifier_accuracy], \n",
    "                                  ['KNeighborsTimeSeriesClassifier','f1_score',KNeighborsTimeSeriesClassifier_f1_score],\n",
    "                                  ['KNeighborsTimeSeriesClassifier','precision', KNeighborsTimeSeriesClassifier_precision],\n",
    "                                  ['KNeighborsTimeSeriesClassifier','recall', KNeighborsTimeSeriesClassifier_recall],\n",
    "                                  ['KNeighborsTimeSeriesClassifier','roc_auc_score', KNeighborsTimeSeriesClassifier_roc_auc_score]]\n",
    "KNeighborsTimeSeriesClassifier_performance_metrics = pd.DataFrame(KNeighborsTimeSeriesClassifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, KNeighborsTimeSeriesClassifier_predictions_prob_df[KNeighborsTimeSeriesClassifier_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot, KNeighborsTimeSeriesClassifier_roc_auc_plot_ax = plt.subplots()\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.set_title(f'KNeighborsTimeSeriesClassifier ROC Curve')\n",
    "# Add legend\n",
    "KNeighborsTimeSeriesClassifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(KNeighborsTimeSeriesClassifier_performance_metrics[KNeighborsTimeSeriesClassifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(KNeighborsTimeSeriesClassifier_performance_metrics)##### End of Model Pipeline for KNN #####\n",
    "##### Model Pipeline for ProximityForest #####\n",
    "\n",
    "from sktime.classification.distance_based import ProximityForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ProximityForest_param_grid = {\n",
    "\"n_estimators\": np.arange(10, 100, 20),\n",
    "\"max_depth\": np.arange(10, 20, 5),\n",
    "}\n",
    "\n",
    "ProximityForest_model = ProximityForest()\n",
    "\n",
    "# Create the grid search\n",
    "ProximityForest_grid_search = GridSearchCV(estimator=ProximityForest_model, param_grid=ProximityForest_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ProximityForest_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ProximityForest_best_estimator = ProximityForest_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ProximityForest_search_results = pd.DataFrame(ProximityForest_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ProximityForest_predictions = pd.DataFrame(ProximityForest_best_estimator.predict(X_test))\n",
    "ProximityForest_predictions_prob = ProximityForest_best_estimator.predict_proba(X_test)\n",
    "ProximityForest_predictions_prob_df = pd.DataFrame()\n",
    "ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[0]] = ProximityForest_predictions_prob[:,0]\n",
    "ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]] = ProximityForest_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ProximityForest_accuracy = accuracy_score(y_test, ProximityForest_predictions.iloc[:,0])\n",
    "ProximityForest_f1_score = f1_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_precision = precision_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_recall = recall_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_roc_auc_score = roc_auc_score(y_test, ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]])\n",
    "ProximityForest_performance_metrics = [['ProximityForest','accuracy',ProximityForest_accuracy], \n",
    "                                  ['ProximityForest','f1_score',ProximityForest_f1_score],\n",
    "                                  ['ProximityForest','precision', ProximityForest_precision],\n",
    "                                  ['ProximityForest','recall', ProximityForest_recall],\n",
    "                                  ['ProximityForest','roc_auc_score', ProximityForest_roc_auc_score]]\n",
    "ProximityForest_performance_metrics = pd.DataFrame(ProximityForest_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ProximityForest_roc_auc_plot, ProximityForest_roc_auc_plot_ax = plt.subplots()\n",
    "ProximityForest_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ProximityForest_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ProximityForest_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ProximityForest_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ProximityForest_roc_auc_plot_ax.set_title(f'ProximityForest ROC Curve')\n",
    "# Add legend\n",
    "ProximityForest_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ProximityForest_performance_metrics[ProximityForest_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ProximityForest_performance_metrics)##### End of Model Pipeline for ProximityForest #####\n",
    "##### Model Pipeline for ProximityStump #####\n",
    "\n",
    "from sktime.classification.distance_based import ProximityStump\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ProximityStump_param_grid = {\n",
    "}\n",
    "\n",
    "ProximityStump_model = ProximityStump()\n",
    "\n",
    "# Create the grid search\n",
    "ProximityStump_grid_search = GridSearchCV(estimator=ProximityStump_model, param_grid=ProximityStump_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ProximityStump_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ProximityStump_best_estimator = ProximityStump_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ProximityStump_search_results = pd.DataFrame(ProximityStump_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ProximityStump_predictions = pd.DataFrame(ProximityStump_best_estimator.predict(X_test))\n",
    "ProximityStump_predictions_prob = ProximityStump_best_estimator.predict_proba(X_test)\n",
    "ProximityStump_predictions_prob_df = pd.DataFrame()\n",
    "ProximityStump_predictions_prob_df[ProximityStump_grid_search.classes_[0]] = ProximityStump_predictions_prob[:,0]\n",
    "ProximityStump_predictions_prob_df[ProximityStump_grid_search.classes_[1]] = ProximityStump_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ProximityStump_accuracy = accuracy_score(y_test, ProximityStump_predictions.iloc[:,0])\n",
    "ProximityStump_f1_score = f1_score(y_test, ProximityStump_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityStump_precision = precision_score(y_test, ProximityStump_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityStump_recall = recall_score(y_test, ProximityStump_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityStump_roc_auc_score = roc_auc_score(y_test, ProximityStump_predictions_prob_df[ProximityStump_grid_search.classes_[1]])\n",
    "ProximityStump_performance_metrics = [['ProximityStump','accuracy',ProximityStump_accuracy], \n",
    "                                  ['ProximityStump','f1_score',ProximityStump_f1_score],\n",
    "                                  ['ProximityStump','precision', ProximityStump_precision],\n",
    "                                  ['ProximityStump','recall', ProximityStump_recall],\n",
    "                                  ['ProximityStump','roc_auc_score', ProximityStump_roc_auc_score]]\n",
    "ProximityStump_performance_metrics = pd.DataFrame(ProximityStump_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ProximityStump_predictions_prob_df[ProximityStump_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ProximityStump_roc_auc_plot, ProximityStump_roc_auc_plot_ax = plt.subplots()\n",
    "ProximityStump_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ProximityStump_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ProximityStump_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ProximityStump_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ProximityStump_roc_auc_plot_ax.set_title(f'ProximityStump ROC Curve')\n",
    "# Add legend\n",
    "ProximityStump_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ProximityStump_performance_metrics[ProximityStump_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ProximityStump_performance_metrics)##### End of Model Pipeline for ProximityStump #####\n",
    "##### Model Pipeline for ProximityTree #####\n",
    "\n",
    "from sktime.classification.distance_based import ProximityTree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ProximityTree_param_grid = {\n",
    "}\n",
    "\n",
    "ProximityTree_model = ProximityTree()\n",
    "\n",
    "# Create the grid search\n",
    "ProximityTree_grid_search = GridSearchCV(estimator=ProximityTree_model, param_grid=ProximityTree_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ProximityTree_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ProximityTree_best_estimator = ProximityTree_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ProximityTree_search_results = pd.DataFrame(ProximityTree_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ProximityTree_predictions = pd.DataFrame(ProximityTree_best_estimator.predict(X_test))\n",
    "ProximityTree_predictions_prob = ProximityTree_best_estimator.predict_proba(X_test)\n",
    "ProximityTree_predictions_prob_df = pd.DataFrame()\n",
    "ProximityTree_predictions_prob_df[ProximityTree_grid_search.classes_[0]] = ProximityTree_predictions_prob[:,0]\n",
    "ProximityTree_predictions_prob_df[ProximityTree_grid_search.classes_[1]] = ProximityTree_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ProximityTree_accuracy = accuracy_score(y_test, ProximityTree_predictions.iloc[:,0])\n",
    "ProximityTree_f1_score = f1_score(y_test, ProximityTree_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityTree_precision = precision_score(y_test, ProximityTree_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityTree_recall = recall_score(y_test, ProximityTree_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityTree_roc_auc_score = roc_auc_score(y_test, ProximityTree_predictions_prob_df[ProximityTree_grid_search.classes_[1]])\n",
    "ProximityTree_performance_metrics = [['ProximityTree','accuracy',ProximityTree_accuracy], \n",
    "                                  ['ProximityTree','f1_score',ProximityTree_f1_score],\n",
    "                                  ['ProximityTree','precision', ProximityTree_precision],\n",
    "                                  ['ProximityTree','recall', ProximityTree_recall],\n",
    "                                  ['ProximityTree','roc_auc_score', ProximityTree_roc_auc_score]]\n",
    "ProximityTree_performance_metrics = pd.DataFrame(ProximityTree_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ProximityTree_predictions_prob_df[ProximityTree_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ProximityTree_roc_auc_plot, ProximityTree_roc_auc_plot_ax = plt.subplots()\n",
    "ProximityTree_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ProximityTree_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ProximityTree_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ProximityTree_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ProximityTree_roc_auc_plot_ax.set_title(f'ProximityTree ROC Curve')\n",
    "# Add legend\n",
    "ProximityTree_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ProximityTree_performance_metrics[ProximityTree_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ProximityTree_performance_metrics)##### End of Model Pipeline for ProximityTree #####\n",
    "##### Model Pipeline for ShapeDTW #####\n",
    "\n",
    "from sktime.classification.distance_based import ShapeDTW\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ShapeDTW_param_grid = {\n",
    "\"n_neighbors\": np.arange(1, 10, 5),\n",
    "\"shape_descriptor_function\": ['raw'],\n",
    "}\n",
    "\n",
    "ShapeDTW_model = ShapeDTW()\n",
    "\n",
    "# Create the grid search\n",
    "ShapeDTW_grid_search = GridSearchCV(estimator=ShapeDTW_model, param_grid=ShapeDTW_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ShapeDTW_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ShapeDTW_best_estimator = ShapeDTW_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ShapeDTW_search_results = pd.DataFrame(ShapeDTW_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ShapeDTW_predictions = pd.DataFrame(ShapeDTW_best_estimator.predict(X_test))\n",
    "ShapeDTW_predictions_prob = ShapeDTW_best_estimator.predict_proba(X_test)\n",
    "ShapeDTW_predictions_prob_df = pd.DataFrame()\n",
    "ShapeDTW_predictions_prob_df[ShapeDTW_grid_search.classes_[0]] = ShapeDTW_predictions_prob[:,0]\n",
    "ShapeDTW_predictions_prob_df[ShapeDTW_grid_search.classes_[1]] = ShapeDTW_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ShapeDTW_accuracy = accuracy_score(y_test, ShapeDTW_predictions.iloc[:,0])\n",
    "ShapeDTW_f1_score = f1_score(y_test, ShapeDTW_predictions.iloc[:,0],pos_label='2')\n",
    "ShapeDTW_precision = precision_score(y_test, ShapeDTW_predictions.iloc[:,0],pos_label='2')\n",
    "ShapeDTW_recall = recall_score(y_test, ShapeDTW_predictions.iloc[:,0],pos_label='2')\n",
    "ShapeDTW_roc_auc_score = roc_auc_score(y_test, ShapeDTW_predictions_prob_df[ShapeDTW_grid_search.classes_[1]])\n",
    "ShapeDTW_performance_metrics = [['ShapeDTW','accuracy',ShapeDTW_accuracy], \n",
    "                                  ['ShapeDTW','f1_score',ShapeDTW_f1_score],\n",
    "                                  ['ShapeDTW','precision', ShapeDTW_precision],\n",
    "                                  ['ShapeDTW','recall', ShapeDTW_recall],\n",
    "                                  ['ShapeDTW','roc_auc_score', ShapeDTW_roc_auc_score]]\n",
    "ShapeDTW_performance_metrics = pd.DataFrame(ShapeDTW_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ShapeDTW_predictions_prob_df[ShapeDTW_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ShapeDTW_roc_auc_plot, ShapeDTW_roc_auc_plot_ax = plt.subplots()\n",
    "ShapeDTW_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ShapeDTW_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ShapeDTW_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ShapeDTW_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ShapeDTW_roc_auc_plot_ax.set_title(f'ShapeDTW ROC Curve')\n",
    "# Add legend\n",
    "ShapeDTW_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ShapeDTW_performance_metrics[ShapeDTW_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ShapeDTW_performance_metrics)##### End of Model Pipeline for ShapeDTW #####\n",
    "##### Model Comparison #####\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(X_test)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canvas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
