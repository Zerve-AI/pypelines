{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Zerve-AI/pypelines.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the pypeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = ''\n",
    "os.chdir(f'{folder}/pypelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST OF MODELS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR CLUSTERING PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.clustering as pipe\n",
    "from pypelines import utils\n",
    "\n",
    "\n",
    "utils.list_supported_models(model_type='clustering')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.clustering as pipe\n",
    "from pypelines import utils\n",
    "import pandas as pd\n",
    "kmeans = pd.read_csv(\"pypelines/datasets/clustering/kmeans.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE CLUSTERING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all = pipe.ClusteringPipeline(data = kmeans, predictions_data=kmeans, nfolds = 5, models = ['KMeansClustering'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all.get_hyperparameters()\n",
    "clustering_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustering_pypelines_all.model_grid_search_settings(model_name='KMeansClustering'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'numerical': [\n",
    "        {'search': True, 'name': 'n_clusters', 'min': 3, 'max': 10, 'step': 1},\n",
    "        {'search': True, 'name': 'max_iter', 'min': 100, 'max': 200, 'step': 50}\n",
    "    ],\n",
    "    'categorical': [\n",
    "        {'search': False, 'name': 'algorithm', 'selected': [\"lloyd\"], 'values': [\"lloyd\", \"elkan\", \"auto\",\"full\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(clustering_pypelines_all.set_model_grid_search_settings(hyperparam_dict=hyperparameters, model_name='KMeansClustering'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: kmeans\n",
    "features = list(kmeans.columns)\n",
    "feature_df = kmeans[features]\n",
    "\n",
    "prediction_df = kmeans\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "kmeans[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "sample_size = np.min([10000, kmeans.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train = kmeans[features]\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustering_pypelines_all = pipe.ClusteringPipeline(data = kmeans, predictions_data=kmeans, nfolds = 5, models = ['KMeansClustering', 'BirchClustering'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: kmeans\n",
    "features = list(kmeans.columns)\n",
    "feature_df = kmeans[features]\n",
    "\n",
    "prediction_df = kmeans\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "kmeans[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "sample_size = np.min([10000, kmeans.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train = kmeans[features]\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for BirchClustering #####\n",
    "\n",
    "from sklearn.cluster import Birch \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "Birch_param_grid = {\n",
    "\"n_clusters\": np.arange(3, 10, 1),\n",
    "}\n",
    "\n",
    "\n",
    "Birch_model = Birch()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "Birch_grid_search = GridSearchCV(estimator=Birch_model, param_grid=Birch_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "Birch_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "Birch_best_estimator = Birch_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "Birch_search_results = pd.DataFrame(Birch_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(Birch_search_results.loc[Birch_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(Birch_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, Birch_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  Birch_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[Birch_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(Birch_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for Birch clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for BirchClustering #####\n",
    "##### Model Comparison #####\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING MODEL - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all = pipe.ClusteringPipeline(data = kmeans, predictions_data=kmeans, nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all.get_hyperparameters()\n",
    "clustering_pypelines_all.model_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for clustering default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: kmeans\n",
    "features = list(kmeans.columns)\n",
    "feature_df = kmeans[features]\n",
    "\n",
    "prediction_df = kmeans\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "kmeans[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "sample_size = np.min([10000, kmeans.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if kmeans[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train = kmeans[features]\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for KMeans #####\n",
    "\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "KMeans_param_grid = {\n",
    "\"n_clusters\": np.arange(3, 10, 1),\n",
    "\"max_iter\": np.arange(100, 200, 50),\n",
    "}\n",
    "\n",
    "\n",
    "KMeans_model = KMeans()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "KMeans_grid_search = GridSearchCV(estimator=KMeans_model, param_grid=KMeans_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "KMeans_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "KMeans_best_estimator = KMeans_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "KMeans_search_results = pd.DataFrame(KMeans_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(KMeans_search_results.loc[KMeans_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(KMeans_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, KMeans_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  KMeans_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[KMeans_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(KMeans_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for KMeans #####\n",
    "##### Model Pipeline for AffinityPropagation #####\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "AffinityPropagation_param_grid = {\n",
    "\"convergence_iter\": np.arange(15, 50, 20),\n",
    "\"max_iter\": np.arange(100, 200, 50),\n",
    "}\n",
    "\n",
    "\n",
    "AffinityPropagation_model = AffinityPropagation()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "AffinityPropagation_grid_search = GridSearchCV(estimator=AffinityPropagation_model, param_grid=AffinityPropagation_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "AffinityPropagation_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "AffinityPropagation_best_estimator = AffinityPropagation_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "AffinityPropagation_search_results = pd.DataFrame(AffinityPropagation_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(AffinityPropagation_search_results.loc[AffinityPropagation_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(AffinityPropagation_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, AffinityPropagation_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  AffinityPropagation_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[AffinityPropagation_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(AffinityPropagation_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for AffinityPropagation clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for AffinityPropagation #####\n",
    "##### Model Pipeline for MeanShift #####\n",
    "\n",
    "from sklearn.cluster import MeanShift \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "MeanShift_param_grid = {\n",
    "\"max_iter\": np.arange(100, 400, 50),\n",
    "}\n",
    "\n",
    "\n",
    "MeanShift_model = MeanShift()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "MeanShift_grid_search = GridSearchCV(estimator=MeanShift_model, param_grid=MeanShift_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "MeanShift_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "MeanShift_best_estimator = MeanShift_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "MeanShift_search_results = pd.DataFrame(MeanShift_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(MeanShift_search_results.loc[MeanShift_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(MeanShift_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, MeanShift_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  MeanShift_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[MeanShift_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(MeanShift_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for MeanShift clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for MeanShift #####\n",
    "##### Model Pipeline for SpectralClustering #####\n",
    "\n",
    "from sklearn.cluster import SpectralClustering \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "SpectralClustering_param_grid = {\n",
    "\"n_clusters\": np.arange(3, 10, 1),\n",
    "}\n",
    "\n",
    "\n",
    "SpectralClustering_model = SpectralClustering()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "SpectralClustering_grid_search = GridSearchCV(estimator=SpectralClustering_model, param_grid=SpectralClustering_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "SpectralClustering_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "SpectralClustering_best_estimator = SpectralClustering_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "SpectralClustering_search_results = pd.DataFrame(SpectralClustering_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(SpectralClustering_search_results.loc[SpectralClustering_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(SpectralClustering_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, SpectralClustering_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  SpectralClustering_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[SpectralClustering_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(SpectralClustering_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for SpectralClustering clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for SpectralClustering #####\n",
    "##### Model Pipeline for AgglomerativeClustering #####\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "AgglomerativeClustering_param_grid = {\n",
    "\"n_clusters\": np.arange(3, 10, 1),\n",
    "}\n",
    "\n",
    "\n",
    "AgglomerativeClustering_model = AgglomerativeClustering()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "AgglomerativeClustering_grid_search = GridSearchCV(estimator=AgglomerativeClustering_model, param_grid=AgglomerativeClustering_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "AgglomerativeClustering_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "AgglomerativeClustering_best_estimator = AgglomerativeClustering_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "AgglomerativeClustering_search_results = pd.DataFrame(AgglomerativeClustering_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(AgglomerativeClustering_search_results.loc[AgglomerativeClustering_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(AgglomerativeClustering_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, AgglomerativeClustering_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  AgglomerativeClustering_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[AgglomerativeClustering_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(AgglomerativeClustering_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for AgglomerativeClustering clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for AgglomerativeClustering #####\n",
    "##### Model Pipeline for DBSCANClustering #####\n",
    "\n",
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "DBSCAN_param_grid = {\n",
    "\"eps\": np.arange(0.1, 0.5, 0.05),\n",
    "\"min_samples\": np.arange(5, 20, 5),\n",
    "}\n",
    "\n",
    "\n",
    "DBSCAN_model = DBSCAN()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "DBSCAN_grid_search = GridSearchCV(estimator=DBSCAN_model, param_grid=DBSCAN_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "DBSCAN_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "DBSCAN_best_estimator = DBSCAN_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "DBSCAN_search_results = pd.DataFrame(DBSCAN_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(DBSCAN_search_results.loc[DBSCAN_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(DBSCAN_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, DBSCAN_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  DBSCAN_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[DBSCAN_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(DBSCAN_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for DBSCAN clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for DBSCANClustering #####\n",
    "##### Model Pipeline for HDBSCANClustering #####\n",
    "\n",
    "from sklearn.cluster import HDBSCAN \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "HDBSCAN_param_grid = {\n",
    "\"min_cluster_size\": np.arange(5, 20, 5),\n",
    "}\n",
    "\n",
    "\n",
    "HDBSCAN_model = HDBSCAN()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "HDBSCAN_grid_search = GridSearchCV(estimator=HDBSCAN_model, param_grid=HDBSCAN_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "HDBSCAN_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "HDBSCAN_best_estimator = HDBSCAN_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "HDBSCAN_search_results = pd.DataFrame(HDBSCAN_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(HDBSCAN_search_results.loc[HDBSCAN_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(HDBSCAN_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, HDBSCAN_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  HDBSCAN_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[HDBSCAN_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(HDBSCAN_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for HDBSCAN clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for HDBSCANClustering #####\n",
    "##### Model Pipeline for OPTICSClustering #####\n",
    "\n",
    "from sklearn.cluster import OPTICS \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "OPTICS_param_grid = {\n",
    "\"min_samples\": np.arange(0.1, 0.9, 0.1),\n",
    "}\n",
    "\n",
    "\n",
    "OPTICS_model = OPTICS()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "OPTICS_grid_search = GridSearchCV(estimator=OPTICS_model, param_grid=OPTICS_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "OPTICS_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "OPTICS_best_estimator = OPTICS_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "OPTICS_search_results = pd.DataFrame(OPTICS_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(OPTICS_search_results.loc[OPTICS_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(OPTICS_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, OPTICS_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  OPTICS_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[OPTICS_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(OPTICS_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for OPTICS clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for OPTICSClustering #####\n",
    "##### Model Pipeline for BirchClustering #####\n",
    "\n",
    "from sklearn.cluster import Birch \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "Birch_param_grid = {\n",
    "\"n_clusters\": np.arange(3, 10, 1),\n",
    "}\n",
    "\n",
    "\n",
    "Birch_model = Birch()\n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X, cluster_labels)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "# Create the grid search\n",
    "Birch_grid_search = GridSearchCV(estimator=Birch_model, param_grid=Birch_param_grid, cv=cv, scoring=cv_silhouette_scorer, verbose=3)\n",
    "Birch_grid_search.fit(X_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "Birch_best_estimator = Birch_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "Birch_search_results = pd.DataFrame(Birch_grid_search.cv_results_)\n",
    "\n",
    "model_comparison_list.append(Birch_search_results.loc[Birch_search_results['mean_test_score'].idxmax()])\n",
    "\n",
    "def as_list(x):\n",
    "    if type(x) is list:\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "range_n_clusters = as_list(len(np.unique(Birch_best_estimator.labels_)))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_train_preprocessed) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_train_preprocessed, Birch_best_estimator.labels_)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_train_preprocessed,  Birch_best_estimator.labels_)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[Birch_best_estimator.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(Birch_best_estimator.labels_.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_train_preprocessed[:, 0], X_train_preprocessed[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for Birch clustering on data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "##### End of Model Pipeline for BirchClustering #####\n",
    "##### Model Comparison #####\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canvas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
