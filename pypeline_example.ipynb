{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elastic Net Regression',\n",
       " 'Linear Regression',\n",
       " 'Lasso Regression',\n",
       " 'Ridge Regression',\n",
       " 'SGD Regressor Regression',\n",
       " 'Histogram Gradient Boost Regression',\n",
       " 'Random Forest Regression',\n",
       " 'AdaBoost Regression',\n",
       " 'Poisson Regression',\n",
       " 'Decision Tree Regression',\n",
       " 'GBT Regression',\n",
       " 'ExtraTree Regression',\n",
       " 'GPR Regression',\n",
       " 'Bayesian ARD Regression',\n",
       " 'Bayesian Ridge Regression',\n",
       " 'Quantile Regression',\n",
       " 'Huber Regression',\n",
       " 'TheilSen Regression',\n",
       " 'Passive Aggressive Regression',\n",
       " 'Gamma Regression',\n",
       " 'Tweedie Regression',\n",
       " 'OMP Regression',\n",
       " 'LassoLars Regression',\n",
       " 'RANSAC Regression']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.list_supported_models(model_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv(\"pypelines/datasets/regression/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all = pipe.SupervisedPipeline(data = housing,target = 'median_house_value'\n",
    "                            , model_type = 'regression'\n",
    "                            , models = ['Linear Regression','Random Forest Regression']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.get_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"pypelines/datasets/classification/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output\n",
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived'\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Logistic Regression','Random Forest Classifier']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'numerical': [{'search': True,\n",
       "    'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.1}],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'penalty',\n",
       "    'selected': ['l2'],\n",
       "    'values': ['l2', 'elasticnet', 'none']}]},\n",
       " 'Random Forest Classifier': {'numerical': [{'search': True,\n",
       "    'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 20},\n",
       "   {'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
       "   {'search': True,\n",
       "    'name': 'min_samples_split',\n",
       "    'min': 0.5,\n",
       "    'max': 1,\n",
       "    'step': 0.1},\n",
       "   {'search': True,\n",
       "    'name': 'min_samples_leaf',\n",
       "    'min': 1,\n",
       "    'max': 10,\n",
       "    'step': 2}],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'criterion',\n",
       "    'selected': ['gini'],\n",
       "    'values': ['gini', 'entropy']},\n",
       "   {'search': False,\n",
       "    'name': 'max_features',\n",
       "    'selected': ['auto'],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'search': False,\n",
       "    'name': 'bootstrap',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'search': False,\n",
       "    'name': 'oob_score',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'search': False,\n",
       "    'name': 'warm_start',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'search': False,\n",
       "    'name': 'class_weight',\n",
       "    'selected': ['balanced'],\n",
       "    'values': ['balanced', 'balanced_subsample']}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update grid search for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numerical': [{'search': True,\n",
       "   'name': 'n_estimators',\n",
       "   'min': 10,\n",
       "   'max': 100,\n",
       "   'step': 20},\n",
       "  {'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
       "  {'search': True,\n",
       "   'name': 'min_samples_split',\n",
       "   'min': 0.5,\n",
       "   'max': 1,\n",
       "   'step': 0.1},\n",
       "  {'search': True,\n",
       "   'name': 'min_samples_leaf',\n",
       "   'min': 1,\n",
       "   'max': 10,\n",
       "   'step': 2}],\n",
       " 'categorical': [{'search': False,\n",
       "   'name': 'criterion',\n",
       "   'selected': ['gini'],\n",
       "   'values': ['gini', 'entropy']},\n",
       "  {'search': False,\n",
       "   'name': 'max_features',\n",
       "   'selected': ['auto'],\n",
       "   'values': ['auto', 'sqrt', 'log2']},\n",
       "  {'search': False,\n",
       "   'name': 'bootstrap',\n",
       "   'selected': [True],\n",
       "   'values': [True, False]},\n",
       "  {'search': False,\n",
       "   'name': 'oob_score',\n",
       "   'selected': [True],\n",
       "   'values': [True, False]},\n",
       "  {'search': False,\n",
       "   'name': 'warm_start',\n",
       "   'selected': [False],\n",
       "   'values': [True, False]},\n",
       "  {'search': False,\n",
       "   'name': 'class_weight',\n",
       "   'selected': ['balanced'],\n",
       "   'values': ['balanced', 'balanced_subsample']}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.model_grid_search_settings(model_name=\"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_updated_dict = {'numerical': [{'search': True,\n",
    "   'name': 'n_estimators',\n",
    "   'min': 100,\n",
    "   'max': 1000,\n",
    "   'step': 20},\n",
    "  {'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
    "  {'search': True,\n",
    "   'name': 'min_samples_split',\n",
    "   'min': 0.50,\n",
    "   'max': 1,\n",
    "   'step': 0.1},\n",
    "  {'search': True,\n",
    "   'name': 'min_samples_leaf',\n",
    "   'min': 1,\n",
    "   'max': 10,\n",
    "   'step': 2}],\n",
    " 'categorical': [{'search': False,\n",
    "   'name': 'criterion',\n",
    "   'selected': ['gini'],\n",
    "   'values': ['gini', 'entropy']},\n",
    "  {'search': False,\n",
    "   'name': 'max_features',\n",
    "   'selected': ['sqrt'],\n",
    "   'values': ['auto', 'sqrt', 'log2']},\n",
    "  {'search': False,\n",
    "   'name': 'bootstrap',\n",
    "   'selected': [True],\n",
    "   'values': [True, False]},\n",
    "  {'search': True,\n",
    "   'name': 'oob_score',\n",
    "   'selected': [True],\n",
    "   'values': [True, False]},\n",
    "  {'search': False,\n",
    "   'name': 'warm_start',\n",
    "   'selected': [False],\n",
    "   'values': [True, False]},\n",
    "  {'search': False,\n",
    "   'name': 'class_weight',\n",
    "   'selected': ['balanced'],\n",
    "   'values': ['balanced', 'balanced_subsample']}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "# target dataframe: titanic\n",
      "target = \"Survived\"\n",
      "features = list(titanic.columns.drop(\"Survived\"))\n",
      "feature_df = titanic[features]\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "\n",
      "\n",
      "sample_size = np.min([10000, titanic.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality and make it text column\n",
      "for col in categorical_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        text_cols.append(col)\n",
      "        categorical_cols.remove(col)\n",
      "        \n",
      "\n",
      "# check text columns for low cardinality and make it categorical columns\n",
      "for col in text_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "print(numerical_cols)\n",
      "print(categorical_cols)\n",
      "print(text_cols)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", StandardScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "        transformers=[('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = titanic[features]\n",
      "y = titanic[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for Logistic Regression #####\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "import matplotlib.pyplot as plt\n",
      "log_reg_param_grid = {\n",
      "\"log_reg__n_estimators\": np.arange(100, 1000, 20),\n",
      "\"log_reg__max_depth\": np.arange(2, 10, 2),\n",
      "\"log_reg__min_samples_split\": np.arange(0.5, 1.0, 0.1),\n",
      "\"log_reg__min_samples_leaf\": np.arange(1, 10, 2),\n",
      "\"log_reg__oob_score\": [True],\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "log_reg_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('log_reg', LogisticRegression())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=1)\n",
      "log_reg_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
      "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
      "log_reg_predictions_prob_df = pd.DataFrame()\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
      "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
      "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
      "                                  ['log_reg','precision', log_reg_precision],\n",
      "                                  ['log_reg','recall', log_reg_recall],\n",
      "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
      "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "roc_auc = auc(fpr, tpr)\n",
      "# Create plot\n",
      "log_reg_roc_auc_plot, log_reg_roc_auc_plot_ax = plt.subplots()\n",
      "log_reg_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
      "log_reg_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
      "\n",
      "# Set axis labels and title\n",
      "log_reg_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
      "log_reg_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
      "log_reg_roc_auc_plot_ax.set_title('ROC Curve')\n",
      "\n",
      "# Add legend\n",
      "log_reg_roc_auc_plot_ax.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "##### Model Metrics Logistic Regression #####\n",
      "\n",
      "log_reg_performance_metrics\n",
      "plt.show()\n",
      "\n",
      "##### End of Model Pipeline for Logistic Regression #####\n",
      "\n",
      "##### Model Pipeline for Random Forest Classifier #####\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "import matplotlib.pyplot as plt\n",
      "random_forest_classifier_param_grid = {\n",
      "\"random_forest_classifier__n_estimators\": np.arange(100, 1000, 20),\n",
      "\"random_forest_classifier__max_depth\": np.arange(2, 10, 2),\n",
      "\"random_forest_classifier__min_samples_split\": np.arange(0.5, 1.0, 0.1),\n",
      "\"random_forest_classifier__min_samples_leaf\": np.arange(1, 10, 2),\n",
      "\"random_forest_classifier__oob_score\": [True],\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "random_forest_classifier_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('random_forest_classifier', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "random_forest_classifier_grid_search = GridSearchCV(estimator=random_forest_classifier_pipe, param_grid=random_forest_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=1)\n",
      "random_forest_classifier_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "random_forest_classifier_best_estimator = random_forest_classifier_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "random_forest_classifier_search_results = pd.DataFrame(random_forest_classifier_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "random_forest_classifier_predictions = pd.DataFrame(random_forest_classifier_best_estimator.predict(X_test))\n",
      "random_forest_classifier_predictions_prob = random_forest_classifier_best_estimator.predict_proba(X_test)\n",
      "random_forest_classifier_predictions_prob_df = pd.DataFrame()\n",
      "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[0]] = random_forest_classifier_predictions_prob[:,0]\n",
      "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]] = random_forest_classifier_predictions_prob[:,1] \n",
      "random_forest_classifier_accuracy = accuracy_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_f1_score = f1_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_precision = precision_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_recall = recall_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_roc_auc_score = roc_auc_score(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
      "random_forest_classifier_performance_metrics = [['random_forest_classifier','accuracy',random_forest_classifier_accuracy], \n",
      "                                  ['random_forest_classifier','f1_score',random_forest_classifier_f1_score],\n",
      "                                  ['random_forest_classifier','precision', random_forest_classifier_precision],\n",
      "                                  ['random_forest_classifier','recall', random_forest_classifier_recall],\n",
      "                                  ['random_forest_classifier','roc_auc_score', random_forest_classifier_roc_auc_score]]\n",
      "random_forest_classifier_performance_metrics = pd.DataFrame(random_forest_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
      "roc_auc = auc(fpr, tpr)\n",
      "# Create plot\n",
      "random_forest_classifier_roc_auc_plot, random_forest_classifier_roc_auc_plot_ax = plt.subplots()\n",
      "random_forest_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
      "random_forest_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
      "\n",
      "# Set axis labels and title\n",
      "random_forest_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
      "random_forest_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
      "random_forest_classifier_roc_auc_plot_ax.set_title('ROC Curve')\n",
      "\n",
      "# Add legend\n",
      "random_forest_classifier_roc_auc_plot_ax.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "##### Model Metrics Random Forest Classifier #####\n",
      "\n",
      "random_forest_classifier_performance_metrics\n",
      "plt.show()\n",
      "\n",
      "##### End of Model Pipeline for Random Forest Classifier #####\n"
     ]
    }
   ],
   "source": [
    "clf_pypelines_all.set_model_grid_search_settings(hyperparam_dict=rf_updated_dict,model_name = 'Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
