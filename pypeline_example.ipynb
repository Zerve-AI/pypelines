{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.datasets import load_unit_test\n",
    "df_train, df_train_y= load_unit_test(split=\"train\")\n",
    "df_test, df_test_y = load_unit_test(split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['class']=df_train_y\n",
    "df_test['class']=df_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypelines import ts_classification_pipeline as pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc = pipe.TSClassificationPipeline(data=df_train,\n",
    "                                    target_column='class',\n",
    "                                    models=['ProximityForest'],\n",
    "                                    test_data=df_test,positive_class='2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 113, in _check_targets\n",
      "    unique_values = np.union1d(y_true, y_pred)\n",
      "  File \"<__array_function__ internals>\", line 180, in union1d\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 781, in union1d\n",
      "    return unique(np.concatenate((ar1, ar2), axis=None))\n",
      "  File \"<__array_function__ internals>\", line 180, in unique\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 274, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 336, in _unique1d\n",
      "    ar.sort()\n",
      "TypeError: '<' not supported between instances of 'int' and 'str'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 192, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 221, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 119, in _check_targets\n",
      "    raise TypeError(\n",
      "TypeError: Labels in y_true and y_pred should be of the same type. Got y_true=['1' '2'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END .......max_depth=10, n_estimators=10;, score=nan total time=  35.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 113, in _check_targets\n",
      "    unique_values = np.union1d(y_true, y_pred)\n",
      "  File \"<__array_function__ internals>\", line 180, in union1d\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 781, in union1d\n",
      "    return unique(np.concatenate((ar1, ar2), axis=None))\n",
      "  File \"<__array_function__ internals>\", line 180, in unique\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 274, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 336, in _unique1d\n",
      "    ar.sort()\n",
      "TypeError: '<' not supported between instances of 'int' and 'str'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 192, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 221, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 119, in _check_targets\n",
      "    raise TypeError(\n",
      "TypeError: Labels in y_true and y_pred should be of the same type. Got y_true=['1' '2'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END .......max_depth=10, n_estimators=10;, score=nan total time=  42.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 113, in _check_targets\n",
      "    unique_values = np.union1d(y_true, y_pred)\n",
      "  File \"<__array_function__ internals>\", line 180, in union1d\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 781, in union1d\n",
      "    return unique(np.concatenate((ar1, ar2), axis=None))\n",
      "  File \"<__array_function__ internals>\", line 180, in unique\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 274, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 336, in _unique1d\n",
      "    ar.sort()\n",
      "TypeError: '<' not supported between instances of 'int' and 'str'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 192, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 221, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 119, in _check_targets\n",
      "    raise TypeError(\n",
      "TypeError: Labels in y_true and y_pred should be of the same type. Got y_true=['1' '2'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END .......max_depth=10, n_estimators=10;, score=nan total time= 1.2min\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Labels in y_true and y_pred should be of the same type. Got y_true=['1' '2'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:113\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     unique_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munion1d(y_true, y_pred)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# strings. So we raise a meaningful error\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munion1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py:781\u001b[0m, in \u001b[0;36munion1d\u001b[0;34m(ar1, ar2)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mFind the union of two arrays.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39marray([1, 2, 3, 4, 6])\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m \u001b[39mreturn\u001b[39;00m unique(np\u001b[39m.\u001b[39;49mconcatenate((ar1, ar2), axis\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m ProximityForest_predictions_prob_df[ProximityForest_grid_search\u001b[39m.\u001b[39mclasses_[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m ProximityForest_predictions_prob[:,\u001b[39m1\u001b[39m] \n\u001b[1;32m     56\u001b[0m \u001b[39m# Generate Model Metrics\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m ProximityForest_accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, ProximityForest_predictions\u001b[39m.\u001b[39;49miloc[:,\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     58\u001b[0m ProximityForest_f1_score \u001b[39m=\u001b[39m f1_score(y_test, ProximityForest_predictions\u001b[39m.\u001b[39miloc[:,\u001b[39m0\u001b[39m],pos_label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m ProximityForest_precision \u001b[39m=\u001b[39m precision_score(y_test, ProximityForest_predictions\u001b[39m.\u001b[39miloc[:,\u001b[39m0\u001b[39m],pos_label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:119\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    113\u001b[0m     unique_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munion1d(y_true, y_pred)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# strings. So we raise a meaningful error\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLabels in y_true and y_pred should be of the same type. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot y_true=\u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39munique(y_true)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred=\u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39munique(y_pred)\u001b[39m}\u001b[39;00m\u001b[39m. Make sure that the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpredictions provided by the classifier coincides with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe true labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unique_values) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    127\u001b[0m     y_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Labels in y_true and y_pred should be of the same type. Got y_true=['1' '2'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels."
     ]
    }
   ],
   "source": [
    "\n",
    "from sktime import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# target dataframe: df_train\n",
    "target = \"class\"\n",
    "features = list(df_train.columns.drop(\"class\"))\n",
    "\n",
    "# train test split\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for ProximityForest #####\n",
    "\n",
    "from sktime.classification.distance_based import ProximityForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "ProximityForest_param_grid = {\n",
    "\"n_estimators\": np.arange(10, 100, 100),\n",
    "\"max_depth\": np.arange(10, 20, 10),\n",
    "}\n",
    "\n",
    "ProximityForest_model = ProximityForest()\n",
    "\n",
    "# Create the grid search\n",
    "ProximityForest_grid_search = GridSearchCV(estimator=ProximityForest_model, param_grid=ProximityForest_param_grid, cv=3, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "ProximityForest_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ProximityForest_best_estimator = ProximityForest_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ProximityForest_search_results = pd.DataFrame(ProximityForest_grid_search.cv_results_)\n",
    "\n",
    "# Generate Predictions\n",
    "ProximityForest_predictions = pd.DataFrame(ProximityForest_best_estimator.predict(X_test))\n",
    "ProximityForest_predictions_prob = ProximityForest_best_estimator.predict_proba(X_test)\n",
    "ProximityForest_predictions_prob_df = pd.DataFrame()\n",
    "ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[0]] = ProximityForest_predictions_prob[:,0]\n",
    "ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]] = ProximityForest_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "ProximityForest_accuracy = accuracy_score(y_test, ProximityForest_predictions.iloc[:,0])\n",
    "ProximityForest_f1_score = f1_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_precision = precision_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_recall = recall_score(y_test, ProximityForest_predictions.iloc[:,0],pos_label='2')\n",
    "ProximityForest_roc_auc_score = roc_auc_score(y_test, ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]])\n",
    "ProximityForest_performance_metrics = [['ProximityForest','accuracy',ProximityForest_accuracy], \n",
    "                                  ['ProximityForest','f1_score',ProximityForest_f1_score],\n",
    "                                  ['ProximityForest','precision', ProximityForest_precision],\n",
    "                                  ['ProximityForest','recall', ProximityForest_recall],\n",
    "                                  ['ProximityForest','roc_auc_score', ProximityForest_roc_auc_score]]\n",
    "ProximityForest_performance_metrics = pd.DataFrame(ProximityForest_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ProximityForest_predictions_prob_df[ProximityForest_grid_search.classes_[1]],pos_label='2')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "ProximityForest_roc_auc_plot, ProximityForest_roc_auc_plot_ax = plt.subplots()\n",
    "ProximityForest_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ProximityForest_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "ProximityForest_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "ProximityForest_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "ProximityForest_roc_auc_plot_ax.set_title(f'ProximityForest ROC Curve')\n",
    "# Add legend\n",
    "ProximityForest_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(ProximityForest_performance_metrics[ProximityForest_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "model_comparison_list.append(ProximityForest_performance_metrics)##### End of Model Pipeline for ProximityForest #####\n",
    "##### Model Comparison #####\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(X_test)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
