{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decision Tree',\n",
       " 'Logistic Regression',\n",
       " 'Random Forest',\n",
       " 'SVC',\n",
       " 'XGBoost',\n",
       " 'MLP',\n",
       " 'Ridge Classifier',\n",
       " 'HistGBT Classifier',\n",
       " 'Perceptron Classifier',\n",
       " 'SGD Classifier',\n",
       " 'GBT Classifier',\n",
       " 'ADABoost Classifier',\n",
       " 'ExtraTrees Classifier',\n",
       " 'PassiveAggressive Classifier',\n",
       " 'LDA Classifier',\n",
       " 'QDA Classifier',\n",
       " 'NuSVC Classifier',\n",
       " 'GaussianNB Classifier',\n",
       " 'MultinomialNB Classifier',\n",
       " 'ComplementNB Classifier',\n",
       " 'BernoulliNB Classifier',\n",
       " 'CategoricalNB Classifier']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.list_supported_models(model_type='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"pypelines/datasets/classification/titanic.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output\n",
    "reg_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived'\n",
    "                            , model_type = 'regression'\n",
    "#                            , models = ['Logistic Regression','Random Forest']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elastic Net Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.5},\n",
       "   {'name': 'l1_ratio', 'min': 0.0, 'max': 1.0, 'step': 0.1},\n",
       "   {'name': 'max_iter', 'min': 500, 'max': 1000, 'step': 100}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'precompute', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'selection',\n",
       "    'selected': ['cyclic'],\n",
       "    'values': ['cyclic', 'random']}]},\n",
       " 'Linear Regression': {'numerical': [{'name': 'n_jobs',\n",
       "    'min': 1,\n",
       "    'max': 10,\n",
       "    'step': 1}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'normalize', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Lasso Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10},\n",
       "   {'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'precompute', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'positive', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'selection',\n",
       "    'selected': ['cyclic'],\n",
       "    'values': ['cyclic', 'random']}]},\n",
       " 'Ridge Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10},\n",
       "   {'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'positive', 'selected': [True], 'values': [True, False]}]},\n",
       " 'SGD Regressor Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.5},\n",
       "   {'name': 'l1_ratio', 'min': 0.0, 'max': 1.0, 'step': 0.1},\n",
       "   {'name': 'max_iter', 'min': 500, 'max': 1000, 'step': 500},\n",
       "   {'name': 'epsilon', 'min': 0.001, 'max': 0.1, 'step': 0.05}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'early_stopping', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'shuffle', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Histogram Gradient Boost Regression': {'numerical': [{'name': 'max_iter',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 5},\n",
       "   {'name': 'max_leaf_nodes', 'min': 2, 'max': 31, 'step': 6},\n",
       "   {'name': 'min_samples_leaf', 'min': 2, 'max': 20, 'step': 5},\n",
       "   {'name': 'max_bins', 'min': 25, 'max': 255, 'step': 25}],\n",
       "  'categorical': [{'name': 'early_stopping',\n",
       "    'selected': [False],\n",
       "    'values': ['auto', True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]}]},\n",
       " 'Random Forest Regression': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 5}],\n",
       "  'categorical': [{'name': 'bootstrap',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]}]},\n",
       " 'AdaBoost Regression': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 50,\n",
       "    'step': 10}],\n",
       "  'categorical': [{'name': 'loss',\n",
       "    'selected': ['linear'],\n",
       "    'values': ['linear', 'square', 'exponential']}]},\n",
       " 'Poisson Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.005},\n",
       "   {'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'solver',\n",
       "    'selected': ['lbfgs'],\n",
       "    'values': ['lbfgs', 'newton-cholesky']},\n",
       "   {'name': 'warm_start', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Decision Tree Regression': {'numerical': [{'name': 'max_depth',\n",
       "    'min': 1,\n",
       "    'max': 100,\n",
       "    'step': 10}],\n",
       "  'categorical': [{'name': 'max_features',\n",
       "    'selected': [None],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'name': 'splitter', 'selected': ['best'], 'values': ['best', 'random']}]},\n",
       " 'GBT Regression': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 1,\n",
       "    'max': 100,\n",
       "    'step': 10},\n",
       "   {'name': 'max_depth', 'min': 1, 'max': 3, 'step': 1},\n",
       "   {'name': 'alpha', 'min': 0.1, 'max': 1, 'step': 0.5}],\n",
       "  'categorical': [{'name': 'max_features',\n",
       "    'selected': [None],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]}]},\n",
       " 'ExtraTree Regression': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10}],\n",
       "  'categorical': [{'name': 'max_features',\n",
       "    'selected': [None],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'bootstrap', 'selected': [False], 'values': [True, False]}]},\n",
       " 'GPR Regression': {'numerical': [],\n",
       "  'categorical': [{'name': 'optimizer',\n",
       "    'selected': [None],\n",
       "    'values': [None, 'fmin_l_bfgs_b']},\n",
       "   {'name': 'normalize_y', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'copy_X_train', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Bayesian ARD Regression': {'numerical': [{'name': 'n_iter',\n",
       "    'min': 10,\n",
       "    'max': 300,\n",
       "    'step': 50}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'compute_score', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'copy_X', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Bayesian Ridge Regression': {'numerical': [{'name': 'n_iter',\n",
       "    'min': 10,\n",
       "    'max': 300,\n",
       "    'step': 50}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'compute_score', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'copy_X', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Quantile Regression': {'numerical': [{'name': 'quantile',\n",
       "    'min': 0,\n",
       "    'max': 1,\n",
       "    'step': 0.5},\n",
       "   {'name': 'alpha', 'min': 0.1, 'max': 1, 'step': 0.5}],\n",
       "  'categorical': [{'name': 'solver',\n",
       "    'selected': ['interior-point'],\n",
       "    'values': ['highs-ds',\n",
       "     'highs-ipm',\n",
       "     'highs',\n",
       "     'interior-point',\n",
       "     'revised simplex']}]},\n",
       " 'Huber Regression': {'numerical': [{'name': 'max_iter',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10}],\n",
       "  'categorical': [{'name': 'warm_start',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}]},\n",
       " 'TheilSen Regression': {'numerical': [{'name': 'max_iter',\n",
       "    'min': 10,\n",
       "    'max': 300,\n",
       "    'step': 50}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'copy_X', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Passive Aggressive Regression': {'numerical': [{'name': 'C',\n",
       "    'min': 0,\n",
       "    'max': 1,\n",
       "    'step': 0.5},\n",
       "   {'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}],\n",
       "  'categorical': [{'name': 'early_stopping',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'early_stopping', 'selected': [False], 'values': [True, False]}]},\n",
       " 'Gamma Regression': {'numerical': [{'name': 'max_iter',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10}],\n",
       "  'categorical': [{'name': 'warm_start',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}]},\n",
       " 'Tweedie Regression': {'numerical': [{'name': 'max_iter',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10},\n",
       "   {'name': 'power', 'min': 0, 'max': 3, 'step': 1}],\n",
       "  'categorical': [{'name': 'warm_start',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}]},\n",
       " 'OMP Regression': {'numerical': [],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'normalize', 'selected': [False], 'values': [True, False]}]},\n",
       " 'LassoLars Regression': {'numerical': [{'name': 'alpha',\n",
       "    'min': 0,\n",
       "    'max': 1,\n",
       "    'step': 0.5},\n",
       "   {'name': 'max_iter', 'min': 10, 'max': 500, 'step': 50}],\n",
       "  'categorical': [{'name': 'warm_start',\n",
       "    'selected': [False],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'fit_intercept', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'positive', 'selected': [False], 'values': [True, False]}]},\n",
       " 'RANSAC Regression': {'numerical': [{'name': 'max_trials',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 10}],\n",
       "  'categorical': []}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression', 'Random Forest']\n"
     ]
    }
   ],
   "source": [
    "reg_pypelines_all.model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "# target dataframe: titanic\n",
      "titanic = pd.read_csv(\"./titanic.csv\")\n",
      "target = \"Survived\"\n",
      "features = list(titanic.columns.drop(\"Survived\"))\n",
      "feature_df = titanic[features]\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "# check categorical columns for high cardinality\n",
      "\n",
      "sample_size = np.min([10000, titanic.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality\n",
      "for col in categorical_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        categorical_cols.remove(col)\n",
      "\n",
      "# check text columns for low cardinality\n",
      "for col in text_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", StandardScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        ('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        ('text', text_transformer, text_cols),\n",
      "    ]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = titanic[features]\n",
      "y = titanic[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_pypelines_all.get_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model files saved to ./code_output/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pypelines_all.code_to_file(path='./code_output/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression - selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_sel = pipe.SupervisedPipeline(data = \"titanic\",target = 'Survived'\n",
    "                            , model_type = 'regression'\n",
    "                            , models = ['Linear Regression','Elastic Net']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'numerical': [{'name': 'n_jobs',\n",
       "    'min': 1,\n",
       "    'max': 10,\n",
       "    'step': 1}],\n",
       "  'categorical': [{'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]},\n",
       "   {'name': 'normalize', 'selected': [True], 'values': [True, False]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pypelines_sel.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Linear Regression', 'Elastic Net']\n"
     ]
    }
   ],
   "source": [
    "reg_pypelines_sel.model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "# target dataframe: titanic\n",
      "titanic = pd.read_csv(\"./titanic.csv\")\n",
      "target = \"Survived\"\n",
      "features = list(titanic.columns.drop(\"Survived\"))\n",
      "feature_df = titanic[features]\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "# check categorical columns for high cardinality\n",
      "\n",
      "sample_size = np.min([10000, titanic.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality\n",
      "for col in categorical_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        categorical_cols.remove(col)\n",
      "\n",
      "# check text columns for low cardinality\n",
      "for col in text_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", StandardScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        ('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        ('text', text_transformer, text_cols),\n",
      "    ]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = titanic[features]\n",
      "y = titanic[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for Linear Regression #####\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "lin_reg_param_grid = {\n",
      "\"lin_reg__n_jobs\": np.arange(1, 10, 1),\n",
      "\"lin_reg__fit_intercept\": [True],\n",
      "\"lin_reg__normalize\": [True],\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "lin_reg_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('lin_reg', LinearRegression())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "lin_reg_grid_search = GridSearchCV(estimator=lin_reg_pipe, param_grid=lin_reg_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=1)\n",
      "lin_reg_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "lin_reg_best_estimator = lin_reg_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "lin_reg_search_results = pd.DataFrame(lin_reg_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "lin_reg_predictions = pd.DataFrame(lin_reg_best_estimator.predict(X_test))\n",
      "lin_reg_r2_score = r2_score(y_test, lin_reg_predictions.iloc[:,0])\n",
      "lin_reg_mean_squared_error = mean_squared_error(y_test, lin_reg_predictions.iloc[:,0])\n",
      "lin_reg_explained_variance_score = explained_variance_score(y_test, lin_reg_predictions.iloc[:,0])\n",
      "lin_reg_performance_metrics = [['lin_reg','r2_score', lin_reg_r2_score], \n",
      "                                  ['lin_reg','mean_squared_error',lin_reg_mean_squared_error],\n",
      "                                  ['lin_reg','explained_variance_score', lin_reg_explained_variance_score]]\n",
      "lin_reg_performance_metrics = pd.DataFrame(lin_reg_performance_metrics, columns=['model','metric', 'value'])\n",
      "lin_reg_actual_predicted_plot = px.scatter(x=y_test, y=lin_reg_predictions.iloc[:,0])\n",
      "lin_reg_actual_predicted_plot.add_shape(type=\"line\", line=dict(dash='dash'),x0=y.min(), y0=y.min(), x1=y.max(), y1=y.max())\n",
      "lin_reg_actual_predicted_plot.update_layout(title=\"Actual vs Predicted\",xaxis_title=\"Actual\",yaxis_title=\"Predicted\")\n",
      "\n",
      "\n",
      "##### Model Metrics Linear Regression #####\n",
      "\n",
      "lin_reg_performance_metrics\n",
      "lin_reg_actual_predicted_plot.show()\n",
      "\n",
      "##### End of Model Pipeline for Linear Regression #####\n"
     ]
    }
   ],
   "source": [
    "reg_pypelines_sel.get_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_sel.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model files saved to ./code_output/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pypelines_sel.code_to_file(path='./code_output/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all = pipe.SupervisedPipeline(data = \"titanic\",target = 'Survived'\n",
    "                            , model_type = 'classification'\n",
    "                           , models = ['Logistic Regression','Random Forest']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'numerical': [{'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.1}],\n",
       "  'categorical': [{'name': 'penalty',\n",
       "    'selected': ['l2'],\n",
       "    'values': ['l2', 'elasticnet', 'none']}]},\n",
       " 'Random Forest': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 20},\n",
       "   {'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
       "   {'name': 'min_samples_split', 'min': 0.5, 'max': 1, 'step': 0.1},\n",
       "   {'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 2}],\n",
       "  'categorical': [{'name': 'criterion',\n",
       "    'selected': ['gini'],\n",
       "    'values': ['gini', 'entropy']},\n",
       "   {'name': 'max_features',\n",
       "    'selected': ['auto'],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'name': 'bootstrap', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'oob_score', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'class_weight',\n",
       "    'selected': ['balanced'],\n",
       "    'values': ['balanced', 'balanced_subsample']}]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression', 'Random Forest']\n"
     ]
    }
   ],
   "source": [
    "clf_pypelines_all.model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "# target dataframe: titanic\n",
      "titanic = pd.read_csv(\"./titanic.csv\")\n",
      "target = \"Survived\"\n",
      "features = list(titanic.columns.drop(\"Survived\"))\n",
      "feature_df = titanic[features]\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "# check categorical columns for high cardinality\n",
      "\n",
      "sample_size = np.min([10000, titanic.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality\n",
      "for col in categorical_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        categorical_cols.remove(col)\n",
      "\n",
      "# check text columns for low cardinality\n",
      "for col in text_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", StandardScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        ('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        ('text', text_transformer, text_cols),\n",
      "    ]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = titanic[features]\n",
      "y = titanic[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for Logistic Regression #####\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "import plotly.express as px\n",
      "log_reg_param_grid = {\n",
      "\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\n",
      "\"log_reg__penalty\": ['l2'],\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "log_reg_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('log_reg', LogisticRegression())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=1)\n",
      "log_reg_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
      "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
      "log_reg_predictions_prob_df = pd.DataFrame()\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
      "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
      "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
      "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
      "                                  ['log_reg','precision', log_reg_precision],\n",
      "                                  ['log_reg','recall', log_reg_recall],\n",
      "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
      "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
      "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "log_reg_roc_auc_plot = px.area(\n",
      "    x=fpr, y=tpr,\n",
      "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
      "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
      "    width=700, height=500\n",
      ")\n",
      "log_reg_roc_auc_plot.add_shape(\n",
      "    type='line', line=dict(dash='dash'),\n",
      "    x0=0, x1=1, y0=0, y1=1\n",
      ")\n",
      "log_reg_roc_auc_plot.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
      "log_reg_roc_auc_plot.update_xaxes(constrain='domain')\n",
      "\n",
      "\n",
      "##### Model Metrics Logistic Regression #####\n",
      "\n",
      "log_reg_performance_metrics\n",
      "log_reg_roc_auc_plot.show()\n",
      "\n",
      "##### End of Model Pipeline for Logistic Regression #####\n",
      "\n",
      "##### Model Pipeline for Random Forest #####\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "import plotly.express as px\n",
      "random_forest_classifier_param_grid = {\n",
      "\"random_forest_classifier__n_estimators\": np.arange(10, 100, 20),\n",
      "\"random_forest_classifier__max_depth\": np.arange(2, 10, 2),\n",
      "\"random_forest_classifier__min_samples_split\": np.arange(0.5, 1.0, 0.1),\n",
      "\"random_forest_classifier__min_samples_leaf\": np.arange(1, 10, 2),\n",
      "\"random_forest_classifier__criterion\": ['gini'],\n",
      "\"random_forest_classifier__max_features\": ['auto'],\n",
      "\"random_forest_classifier__bootstrap\": [True],\n",
      "\"random_forest_classifier__oob_score\": [True],\n",
      "\"random_forest_classifier__warm_start\": [False],\n",
      "\"random_forest_classifier__class_weight\": ['balanced'],\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "random_forest_classifier_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('random_forest_classifier', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "random_forest_classifier_grid_search = GridSearchCV(estimator=random_forest_classifier_pipe, param_grid=random_forest_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=1)\n",
      "random_forest_classifier_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "random_forest_classifier_best_estimator = random_forest_classifier_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "random_forest_classifier_search_results = pd.DataFrame(random_forest_classifier_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "random_forest_classifier_predictions = pd.DataFrame(random_forest_classifier_best_estimator.predict(X_test))\n",
      "random_forest_classifier_predictions_prob = random_forest_classifier_best_estimator.predict_proba(X_test)\n",
      "random_forest_classifier_predictions_prob_df = pd.DataFrame()\n",
      "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[0]] = random_forest_classifier_predictions_prob[:,0]\n",
      "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]] = random_forest_classifier_predictions_prob[:,1] \n",
      "random_forest_classifier_accuracy = accuracy_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_f1_score = f1_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_precision = precision_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_recall = recall_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
      "random_forest_classifier_roc_auc_score = roc_auc_score(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
      "random_forest_classifier_performance_metrics = [['random_forest_classifier','accuracy',random_forest_classifier_accuracy], \n",
      "                                  ['random_forest_classifier','f1_score',random_forest_classifier_f1_score],\n",
      "                                  ['random_forest_classifier','precision', random_forest_classifier_precision],\n",
      "                                  ['random_forest_classifier','recall', random_forest_classifier_recall],\n",
      "                                  ['random_forest_classifier','roc_auc_score', random_forest_classifier_roc_auc_score]]\n",
      "random_forest_classifier_performance_metrics = pd.DataFrame(random_forest_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
      "fpr, tpr, thresholds = roc_curve(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
      "random_forest_classifier_roc_auc_plot = px.area(\n",
      "    x=fpr, y=tpr,\n",
      "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
      "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
      "    width=700, height=500\n",
      ")\n",
      "random_forest_classifier_roc_auc_plot.add_shape(\n",
      "    type='line', line=dict(dash='dash'),\n",
      "    x0=0, x1=1, y0=0, y1=1\n",
      ")\n",
      "random_forest_classifier_roc_auc_plot.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
      "random_forest_classifier_roc_auc_plot.update_xaxes(constrain='domain')\n",
      "\n",
      "\n",
      "##### Model Metrics Random Forest #####\n",
      "\n",
      "random_forest_classifier_performance_metrics\n",
      "random_forest_classifier_roc_auc_plot.show()\n",
      "\n",
      "##### End of Model Pipeline for Random Forest #####\n"
     ]
    }
   ],
   "source": [
    "clf_pypelines_all.get_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model files saved to ./code_output/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.code_to_file(path='./code_output/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification - selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf_pypelines_sel = pipe.SupervisedPipeline(data = \"titanic\",target = 'Survived'\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Logistic Regression','Random Forest']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'numerical': [{'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.1}],\n",
       "  'categorical': [{'name': 'penalty',\n",
       "    'selected': ['l2'],\n",
       "    'values': ['l2', 'elasticnet', 'none']}]},\n",
       " 'Random Forest': {'numerical': [{'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 20},\n",
       "   {'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
       "   {'name': 'min_samples_split', 'min': 0.5, 'max': 1, 'step': 0.1},\n",
       "   {'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 2}],\n",
       "  'categorical': [{'name': 'criterion',\n",
       "    'selected': ['gini'],\n",
       "    'values': ['gini', 'entropy']},\n",
       "   {'name': 'max_features',\n",
       "    'selected': ['auto'],\n",
       "    'values': ['auto', 'sqrt', 'log2']},\n",
       "   {'name': 'bootstrap', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'oob_score', 'selected': [True], 'values': [True, False]},\n",
       "   {'name': 'warm_start', 'selected': [False], 'values': [True, False]},\n",
       "   {'name': 'class_weight',\n",
       "    'selected': ['balanced'],\n",
       "    'values': ['balanced', 'balanced_subsample']}]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_sel.get_hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression', 'Random Forest']\n"
     ]
    }
   ],
   "source": [
    "clf_pypelines_sel.model_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_sel.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model files saved to ./code_output/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_sel.code_to_file(path='./code_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'numerical': [{'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.1}],\n",
       "  'categorical': [{'name': 'penalty',\n",
       "    'selected': ['l2'],\n",
       "    'values': ['l2', 'elasticnet', 'none']}]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_sel.grid_search_for_model('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "#titanic = pd.read_csv(\"./titanic.csv\")\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "# check categorical columns for high cardinality\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        categorical_cols.remove(col)\n",
    "        text_cols.append(col)\n",
    "\n",
    "# check text columns for low cardinality\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
