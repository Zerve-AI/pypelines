{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Zerve-AI/pypelines.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the pypeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\zerve\\pypelines\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (1.8.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pypelines==0.1.0) (1.10.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from jinja2->pypelines==0.1.0) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (9.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (4.39.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (23.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from matplotlib->pypelines==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pandas->pypelines==0.1.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pandas->pypelines==0.1.0) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from pydantic->pypelines==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from scikit-learn->pypelines==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from scikit-learn->pypelines==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from scikit-learn->pypelines==0.1.0) (1.10.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->pypelines==0.1.0) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\umerk\\anaconda3\\envs\\canvas\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->pypelines==0.1.0) (1.16.0)\n",
      "Building wheels for collected packages: pypelines\n",
      "  Building wheel for pypelines (setup.py): started\n",
      "  Building wheel for pypelines (setup.py): finished with status 'done'\n",
      "  Created wheel for pypelines: filename=pypelines-0.1.0-py3-none-any.whl size=73737 sha256=c9156fa9792551fbc8b118351cb34e542788d7459b54f2f5224e81a684b1db2d\n",
      "  Stored in directory: C:\\Users\\umerk\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-15_zx_ok\\wheels\\fb\\0f\\cb\\b795a2b8900b1fb79f7765687c7d6761983c7ba4d03e7047b8\n",
      "Successfully built pypelines\n",
      "Installing collected packages: pypelines\n",
      "  Attempting uninstall: pypelines\n",
      "    Found existing installation: pypelines 0.1.0\n",
      "    Uninstalling pypelines-0.1.0:\n",
      "      Successfully uninstalled pypelines-0.1.0\n",
      "Successfully installed pypelines-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST OF MODELS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR REGRESSION PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elastic Net Regression',\n",
       " 'Linear Regression',\n",
       " 'Lasso Regression',\n",
       " 'Ridge Regression',\n",
       " 'SGD Regressor Regression',\n",
       " 'Histogram Gradient Boost Regression',\n",
       " 'Random Forest Regression',\n",
       " 'AdaBoost Regression',\n",
       " 'Poisson Regression',\n",
       " 'Decision Tree Regression',\n",
       " 'GBT Regression',\n",
       " 'ExtraTree Regression',\n",
       " 'GPR Regression',\n",
       " 'Bayesian ARD Regression',\n",
       " 'Bayesian Ridge Regression',\n",
       " 'Quantile Regression',\n",
       " 'Huber Regression',\n",
       " 'TheilSen Regression',\n",
       " 'Passive Aggressive Regression',\n",
       " 'Gamma Regression',\n",
       " 'Tweedie Regression',\n",
       " 'OMP Regression',\n",
       " 'LassoLars Regression',\n",
       " 'RANSAC Regression']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils\n",
    "\n",
    "\n",
    "utils.list_supported_models(model_type='regression')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR CLASSIFICATION PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decision Tree Classifier',\n",
       " 'Logistic Regression',\n",
       " 'Random Forest Classifier',\n",
       " 'SVC Classifier',\n",
       " 'XGBoost Classifier',\n",
       " 'MLP Classifier',\n",
       " 'Ridge Classifier',\n",
       " 'Perceptron Classifier',\n",
       " 'SGD Classifier',\n",
       " 'GBT Classifier',\n",
       " 'ADABoost Classifier',\n",
       " 'ExtraTrees Classifier',\n",
       " 'PassiveAggressive Classifier',\n",
       " 'NuSVC Classifier',\n",
       " 'MultinomialNB Classifier',\n",
       " 'ComplementNB Classifier',\n",
       " 'BernoulliNB Classifier',\n",
       " 'Gaussian Process Classifier']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils\n",
    "\n",
    "\n",
    "utils.list_supported_models(model_type='classification')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS FOR ANOMALY DETECTION PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABOD Anomaly Detection',\n",
       " 'ALAD Anomaly Detection',\n",
       " 'AnoGAN Anomaly Detection',\n",
       " 'Clustering Based Local Outlier Factor',\n",
       " 'Connectivity-Based Outlier Factor',\n",
       " 'Cooks Distance Outlier Factor',\n",
       " 'Copula Based Outlier Detector',\n",
       " 'Deep One-Class Classification',\n",
       " 'Empirical Cumulative Distribution',\n",
       " 'Gaussian Mixture Model',\n",
       " 'Principal Component Analysis',\n",
       " 'R Graph',\n",
       " 'Outlier detection based on Sampling',\n",
       " 'SUOD',\n",
       " 'Rotation-based Outlier Detector',\n",
       " 'Stochastic Outlier Selection',\n",
       " 'Quasi-Monte Carlo Discrepancy outlier detection',\n",
       " 'Single-Objective Generative Adversarial Active Learning',\n",
       " 'One-class SVM detector',\n",
       " 'MO_GAAL',\n",
       " 'MCD',\n",
       " 'LUNAR',\n",
       " 'LOF',\n",
       " 'LODA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypelines.anomaly_detection_pipeline as pipe \n",
    "from pypelines import utils\n",
    "\n",
    "utils.list_supported_models(model_type='anomalydetection')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.supervised_pipeline as pipe\n",
    "from pypelines import utils\n",
    "import pandas as pd\n",
    "housing = pd.read_csv(\"pypelines/datasets/regression/housing.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE REGRESSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reg_pypelines_all = pipe.SupervisedPipeline(data = housing,target = 'median_house_value',predictions_data=housing\n",
    "                            , model_type = 'regression'\n",
    "                            , models = ['Random Forest Regression']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.get_hyperparameters()\n",
    "reg_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numerical': [{'search': True, 'name': 'n_estimators', 'min': 50, 'max': 150, 'step': 35}, {'search': True, 'name': 'max_depth', 'min': 5, 'max': 50, 'step': 10}, {'search': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 50, 'step': 20}], 'categorical': [{'search': False, 'name': 'bootstrap', 'selected': [True], 'values': [True, False]}]}\n"
     ]
    }
   ],
   "source": [
    "print(reg_pypelines_all.model_grid_search_settings(model_name='Random Forest Regression'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "\n",
      "# target dataframe: housing\n",
      "target = \"median_house_value\"\n",
      "features = list(housing.columns.drop(\"median_house_value\"))\n",
      "feature_df = housing[features]\n",
      "\n",
      "prediction_df = housing\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "housing[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "\n",
      "\n",
      "sample_size = np.min([10000, housing.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality and make it text column\n",
      "for col in categorical_cols:\n",
      "    if housing[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        text_cols.append(col)\n",
      "        categorical_cols.remove(col)\n",
      "        \n",
      "\n",
      "# check text columns for low cardinality and make it categorical columns\n",
      "for col in text_cols:\n",
      "    if housing[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "print(numerical_cols)\n",
      "print(categorical_cols)\n",
      "print(text_cols)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", MinMaxScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "        transformers=[('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = housing[features]\n",
      "y = housing[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "model_comparison_list = []\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for Random Forest Regression #####\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor \n",
      "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "random_forest_regression_param_grid = {\n",
      "\"random_forest_regression__n_estimators\": np.arange(50, 150, 35),\n",
      "\"random_forest_regression__max_depth\": np.arange(5, 50, 10),\n",
      "\"random_forest_regression__min_samples_leaf\": np.arange(1, 50, 20),\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "random_forest_regression_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('random_forest_regression', RandomForestRegressor())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "random_forest_regression_grid_search = GridSearchCV(estimator=random_forest_regression_pipe, param_grid=random_forest_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
      "random_forest_regression_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "random_forest_regression_best_estimator = random_forest_regression_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "random_forest_regression_search_results = pd.DataFrame(random_forest_regression_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "# Generate Predictions\n",
      "random_forest_regression_predictions = random_forest_regression_best_estimator.predict(X_test)\n",
      "random_forest_regression_predictions_df = pd.DataFrame(random_forest_regression_best_estimator.predict(X_test))x`\n",
      "\n",
      "# Generate Model Metrics\n",
      "random_forest_regression_r2_score = r2_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
      "random_forest_regression_mean_squared_error = mean_squared_error(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
      "random_forest_regression_explained_variance_score = explained_variance_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
      "random_forest_regression_performance_metrics = [['random_forest_regression','r2_score', random_forest_regression_r2_score], \n",
      "                                  ['random_forest_regression','mean_squared_error',random_forest_regression_mean_squared_error],\n",
      "                                  ['random_forest_regression','explained_variance_score', random_forest_regression_explained_variance_score]]\n",
      "random_forest_regression_performance_metrics = pd.DataFrame(random_forest_regression_performance_metrics, columns=['model','metric', 'value'])\n",
      "\n",
      "# Generate Actual vs Predicted Plot\n",
      "random_forest_regression_actual_predicted_plot, random_forest_regression_actual_predicted_plot_ax = plt.subplots()\n",
      "random_forest_regression_actual_predicted_plot = random_forest_regression_actual_predicted_plot_ax.scatter(x=y_test, y=random_forest_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
      "# Add diagonal line\n",
      "random_forest_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
      "# Set axis labels and title\n",
      "random_forest_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
      "random_forest_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
      "random_forest_regression_actual_predicted_plot_ax.set_title(f'random_forest_regression Actual vs. Predicted')\n",
      "plt.show(block=False)\n",
      "\n",
      "# Generate Decile Lift Chart\n",
      "# Calculate the deciles based on the residuals\n",
      "random_forest_regression_deciles = np.percentile(random_forest_regression_predictions, np.arange(0, 100, 10))\n",
      "# Calculate the mean actual and predicted values for each decile\n",
      "random_forest_regression_mean_actual = []\n",
      "random_forest_regression_mean_predicted = []\n",
      "for i in range(len(random_forest_regression_deciles) - 1):\n",
      "    mask = (random_forest_regression_predictions >= random_forest_regression_deciles[i]) & (random_forest_regression_predictions < random_forest_regression_deciles[i + 1])\n",
      "    random_forest_regression_mean_actual.append(np.mean(y_test[mask]))\n",
      "    random_forest_regression_mean_predicted.append(np.mean(random_forest_regression_predictions[mask]))\n",
      "\n",
      "# Create a bar chart of the mean actual and predicted values for each decile\n",
      "random_forest_regression_lift_plot, random_forest_regression_lift_plot_ax = plt.subplots()\n",
      "random_forest_regression_lift_plot_ax.bar(np.arange(len(random_forest_regression_mean_actual)), random_forest_regression_mean_actual, label='Actual')\n",
      "random_forest_regression_lift_plot_ax.plot(np.arange(len(random_forest_regression_mean_predicted)), random_forest_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
      "random_forest_regression_lift_plot_ax.set_xlabel('Deciles')\n",
      "random_forest_regression_lift_plot_ax.set_ylabel('Mean')\n",
      "random_forest_regression_lift_plot_ax.set_title(f'random_forest_regression Decile Analysis Chart')\n",
      "random_forest_regression_lift_plot_ax.legend()\n",
      "plt.show(block=False)\n",
      "\n",
      "\n",
      "model_comparison_list.append(random_forest_regression_performance_metrics)##### Model Metrics Random Forest Regression #####\n",
      "\n",
      "table = pd.concat(model_comparison_list)\n",
      "table = table.sort_values(by=['value'], ascending=False)\n",
      "table = table[table['metric'] == 'r2_score']\n",
      "print(table)\n",
      "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
      "\n",
      "\n",
      "# Predict test data using the best model\n",
      "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
      "print('Predictions from best model are stored in test_predictions')\n",
      "##### End of Model Pipeline for Random Forest Regression #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hyperparameter = {\n",
    "    'numerical': [\n",
    "        {'search': True, 'name': 'n_estimators', 'min': 50, 'max': 150, 'step': 35},\n",
    "        {'search': True, 'name': 'max_depth', 'min': 5, 'max': 50, 'step': 10},\n",
    "        {'search': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 50, 'step': 20}\n",
    "    ],\n",
    "    'categorical': [\n",
    "        {'search': False, 'name': 'bootstrap', 'selected': [True], 'values': [True, False]},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(reg_pypelines_all.set_model_grid_search_settings(hyperparam_dict=hyperparameter, model_name='Random Forest Regression'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: housing\n",
    "target = \"median_house_value\"\n",
    "features = list(housing.columns.drop(\"median_house_value\"))\n",
    "feature_df = housing[features]\n",
    "\n",
    "prediction_df = housing\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "housing[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, housing.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if housing[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if housing[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = housing[features]\n",
    "y = housing[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Random Forest Regression #####\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "random_forest_regression_param_grid = {\n",
    "\"random_forest_regression__n_estimators\": np.arange(50, 150, 35),\n",
    "\"random_forest_regression__max_depth\": np.arange(5, 50, 10),\n",
    "\"random_forest_regression__min_samples_leaf\": np.arange(1, 50, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "random_forest_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest_regression', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "random_forest_regression_grid_search = GridSearchCV(estimator=random_forest_regression_pipe, param_grid=random_forest_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "random_forest_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "random_forest_regression_best_estimator = random_forest_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "random_forest_regression_search_results = pd.DataFrame(random_forest_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "random_forest_regression_predictions = random_forest_regression_best_estimator.predict(X_test)\n",
    "random_forest_regression_predictions_df = pd.DataFrame(random_forest_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "random_forest_regression_r2_score = r2_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_mean_squared_error = mean_squared_error(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_explained_variance_score = explained_variance_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_performance_metrics = [['random_forest_regression','r2_score', random_forest_regression_r2_score], \n",
    "                                  ['random_forest_regression','mean_squared_error',random_forest_regression_mean_squared_error],\n",
    "                                  ['random_forest_regression','explained_variance_score', random_forest_regression_explained_variance_score]]\n",
    "random_forest_regression_performance_metrics = pd.DataFrame(random_forest_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "random_forest_regression_actual_predicted_plot, random_forest_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "random_forest_regression_actual_predicted_plot = random_forest_regression_actual_predicted_plot_ax.scatter(x=y_test, y=random_forest_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "random_forest_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "random_forest_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "random_forest_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "random_forest_regression_actual_predicted_plot_ax.set_title(f'random_forest_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "random_forest_regression_deciles = np.percentile(random_forest_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "random_forest_regression_mean_actual = []\n",
    "random_forest_regression_mean_predicted = []\n",
    "for i in range(len(random_forest_regression_deciles) - 1):\n",
    "    mask = (random_forest_regression_predictions >= random_forest_regression_deciles[i]) & (random_forest_regression_predictions < random_forest_regression_deciles[i + 1])\n",
    "    random_forest_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    random_forest_regression_mean_predicted.append(np.mean(random_forest_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "random_forest_regression_lift_plot, random_forest_regression_lift_plot_ax = plt.subplots()\n",
    "random_forest_regression_lift_plot_ax.bar(np.arange(len(random_forest_regression_mean_actual)), random_forest_regression_mean_actual, label='Actual')\n",
    "random_forest_regression_lift_plot_ax.plot(np.arange(len(random_forest_regression_mean_predicted)), random_forest_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "random_forest_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "random_forest_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "random_forest_regression_lift_plot_ax.set_title(f'random_forest_regression Decile Analysis Chart')\n",
    "random_forest_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(random_forest_regression_performance_metrics)##### End of Model Pipeline for Random Forest Regression #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'r2_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_pypelines_all = pipe.SupervisedPipeline(data = housing,target = 'median_house_value',predictions_data=housing\n",
    "                            , model_type = 'regression'\n",
    "                            , models = ['Linear Regression', 'AdaBoost Regression']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'numerical': [],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'fit_intercept',\n",
       "    'selected': [True],\n",
       "    'values': [True, False]}]},\n",
       " 'AdaBoost Regression': {'numerical': [{'search': True,\n",
       "    'name': 'n_estimators',\n",
       "    'min': 10,\n",
       "    'max': 100,\n",
       "    'step': 20}],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'loss',\n",
       "    'selected': ['linear'],\n",
       "    'values': ['linear', 'square', 'exponential']}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pypelines_all.get_hyperparameters()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: housing\n",
    "target = \"median_house_value\"\n",
    "features = list(housing.columns.drop(\"median_house_value\"))\n",
    "feature_df = housing[features]\n",
    "\n",
    "prediction_df = housing\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "housing[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, housing.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if housing[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if housing[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = housing[features]\n",
    "y = housing[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Linear Regression #####\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "lin_reg_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "lin_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "lin_reg_grid_search = GridSearchCV(estimator=lin_reg_pipe, param_grid=lin_reg_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "lin_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "lin_reg_best_estimator = lin_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "lin_reg_search_results = pd.DataFrame(lin_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "lin_reg_predictions = lin_reg_best_estimator.predict(X_test)\n",
    "lin_reg_predictions_df = pd.DataFrame(lin_reg_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "lin_reg_r2_score = r2_score(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_mean_squared_error = mean_squared_error(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_explained_variance_score = explained_variance_score(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_performance_metrics = [['lin_reg','r2_score', lin_reg_r2_score], \n",
    "                                  ['lin_reg','mean_squared_error',lin_reg_mean_squared_error],\n",
    "                                  ['lin_reg','explained_variance_score', lin_reg_explained_variance_score]]\n",
    "lin_reg_performance_metrics = pd.DataFrame(lin_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "lin_reg_actual_predicted_plot, lin_reg_actual_predicted_plot_ax = plt.subplots()\n",
    "lin_reg_actual_predicted_plot = lin_reg_actual_predicted_plot_ax.scatter(x=y_test, y=lin_reg_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "lin_reg_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "lin_reg_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "lin_reg_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "lin_reg_actual_predicted_plot_ax.set_title(f'lin_reg Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "lin_reg_deciles = np.percentile(lin_reg_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "lin_reg_mean_actual = []\n",
    "lin_reg_mean_predicted = []\n",
    "for i in range(len(lin_reg_deciles) - 1):\n",
    "    mask = (lin_reg_predictions >= lin_reg_deciles[i]) & (lin_reg_predictions < lin_reg_deciles[i + 1])\n",
    "    lin_reg_mean_actual.append(np.mean(y_test[mask]))\n",
    "    lin_reg_mean_predicted.append(np.mean(lin_reg_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "lin_reg_lift_plot, lin_reg_lift_plot_ax = plt.subplots()\n",
    "lin_reg_lift_plot_ax.bar(np.arange(len(lin_reg_mean_actual)), lin_reg_mean_actual, label='Actual')\n",
    "lin_reg_lift_plot_ax.plot(np.arange(len(lin_reg_mean_predicted)), lin_reg_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "lin_reg_lift_plot_ax.set_xlabel('Deciles')\n",
    "lin_reg_lift_plot_ax.set_ylabel('Mean')\n",
    "lin_reg_lift_plot_ax.set_title(f'lin_reg Decile Analysis Chart')\n",
    "lin_reg_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(lin_reg_performance_metrics)##### End of Model Pipeline for Linear Regression #####\n",
    "##### Model Pipeline for AdaBoost Regression #####\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "adaboost_regression_param_grid = {\n",
    "\"adaboost_regression__n_estimators\": np.arange(10, 100, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "adaboost_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('adaboost_regression', AdaBoostRegressor())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "adaboost_regression_grid_search = GridSearchCV(estimator=adaboost_regression_pipe, param_grid=adaboost_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "adaboost_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "adaboost_regression_best_estimator = adaboost_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "adaboost_regression_search_results = pd.DataFrame(adaboost_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "adaboost_regression_predictions = adaboost_regression_best_estimator.predict(X_test)\n",
    "adaboost_regression_predictions_df = pd.DataFrame(adaboost_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "adaboost_regression_r2_score = r2_score(y_test, adaboost_regression_predictions_df.iloc[:,0])\n",
    "adaboost_regression_mean_squared_error = mean_squared_error(y_test, adaboost_regression_predictions_df.iloc[:,0])\n",
    "adaboost_regression_explained_variance_score = explained_variance_score(y_test, adaboost_regression_predictions_df.iloc[:,0])\n",
    "adaboost_regression_performance_metrics = [['adaboost_regression','r2_score', adaboost_regression_r2_score], \n",
    "                                  ['adaboost_regression','mean_squared_error',adaboost_regression_mean_squared_error],\n",
    "                                  ['adaboost_regression','explained_variance_score', adaboost_regression_explained_variance_score]]\n",
    "adaboost_regression_performance_metrics = pd.DataFrame(adaboost_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "adaboost_regression_actual_predicted_plot, adaboost_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "adaboost_regression_actual_predicted_plot = adaboost_regression_actual_predicted_plot_ax.scatter(x=y_test, y=adaboost_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "adaboost_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "adaboost_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "adaboost_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "adaboost_regression_actual_predicted_plot_ax.set_title(f'adaboost_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "adaboost_regression_deciles = np.percentile(adaboost_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "adaboost_regression_mean_actual = []\n",
    "adaboost_regression_mean_predicted = []\n",
    "for i in range(len(adaboost_regression_deciles) - 1):\n",
    "    mask = (adaboost_regression_predictions >= adaboost_regression_deciles[i]) & (adaboost_regression_predictions < adaboost_regression_deciles[i + 1])\n",
    "    adaboost_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    adaboost_regression_mean_predicted.append(np.mean(adaboost_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "adaboost_regression_lift_plot, adaboost_regression_lift_plot_ax = plt.subplots()\n",
    "adaboost_regression_lift_plot_ax.bar(np.arange(len(adaboost_regression_mean_actual)), adaboost_regression_mean_actual, label='Actual')\n",
    "adaboost_regression_lift_plot_ax.plot(np.arange(len(adaboost_regression_mean_predicted)), adaboost_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "adaboost_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "adaboost_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "adaboost_regression_lift_plot_ax.set_title(f'adaboost_regression Decile Analysis Chart')\n",
    "adaboost_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(adaboost_regression_performance_metrics)##### End of Model Pipeline for AdaBoost Regression #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'r2_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESSION MODEL - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all = pipe.SupervisedPipeline(data = housing,target = 'median_house_value',predictions_data=housing\n",
    "                            , model_type = 'regression'\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elastic Net Regression', 'Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Random Forest Regression', 'Decision Tree Regression', 'GBT Regression']\n"
     ]
    }
   ],
   "source": [
    "reg_pypelines_all.get_hyperparameters()\n",
    "reg_pypelines_all.model_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for regression default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: housing\n",
    "target = \"median_house_value\"\n",
    "features = list(housing.columns.drop(\"median_house_value\"))\n",
    "feature_df = housing[features]\n",
    "\n",
    "prediction_df = housing\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "housing[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, housing.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if housing[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if housing[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = housing[features]\n",
    "y = housing[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Elastic Net Regression #####\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "elastic_net_regression_param_grid = {\n",
    "\"elastic_net_regression__alpha\": np.arange(0.1, 2.0, 0.5),\n",
    "\"elastic_net_regression__l1_ratio\": np.arange(0.1, 1.0, 0.3),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "elastic_net_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('elastic_net_regression', ElasticNet())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "elastic_net_regression_grid_search = GridSearchCV(estimator=elastic_net_regression_pipe, param_grid=elastic_net_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "elastic_net_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "elastic_net_regression_best_estimator = elastic_net_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "elastic_net_regression_search_results = pd.DataFrame(elastic_net_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "elastic_net_regression_predictions = elastic_net_regression_best_estimator.predict(X_test)\n",
    "elastic_net_regression_predictions_df = pd.DataFrame(elastic_net_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "elastic_net_regression_r2_score = r2_score(y_test, elastic_net_regression_predictions_df.iloc[:,0])\n",
    "elastic_net_regression_mean_squared_error = mean_squared_error(y_test, elastic_net_regression_predictions_df.iloc[:,0])\n",
    "elastic_net_regression_explained_variance_score = explained_variance_score(y_test, elastic_net_regression_predictions_df.iloc[:,0])\n",
    "elastic_net_regression_performance_metrics = [['elastic_net_regression','r2_score', elastic_net_regression_r2_score], \n",
    "                                  ['elastic_net_regression','mean_squared_error',elastic_net_regression_mean_squared_error],\n",
    "                                  ['elastic_net_regression','explained_variance_score', elastic_net_regression_explained_variance_score]]\n",
    "elastic_net_regression_performance_metrics = pd.DataFrame(elastic_net_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "elastic_net_regression_actual_predicted_plot, elastic_net_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "elastic_net_regression_actual_predicted_plot = elastic_net_regression_actual_predicted_plot_ax.scatter(x=y_test, y=elastic_net_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "elastic_net_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "elastic_net_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "elastic_net_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "elastic_net_regression_actual_predicted_plot_ax.set_title(f'elastic_net_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "elastic_net_regression_deciles = np.percentile(elastic_net_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "elastic_net_regression_mean_actual = []\n",
    "elastic_net_regression_mean_predicted = []\n",
    "for i in range(len(elastic_net_regression_deciles) - 1):\n",
    "    mask = (elastic_net_regression_predictions >= elastic_net_regression_deciles[i]) & (elastic_net_regression_predictions < elastic_net_regression_deciles[i + 1])\n",
    "    elastic_net_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    elastic_net_regression_mean_predicted.append(np.mean(elastic_net_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "elastic_net_regression_lift_plot, elastic_net_regression_lift_plot_ax = plt.subplots()\n",
    "elastic_net_regression_lift_plot_ax.bar(np.arange(len(elastic_net_regression_mean_actual)), elastic_net_regression_mean_actual, label='Actual')\n",
    "elastic_net_regression_lift_plot_ax.plot(np.arange(len(elastic_net_regression_mean_predicted)), elastic_net_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "elastic_net_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "elastic_net_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "elastic_net_regression_lift_plot_ax.set_title(f'elastic_net_regression Decile Analysis Chart')\n",
    "elastic_net_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(elastic_net_regression_performance_metrics)##### End of Model Pipeline for Elastic Net Regression #####\n",
    "##### Model Pipeline for Linear Regression #####\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "lin_reg_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "lin_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "lin_reg_grid_search = GridSearchCV(estimator=lin_reg_pipe, param_grid=lin_reg_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "lin_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "lin_reg_best_estimator = lin_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "lin_reg_search_results = pd.DataFrame(lin_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "lin_reg_predictions = lin_reg_best_estimator.predict(X_test)\n",
    "lin_reg_predictions_df = pd.DataFrame(lin_reg_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "lin_reg_r2_score = r2_score(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_mean_squared_error = mean_squared_error(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_explained_variance_score = explained_variance_score(y_test, lin_reg_predictions_df.iloc[:,0])\n",
    "lin_reg_performance_metrics = [['lin_reg','r2_score', lin_reg_r2_score], \n",
    "                                  ['lin_reg','mean_squared_error',lin_reg_mean_squared_error],\n",
    "                                  ['lin_reg','explained_variance_score', lin_reg_explained_variance_score]]\n",
    "lin_reg_performance_metrics = pd.DataFrame(lin_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "lin_reg_actual_predicted_plot, lin_reg_actual_predicted_plot_ax = plt.subplots()\n",
    "lin_reg_actual_predicted_plot = lin_reg_actual_predicted_plot_ax.scatter(x=y_test, y=lin_reg_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "lin_reg_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "lin_reg_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "lin_reg_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "lin_reg_actual_predicted_plot_ax.set_title(f'lin_reg Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "lin_reg_deciles = np.percentile(lin_reg_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "lin_reg_mean_actual = []\n",
    "lin_reg_mean_predicted = []\n",
    "for i in range(len(lin_reg_deciles) - 1):\n",
    "    mask = (lin_reg_predictions >= lin_reg_deciles[i]) & (lin_reg_predictions < lin_reg_deciles[i + 1])\n",
    "    lin_reg_mean_actual.append(np.mean(y_test[mask]))\n",
    "    lin_reg_mean_predicted.append(np.mean(lin_reg_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "lin_reg_lift_plot, lin_reg_lift_plot_ax = plt.subplots()\n",
    "lin_reg_lift_plot_ax.bar(np.arange(len(lin_reg_mean_actual)), lin_reg_mean_actual, label='Actual')\n",
    "lin_reg_lift_plot_ax.plot(np.arange(len(lin_reg_mean_predicted)), lin_reg_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "lin_reg_lift_plot_ax.set_xlabel('Deciles')\n",
    "lin_reg_lift_plot_ax.set_ylabel('Mean')\n",
    "lin_reg_lift_plot_ax.set_title(f'lin_reg Decile Analysis Chart')\n",
    "lin_reg_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(lin_reg_performance_metrics)##### End of Model Pipeline for Linear Regression #####\n",
    "##### Model Pipeline for Lasso Regression #####\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "lasso_regression_param_grid = {\n",
    "\"lasso_regression__alpha\": np.arange(0.0, 2.0, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "lasso_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lasso_regression', Lasso())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "lasso_regression_grid_search = GridSearchCV(estimator=lasso_regression_pipe, param_grid=lasso_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "lasso_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "lasso_regression_best_estimator = lasso_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "lasso_regression_search_results = pd.DataFrame(lasso_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "lasso_regression_predictions = lasso_regression_best_estimator.predict(X_test)\n",
    "lasso_regression_predictions_df = pd.DataFrame(lasso_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "lasso_regression_r2_score = r2_score(y_test, lasso_regression_predictions_df.iloc[:,0])\n",
    "lasso_regression_mean_squared_error = mean_squared_error(y_test, lasso_regression_predictions_df.iloc[:,0])\n",
    "lasso_regression_explained_variance_score = explained_variance_score(y_test, lasso_regression_predictions_df.iloc[:,0])\n",
    "lasso_regression_performance_metrics = [['lasso_regression','r2_score', lasso_regression_r2_score], \n",
    "                                  ['lasso_regression','mean_squared_error',lasso_regression_mean_squared_error],\n",
    "                                  ['lasso_regression','explained_variance_score', lasso_regression_explained_variance_score]]\n",
    "lasso_regression_performance_metrics = pd.DataFrame(lasso_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "lasso_regression_actual_predicted_plot, lasso_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "lasso_regression_actual_predicted_plot = lasso_regression_actual_predicted_plot_ax.scatter(x=y_test, y=lasso_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "lasso_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "lasso_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "lasso_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "lasso_regression_actual_predicted_plot_ax.set_title(f'lasso_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "lasso_regression_deciles = np.percentile(lasso_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "lasso_regression_mean_actual = []\n",
    "lasso_regression_mean_predicted = []\n",
    "for i in range(len(lasso_regression_deciles) - 1):\n",
    "    mask = (lasso_regression_predictions >= lasso_regression_deciles[i]) & (lasso_regression_predictions < lasso_regression_deciles[i + 1])\n",
    "    lasso_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    lasso_regression_mean_predicted.append(np.mean(lasso_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "lasso_regression_lift_plot, lasso_regression_lift_plot_ax = plt.subplots()\n",
    "lasso_regression_lift_plot_ax.bar(np.arange(len(lasso_regression_mean_actual)), lasso_regression_mean_actual, label='Actual')\n",
    "lasso_regression_lift_plot_ax.plot(np.arange(len(lasso_regression_mean_predicted)), lasso_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "lasso_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "lasso_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "lasso_regression_lift_plot_ax.set_title(f'lasso_regression Decile Analysis Chart')\n",
    "lasso_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(lasso_regression_performance_metrics)##### End of Model Pipeline for Lasso Regression #####\n",
    "##### Model Pipeline for Ridge Regression #####\n",
    "\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "ridge_regression_param_grid = {\n",
    "\"ridge_regression__alpha\": np.arange(0.1, 2.0, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "ridge_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ridge_regression', Ridge())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "ridge_regression_grid_search = GridSearchCV(estimator=ridge_regression_pipe, param_grid=ridge_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "ridge_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ridge_regression_best_estimator = ridge_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "ridge_regression_search_results = pd.DataFrame(ridge_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "ridge_regression_predictions = ridge_regression_best_estimator.predict(X_test)\n",
    "ridge_regression_predictions_df = pd.DataFrame(ridge_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "ridge_regression_r2_score = r2_score(y_test, ridge_regression_predictions_df.iloc[:,0])\n",
    "ridge_regression_mean_squared_error = mean_squared_error(y_test, ridge_regression_predictions_df.iloc[:,0])\n",
    "ridge_regression_explained_variance_score = explained_variance_score(y_test, ridge_regression_predictions_df.iloc[:,0])\n",
    "ridge_regression_performance_metrics = [['ridge_regression','r2_score', ridge_regression_r2_score], \n",
    "                                  ['ridge_regression','mean_squared_error',ridge_regression_mean_squared_error],\n",
    "                                  ['ridge_regression','explained_variance_score', ridge_regression_explained_variance_score]]\n",
    "ridge_regression_performance_metrics = pd.DataFrame(ridge_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "ridge_regression_actual_predicted_plot, ridge_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "ridge_regression_actual_predicted_plot = ridge_regression_actual_predicted_plot_ax.scatter(x=y_test, y=ridge_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "ridge_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "ridge_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "ridge_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "ridge_regression_actual_predicted_plot_ax.set_title(f'ridge_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "ridge_regression_deciles = np.percentile(ridge_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "ridge_regression_mean_actual = []\n",
    "ridge_regression_mean_predicted = []\n",
    "for i in range(len(ridge_regression_deciles) - 1):\n",
    "    mask = (ridge_regression_predictions >= ridge_regression_deciles[i]) & (ridge_regression_predictions < ridge_regression_deciles[i + 1])\n",
    "    ridge_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    ridge_regression_mean_predicted.append(np.mean(ridge_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "ridge_regression_lift_plot, ridge_regression_lift_plot_ax = plt.subplots()\n",
    "ridge_regression_lift_plot_ax.bar(np.arange(len(ridge_regression_mean_actual)), ridge_regression_mean_actual, label='Actual')\n",
    "ridge_regression_lift_plot_ax.plot(np.arange(len(ridge_regression_mean_predicted)), ridge_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "ridge_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "ridge_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "ridge_regression_lift_plot_ax.set_title(f'ridge_regression Decile Analysis Chart')\n",
    "ridge_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(ridge_regression_performance_metrics)##### End of Model Pipeline for Ridge Regression #####\n",
    "##### Model Pipeline for Random Forest Regression #####\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "random_forest_regression_param_grid = {\n",
    "\"random_forest_regression__n_estimators\": np.arange(50, 150, 35),\n",
    "\"random_forest_regression__max_depth\": np.arange(5, 50, 10),\n",
    "\"random_forest_regression__min_samples_leaf\": np.arange(1, 50, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "random_forest_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest_regression', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "random_forest_regression_grid_search = GridSearchCV(estimator=random_forest_regression_pipe, param_grid=random_forest_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "random_forest_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "random_forest_regression_best_estimator = random_forest_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "random_forest_regression_search_results = pd.DataFrame(random_forest_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "random_forest_regression_predictions = random_forest_regression_best_estimator.predict(X_test)\n",
    "random_forest_regression_predictions_df = pd.DataFrame(random_forest_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "random_forest_regression_r2_score = r2_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_mean_squared_error = mean_squared_error(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_explained_variance_score = explained_variance_score(y_test, random_forest_regression_predictions_df.iloc[:,0])\n",
    "random_forest_regression_performance_metrics = [['random_forest_regression','r2_score', random_forest_regression_r2_score], \n",
    "                                  ['random_forest_regression','mean_squared_error',random_forest_regression_mean_squared_error],\n",
    "                                  ['random_forest_regression','explained_variance_score', random_forest_regression_explained_variance_score]]\n",
    "random_forest_regression_performance_metrics = pd.DataFrame(random_forest_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "random_forest_regression_actual_predicted_plot, random_forest_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "random_forest_regression_actual_predicted_plot = random_forest_regression_actual_predicted_plot_ax.scatter(x=y_test, y=random_forest_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "random_forest_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "random_forest_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "random_forest_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "random_forest_regression_actual_predicted_plot_ax.set_title(f'random_forest_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "random_forest_regression_deciles = np.percentile(random_forest_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "random_forest_regression_mean_actual = []\n",
    "random_forest_regression_mean_predicted = []\n",
    "for i in range(len(random_forest_regression_deciles) - 1):\n",
    "    mask = (random_forest_regression_predictions >= random_forest_regression_deciles[i]) & (random_forest_regression_predictions < random_forest_regression_deciles[i + 1])\n",
    "    random_forest_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    random_forest_regression_mean_predicted.append(np.mean(random_forest_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "random_forest_regression_lift_plot, random_forest_regression_lift_plot_ax = plt.subplots()\n",
    "random_forest_regression_lift_plot_ax.bar(np.arange(len(random_forest_regression_mean_actual)), random_forest_regression_mean_actual, label='Actual')\n",
    "random_forest_regression_lift_plot_ax.plot(np.arange(len(random_forest_regression_mean_predicted)), random_forest_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "random_forest_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "random_forest_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "random_forest_regression_lift_plot_ax.set_title(f'random_forest_regression Decile Analysis Chart')\n",
    "random_forest_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(random_forest_regression_performance_metrics)##### End of Model Pipeline for Random Forest Regression #####\n",
    "##### Model Pipeline for Decision Tree Regression #####\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "decision_tree_regression_param_grid = {\n",
    "\"decision_tree_regression__max_depth\": np.arange(1, 10, 3),\n",
    "\"decision_tree_regression__max_features\": ['auto'],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "decision_tree_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('decision_tree_regression', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "decision_tree_regression_grid_search = GridSearchCV(estimator=decision_tree_regression_pipe, param_grid=decision_tree_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "decision_tree_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "decision_tree_regression_best_estimator = decision_tree_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "decision_tree_regression_search_results = pd.DataFrame(decision_tree_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "decision_tree_regression_predictions = decision_tree_regression_best_estimator.predict(X_test)\n",
    "decision_tree_regression_predictions_df = pd.DataFrame(decision_tree_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "decision_tree_regression_r2_score = r2_score(y_test, decision_tree_regression_predictions_df.iloc[:,0])\n",
    "decision_tree_regression_mean_squared_error = mean_squared_error(y_test, decision_tree_regression_predictions_df.iloc[:,0])\n",
    "decision_tree_regression_explained_variance_score = explained_variance_score(y_test, decision_tree_regression_predictions_df.iloc[:,0])\n",
    "decision_tree_regression_performance_metrics = [['decision_tree_regression','r2_score', decision_tree_regression_r2_score], \n",
    "                                  ['decision_tree_regression','mean_squared_error',decision_tree_regression_mean_squared_error],\n",
    "                                  ['decision_tree_regression','explained_variance_score', decision_tree_regression_explained_variance_score]]\n",
    "decision_tree_regression_performance_metrics = pd.DataFrame(decision_tree_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "decision_tree_regression_actual_predicted_plot, decision_tree_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "decision_tree_regression_actual_predicted_plot = decision_tree_regression_actual_predicted_plot_ax.scatter(x=y_test, y=decision_tree_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "decision_tree_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "decision_tree_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "decision_tree_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "decision_tree_regression_actual_predicted_plot_ax.set_title(f'decision_tree_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "decision_tree_regression_deciles = np.percentile(decision_tree_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "decision_tree_regression_mean_actual = []\n",
    "decision_tree_regression_mean_predicted = []\n",
    "for i in range(len(decision_tree_regression_deciles) - 1):\n",
    "    mask = (decision_tree_regression_predictions >= decision_tree_regression_deciles[i]) & (decision_tree_regression_predictions < decision_tree_regression_deciles[i + 1])\n",
    "    decision_tree_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    decision_tree_regression_mean_predicted.append(np.mean(decision_tree_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "decision_tree_regression_lift_plot, decision_tree_regression_lift_plot_ax = plt.subplots()\n",
    "decision_tree_regression_lift_plot_ax.bar(np.arange(len(decision_tree_regression_mean_actual)), decision_tree_regression_mean_actual, label='Actual')\n",
    "decision_tree_regression_lift_plot_ax.plot(np.arange(len(decision_tree_regression_mean_predicted)), decision_tree_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "decision_tree_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "decision_tree_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "decision_tree_regression_lift_plot_ax.set_title(f'decision_tree_regression Decile Analysis Chart')\n",
    "decision_tree_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(decision_tree_regression_performance_metrics)##### End of Model Pipeline for Decision Tree Regression #####\n",
    "##### Model Pipeline for GBT Regression #####\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error,make_scorer,r2_score,explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "gbt_regression_param_grid = {\n",
    "\"gbt_regression__n_estimators\": np.arange(25, 200, 50),\n",
    "\"gbt_regression__max_depth\": np.arange(1, 10, 3),\n",
    "\"gbt_regression__alpha\": np.arange(0.1, 1.0, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "gbt_regression_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gbt_regression', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "gbt_regression_grid_search = GridSearchCV(estimator=gbt_regression_pipe, param_grid=gbt_regression_param_grid, cv=5, scoring=make_scorer(mean_squared_error), verbose=3)\n",
    "gbt_regression_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "gbt_regression_best_estimator = gbt_regression_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "gbt_regression_search_results = pd.DataFrame(gbt_regression_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "gbt_regression_predictions = gbt_regression_best_estimator.predict(X_test)\n",
    "gbt_regression_predictions_df = pd.DataFrame(gbt_regression_best_estimator.predict(X_test))x`\n",
    "\n",
    "# Generate Model Metrics\n",
    "gbt_regression_r2_score = r2_score(y_test, gbt_regression_predictions_df.iloc[:,0])\n",
    "gbt_regression_mean_squared_error = mean_squared_error(y_test, gbt_regression_predictions_df.iloc[:,0])\n",
    "gbt_regression_explained_variance_score = explained_variance_score(y_test, gbt_regression_predictions_df.iloc[:,0])\n",
    "gbt_regression_performance_metrics = [['gbt_regression','r2_score', gbt_regression_r2_score], \n",
    "                                  ['gbt_regression','mean_squared_error',gbt_regression_mean_squared_error],\n",
    "                                  ['gbt_regression','explained_variance_score', gbt_regression_explained_variance_score]]\n",
    "gbt_regression_performance_metrics = pd.DataFrame(gbt_regression_performance_metrics, columns=['model','metric', 'value'])\n",
    "\n",
    "# Generate Actual vs Predicted Plot\n",
    "gbt_regression_actual_predicted_plot, gbt_regression_actual_predicted_plot_ax = plt.subplots()\n",
    "gbt_regression_actual_predicted_plot = gbt_regression_actual_predicted_plot_ax.scatter(x=y_test, y=gbt_regression_predictions_df.iloc[:,0], alpha=0.5)\n",
    "# Add diagonal line\n",
    "gbt_regression_actual_predicted_plot_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.5)\n",
    "# Set axis labels and title\n",
    "gbt_regression_actual_predicted_plot_ax.set_xlabel('Actual')\n",
    "gbt_regression_actual_predicted_plot_ax.set_ylabel('Predicted')\n",
    "gbt_regression_actual_predicted_plot_ax.set_title(f'gbt_regression Actual vs. Predicted')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Generate Decile Lift Chart\n",
    "# Calculate the deciles based on the residuals\n",
    "gbt_regression_deciles = np.percentile(gbt_regression_predictions, np.arange(0, 100, 10))\n",
    "# Calculate the mean actual and predicted values for each decile\n",
    "gbt_regression_mean_actual = []\n",
    "gbt_regression_mean_predicted = []\n",
    "for i in range(len(gbt_regression_deciles) - 1):\n",
    "    mask = (gbt_regression_predictions >= gbt_regression_deciles[i]) & (gbt_regression_predictions < gbt_regression_deciles[i + 1])\n",
    "    gbt_regression_mean_actual.append(np.mean(y_test[mask]))\n",
    "    gbt_regression_mean_predicted.append(np.mean(gbt_regression_predictions[mask]))\n",
    "\n",
    "# Create a bar chart of the mean actual and predicted values for each decile\n",
    "gbt_regression_lift_plot, gbt_regression_lift_plot_ax = plt.subplots()\n",
    "gbt_regression_lift_plot_ax.bar(np.arange(len(gbt_regression_mean_actual)), gbt_regression_mean_actual, label='Actual')\n",
    "gbt_regression_lift_plot_ax.plot(np.arange(len(gbt_regression_mean_predicted)), gbt_regression_mean_predicted, color='red', linewidth=2, label='Predicted')\n",
    "gbt_regression_lift_plot_ax.set_xlabel('Deciles')\n",
    "gbt_regression_lift_plot_ax.set_ylabel('Mean')\n",
    "gbt_regression_lift_plot_ax.set_title(f'gbt_regression Decile Analysis Chart')\n",
    "gbt_regression_lift_plot_ax.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "model_comparison_list.append(gbt_regression_performance_metrics)##### End of Model Pipeline for GBT Regression #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'r2_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE CLASSIFICATION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"pypelines/datasets/classification/titanic.csv\")\n",
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Decision Tree Classifier']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision Tree Classifier': {'numerical': [{'search': True,\n",
       "    'name': 'max_depth',\n",
       "    'min': 2,\n",
       "    'max': 10,\n",
       "    'step': 2},\n",
       "   {'search': True,\n",
       "    'name': 'min_samples_split',\n",
       "    'min': 2,\n",
       "    'max': 10,\n",
       "    'step': 2},\n",
       "   {'search': True,\n",
       "    'name': 'min_samples_leaf',\n",
       "    'min': 1,\n",
       "    'max': 10,\n",
       "    'step': 5},\n",
       "   {'search': True,\n",
       "    'name': 'min_weight_fraction_leaf',\n",
       "    'min': 0.0,\n",
       "    'max': 0.5,\n",
       "    'step': 0.25},\n",
       "   {'search': True, 'name': 'max_leaf_nodes', 'min': 1, 'max': 10, 'step': 5},\n",
       "   {'search': True,\n",
       "    'name': 'min_impurity_decrease',\n",
       "    'min': 0.0,\n",
       "    'max': 0.5,\n",
       "    'step': 0.25}],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'criterion',\n",
       "    'selected': ['gini'],\n",
       "    'values': ['gini', 'entropy']},\n",
       "   {'search': False,\n",
       "    'name': 'splitter',\n",
       "    'selected': ['best'],\n",
       "    'values': ['best', 'random']},\n",
       "   {'search': False,\n",
       "    'name': 'max_features',\n",
       "    'selected': ['auto'],\n",
       "    'values': ['auto', 'sqrt', 'log2']}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numerical': [{'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2}, {'search': True, 'name': 'min_samples_split', 'min': 2, 'max': 10, 'step': 2}, {'search': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 5}, {'search': True, 'name': 'min_weight_fraction_leaf', 'min': 0.0, 'max': 0.5, 'step': 0.25}, {'search': True, 'name': 'max_leaf_nodes', 'min': 1, 'max': 10, 'step': 5}, {'search': True, 'name': 'min_impurity_decrease', 'min': 0.0, 'max': 0.5, 'step': 0.25}], 'categorical': [{'search': False, 'name': 'criterion', 'selected': ['gini'], 'values': ['gini', 'entropy']}, {'search': False, 'name': 'splitter', 'selected': ['best'], 'values': ['best', 'random']}, {'search': False, 'name': 'max_features', 'selected': ['auto'], 'values': ['auto', 'sqrt', 'log2']}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(clf_pypelines_all.model_grid_search_settings(model_name='Decision Tree Classifier'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "\n",
      "# target dataframe: titanic\n",
      "target = \"Survived\"\n",
      "features = list(titanic.columns.drop(\"Survived\"))\n",
      "feature_df = titanic[features]\n",
      "\n",
      "prediction_df = titanic\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
      "\n",
      "\n",
      "sample_size = np.min([10000, titanic.shape[0]])\n",
      "unique_theshold = np.min([100, sample_size/10])\n",
      "\n",
      "# check categorical columns for high cardinality and make it text column\n",
      "for col in categorical_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
      "        text_cols.append(col)\n",
      "        categorical_cols.remove(col)\n",
      "        \n",
      "\n",
      "# check text columns for low cardinality and make it categorical columns\n",
      "for col in text_cols:\n",
      "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
      "        categorical_cols.append(col)\n",
      "        text_cols.remove(col)\n",
      "\n",
      "print(numerical_cols)\n",
      "print(categorical_cols)\n",
      "print(text_cols)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", MinMaxScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
      "    ]\n",
      ")\n",
      "\n",
      "# define text transformer steps\n",
      "text_transformer = Pipeline(\n",
      "    steps=[\n",
      "        ('text', TfidfVectorizer())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "        transformers=[('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols),\n",
      "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "X = titanic[features]\n",
      "y = titanic[target]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "model_comparison_list = []\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for Decision Tree Classifier #####\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "import matplotlib.pyplot as plt\n",
      "dt_classifier_param_grid = {\n",
      "\"dt_classifier__max_depth\": np.arange(2, 10, 2),\n",
      "\"dt_classifier__min_samples_split\": np.arange(2, 10, 2),\n",
      "\"dt_classifier__min_samples_leaf\": np.arange(1, 10, 5),\n",
      "\"dt_classifier__min_weight_fraction_leaf\": np.arange(0.0, 0.5, 0.25),\n",
      "\"dt_classifier__max_leaf_nodes\": np.arange(1, 10, 5),\n",
      "\"dt_classifier__min_impurity_decrease\": np.arange(0.0, 0.5, 0.25),\n",
      "}\n",
      "\n",
      "\n",
      "# Create the pipeline\n",
      "dt_classifier_pipe = Pipeline([\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('dt_classifier', DecisionTreeClassifier())\n",
      "])\n",
      "\n",
      "# Create the grid search\n",
      "dt_classifier_grid_search = GridSearchCV(estimator=dt_classifier_pipe, param_grid=dt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
      "dt_classifier_grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "dt_classifier_best_estimator = dt_classifier_grid_search.best_estimator_\n",
      "\n",
      "# Store results as a dataframe  \n",
      "dt_classifier_search_results = pd.DataFrame(dt_classifier_grid_search.cv_results_)\n",
      "\n",
      "# Model metrics\n",
      "\n",
      "# Generate Predictions\n",
      "dt_classifier_predictions = pd.DataFrame(dt_classifier_best_estimator.predict(X_test))\n",
      "\n",
      "dt_classifier_predictions_prob = dt_classifier_best_estimator.predict_proba(X_test)\n",
      "dt_classifier_predictions_prob_df = pd.DataFrame()\n",
      "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[0]] = dt_classifier_predictions_prob[:,0]\n",
      "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]] = dt_classifier_predictions_prob[:,1] \n",
      "\n",
      "\n",
      "# Generate Model Metrics\n",
      "dt_classifier_accuracy = accuracy_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
      "dt_classifier_f1_score = f1_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
      "dt_classifier_precision = precision_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
      "dt_classifier_recall = recall_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
      "dt_classifier_roc_auc_score = roc_auc_score(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
      "dt_classifier_performance_metrics = [['dt_classifier','accuracy',dt_classifier_accuracy], \n",
      "                                  ['dt_classifier','f1_score',dt_classifier_f1_score],\n",
      "                                  ['dt_classifier','precision', dt_classifier_precision],\n",
      "                                  ['dt_classifier','recall', dt_classifier_recall],\n",
      "                                  ['dt_classifier','roc_auc_score', dt_classifier_roc_auc_score]]\n",
      "dt_classifier_performance_metrics = pd.DataFrame(dt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
      "fpr, tpr, thresholds = roc_curve(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
      "roc_auc = auc(fpr, tpr)\n",
      "\n",
      "# ROC Curve plot\n",
      "dt_classifier_roc_auc_plot, dt_classifier_roc_auc_plot_ax = plt.subplots()\n",
      "dt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
      "dt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
      "# Set axis labels and title\n",
      "dt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
      "dt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
      "dt_classifier_roc_auc_plot_ax.set_title(f'dt_classifier ROC Curve')\n",
      "# Add legend\n",
      "dt_classifier_roc_auc_plot_ax.legend()\n",
      "\n",
      "\n",
      "print(dt_classifier_performance_metrics[dt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
      "\n",
      "# Lift Chart\n",
      "aux_df = pd.DataFrame()\n",
      "aux_df['y_real'] = y_test\n",
      "aux_df['y_proba'] = dt_classifier_predictions_prob_df.iloc[:,1].values\n",
      "\n",
      "# Sort by predicted probability\n",
      "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
      "\n",
      "# Find the total positive ratio of the whole dataset\n",
      "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
      "\n",
      "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
      "lift_values = []\n",
      "for i in aux_df.index:\n",
      "    threshold = aux_df.loc[i]['y_proba']\n",
      "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
      "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
      "    lift = subset_positive_ratio / total_positive_ratio\n",
      "    lift_values.append(lift)\n",
      "\n",
      "# Plot the lift curve\n",
      "dt_classifier_lift_plot, dt_classifier_lift_plot_ax = plt.subplots()\n",
      "dt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
      "dt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
      "dt_classifier_lift_plot_ax.set_title(f'dt_classifier Lift Curve')\n",
      "\n",
      "# plot the lift curve\n",
      "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
      "dt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
      "\n",
      "# add dashed horizontal line at lift of 1\n",
      "dt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
      "\n",
      "\n",
      "model_comparison_list.append(dt_classifier_performance_metrics)##### Model Metrics Decision Tree Classifier #####\n",
      "\n",
      "table = pd.concat(model_comparison_list)\n",
      "table = table.sort_values(by=['value'], ascending=False)\n",
      "table = table[table['metric'] == 'roc_auc_score']\n",
      "print(table)\n",
      "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
      "\n",
      "\n",
      "# Predict test data using the best model\n",
      "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
      "print('Predictions from best model are stored in test_predictions')\n",
      "##### End of Model Pipeline for Decision Tree Classifier #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hyperparameter = {\n",
    "    'numerical': [\n",
    "        {'search': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2},\n",
    "        {'search': True, 'name': 'min_samples_split', 'min': 2, 'max': 10, 'step': 2},\n",
    "        {'search': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 5},\n",
    "        {'search': True, 'name': 'min_weight_fraction_leaf', 'min': 0.0, 'max': 0.5, 'step': 0.25},\n",
    "        {'search': True, 'name': 'max_leaf_nodes', 'min': 1, 'max': 10, 'step': 5},\n",
    "        {'search': True, 'name': 'min_impurity_decrease', 'min': 0.0, 'max': 0.5, 'step': 0.25}\n",
    "    ],\n",
    "    'categorical': [\n",
    "        {'search': False, 'name': 'criterion', 'selected': ['gini'], 'values': ['gini', 'entropy']},\n",
    "        {'search': False, 'name': 'splitter', 'selected': ['best'], 'values': ['best', 'random']},\n",
    "        {'search': False, 'name': 'max_features', 'selected': ['auto'], 'values': ['auto', 'sqrt', 'log2']},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(clf_pypelines_all.set_model_grid_search_settings(hyperparam_dict=hyperparameter, model_name='Decision Tree Classifier'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Decision Tree Classifier #####\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "dt_classifier_param_grid = {\n",
    "\"dt_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_split\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_leaf\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_weight_fraction_leaf\": np.arange(0.0, 0.5, 0.25),\n",
    "\"dt_classifier__max_leaf_nodes\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_impurity_decrease\": np.arange(0.0, 0.5, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "dt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "dt_classifier_grid_search = GridSearchCV(estimator=dt_classifier_pipe, param_grid=dt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "dt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "dt_classifier_best_estimator = dt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "dt_classifier_search_results = pd.DataFrame(dt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "dt_classifier_predictions = pd.DataFrame(dt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "dt_classifier_predictions_prob = dt_classifier_best_estimator.predict_proba(X_test)\n",
    "dt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[0]] = dt_classifier_predictions_prob[:,0]\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]] = dt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "dt_classifier_accuracy = accuracy_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_f1_score = f1_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_precision = precision_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_recall = recall_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_roc_auc_score = roc_auc_score(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "dt_classifier_performance_metrics = [['dt_classifier','accuracy',dt_classifier_accuracy], \n",
    "                                  ['dt_classifier','f1_score',dt_classifier_f1_score],\n",
    "                                  ['dt_classifier','precision', dt_classifier_precision],\n",
    "                                  ['dt_classifier','recall', dt_classifier_recall],\n",
    "                                  ['dt_classifier','roc_auc_score', dt_classifier_roc_auc_score]]\n",
    "dt_classifier_performance_metrics = pd.DataFrame(dt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "dt_classifier_roc_auc_plot, dt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "dt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "dt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "dt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_title(f'dt_classifier ROC Curve')\n",
    "# Add legend\n",
    "dt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(dt_classifier_performance_metrics[dt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = dt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "dt_classifier_lift_plot, dt_classifier_lift_plot_ax = plt.subplots()\n",
    "dt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "dt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "dt_classifier_lift_plot_ax.set_title(f'dt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "dt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "dt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(dt_classifier_performance_metrics)##### End of Model Pipeline for Decision Tree Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE CLASSIFICATION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output\n",
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , models = ['Logistic Regression', 'SVC Classifier']\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'numerical': [{'search': True,\n",
       "    'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.1}],\n",
       "  'categorical': [{'search': False,\n",
       "    'name': 'penalty',\n",
       "    'selected': ['l2'],\n",
       "    'values': ['l2', 'elasticnet', 'none']}]},\n",
       " 'SVC Classifier': {'numerical': [{'search': True,\n",
       "    'name': 'C',\n",
       "    'min': 0.1,\n",
       "    'max': 1,\n",
       "    'step': 0.2},\n",
       "   {'search': True, 'name': 'degree', 'min': 2, 'max': 5, 'step': 1}],\n",
       "  'categorical': [{'search': True,\n",
       "    'name': 'kernel',\n",
       "    'selected': ['linear'],\n",
       "    'values': ['linear', 'rbf', 'sigmoid']},\n",
       "   {'search': False,\n",
       "    'name': 'gamma',\n",
       "    'selected': ['scale'],\n",
       "    'values': ['scale', 'auto']},\n",
       "   {'search': True,\n",
       "    'name': 'probability',\n",
       "    'selected': [True],\n",
       "    'values': [True]}]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pypelines_all.get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Logistic Regression #####\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "log_reg_param_grid = {\n",
    "\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
    "\n",
    "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
    "log_reg_predictions_prob_df = pd.DataFrame()\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
    "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
    "                                  ['log_reg','precision', log_reg_precision],\n",
    "                                  ['log_reg','recall', log_reg_recall],\n",
    "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
    "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "log_reg_roc_auc_plot, log_reg_roc_auc_plot_ax = plt.subplots()\n",
    "log_reg_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "log_reg_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "log_reg_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_title(f'log_reg ROC Curve')\n",
    "# Add legend\n",
    "log_reg_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(log_reg_performance_metrics[log_reg_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = log_reg_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "log_reg_lift_plot, log_reg_lift_plot_ax = plt.subplots()\n",
    "log_reg_lift_plot_ax.set_xlabel('Proportion')\n",
    "log_reg_lift_plot_ax.set_ylabel('Lift')\n",
    "log_reg_lift_plot_ax.set_title(f'log_reg Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "log_reg_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "log_reg_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(log_reg_performance_metrics)##### End of Model Pipeline for Logistic Regression #####\n",
    "##### Model Pipeline for SVC Classifier #####\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "svc_classifier_param_grid = {\n",
    "\"svc_classifier__C\": np.arange(0.1, 1.0, 0.2),\n",
    "\"svc_classifier__degree\": np.arange(2, 5, 1),\n",
    "\"svc_classifier__kernel\": ['linear'],\n",
    "\"svc_classifier__probability\": [True],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "svc_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc_classifier', SVC())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "svc_classifier_grid_search = GridSearchCV(estimator=svc_classifier_pipe, param_grid=svc_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "svc_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "svc_classifier_best_estimator = svc_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "svc_classifier_search_results = pd.DataFrame(svc_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "svc_classifier_predictions = pd.DataFrame(svc_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "svc_classifier_predictions_prob = svc_classifier_best_estimator.predict_proba(X_test)\n",
    "svc_classifier_predictions_prob_df = pd.DataFrame()\n",
    "svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[0]] = svc_classifier_predictions_prob[:,0]\n",
    "svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]] = svc_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "svc_classifier_accuracy = accuracy_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_f1_score = f1_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_precision = precision_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_recall = recall_score(y_test, svc_classifier_predictions.iloc[:,0])\n",
    "svc_classifier_roc_auc_score = roc_auc_score(y_test, svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]])\n",
    "svc_classifier_performance_metrics = [['svc_classifier','accuracy',svc_classifier_accuracy], \n",
    "                                  ['svc_classifier','f1_score',svc_classifier_f1_score],\n",
    "                                  ['svc_classifier','precision', svc_classifier_precision],\n",
    "                                  ['svc_classifier','recall', svc_classifier_recall],\n",
    "                                  ['svc_classifier','roc_auc_score', svc_classifier_roc_auc_score]]\n",
    "svc_classifier_performance_metrics = pd.DataFrame(svc_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc_classifier_predictions_prob_df[svc_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "svc_classifier_roc_auc_plot, svc_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "svc_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "svc_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "svc_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "svc_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "svc_classifier_roc_auc_plot_ax.set_title(f'svc_classifier ROC Curve')\n",
    "# Add legend\n",
    "svc_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(svc_classifier_performance_metrics[svc_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = svc_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "svc_classifier_lift_plot, svc_classifier_lift_plot_ax = plt.subplots()\n",
    "svc_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "svc_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "svc_classifier_lift_plot_ax.set_title(f'svc_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "svc_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "svc_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(svc_classifier_performance_metrics)##### End of Model Pipeline for SVC Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION MODEL - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all = pipe.SupervisedPipeline(data = titanic,target = 'Survived',predictions_data=titanic\n",
    "                            , model_type = 'classification'\n",
    "                            , nfolds = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Decision Tree Classifier', 'Logistic Regression', 'Random Forest Classifier', 'XGBoost Classifier', 'GBT Classifier']\n"
     ]
    }
   ],
   "source": [
    "clf_pypelines_all.get_hyperparameters()\n",
    "clf_pypelines_all.model_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for classification default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: titanic\n",
    "target = \"Survived\"\n",
    "features = list(titanic.columns.drop(\"Survived\"))\n",
    "feature_df = titanic[features]\n",
    "\n",
    "prediction_df = titanic\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "titanic[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "text_cols = feature_df.select_dtypes(include=['string']).columns.tolist()\n",
    "\n",
    "\n",
    "sample_size = np.min([10000, titanic.shape[0]])\n",
    "unique_theshold = np.min([100, sample_size/10])\n",
    "\n",
    "# check categorical columns for high cardinality and make it text column\n",
    "for col in categorical_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() > unique_theshold:\n",
    "        text_cols.append(col)\n",
    "        categorical_cols.remove(col)\n",
    "        \n",
    "\n",
    "# check text columns for low cardinality and make it categorical columns\n",
    "for col in text_cols:\n",
    "    if titanic[col].sample(sample_size).nunique() < unique_theshold:\n",
    "        categorical_cols.append(col)\n",
    "        text_cols.remove(col)\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "print(text_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define text transformer steps\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('text', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        *[(f'text_{t_col}', text_transformer, t_col) for t_col in text_cols]]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X = titanic[features]\n",
    "y = titanic[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Decision Tree Classifier #####\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "dt_classifier_param_grid = {\n",
    "\"dt_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_split\": np.arange(2, 10, 2),\n",
    "\"dt_classifier__min_samples_leaf\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_weight_fraction_leaf\": np.arange(0.0, 0.5, 0.25),\n",
    "\"dt_classifier__max_leaf_nodes\": np.arange(1, 10, 5),\n",
    "\"dt_classifier__min_impurity_decrease\": np.arange(0.0, 0.5, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "dt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "dt_classifier_grid_search = GridSearchCV(estimator=dt_classifier_pipe, param_grid=dt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "dt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "dt_classifier_best_estimator = dt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "dt_classifier_search_results = pd.DataFrame(dt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "dt_classifier_predictions = pd.DataFrame(dt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "dt_classifier_predictions_prob = dt_classifier_best_estimator.predict_proba(X_test)\n",
    "dt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[0]] = dt_classifier_predictions_prob[:,0]\n",
    "dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]] = dt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "dt_classifier_accuracy = accuracy_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_f1_score = f1_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_precision = precision_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_recall = recall_score(y_test, dt_classifier_predictions.iloc[:,0])\n",
    "dt_classifier_roc_auc_score = roc_auc_score(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "dt_classifier_performance_metrics = [['dt_classifier','accuracy',dt_classifier_accuracy], \n",
    "                                  ['dt_classifier','f1_score',dt_classifier_f1_score],\n",
    "                                  ['dt_classifier','precision', dt_classifier_precision],\n",
    "                                  ['dt_classifier','recall', dt_classifier_recall],\n",
    "                                  ['dt_classifier','roc_auc_score', dt_classifier_roc_auc_score]]\n",
    "dt_classifier_performance_metrics = pd.DataFrame(dt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, dt_classifier_predictions_prob_df[dt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "dt_classifier_roc_auc_plot, dt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "dt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "dt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "dt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "dt_classifier_roc_auc_plot_ax.set_title(f'dt_classifier ROC Curve')\n",
    "# Add legend\n",
    "dt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(dt_classifier_performance_metrics[dt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = dt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "dt_classifier_lift_plot, dt_classifier_lift_plot_ax = plt.subplots()\n",
    "dt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "dt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "dt_classifier_lift_plot_ax.set_title(f'dt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "dt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "dt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(dt_classifier_performance_metrics)##### End of Model Pipeline for Decision Tree Classifier #####\n",
    "##### Model Pipeline for Logistic Regression #####\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "log_reg_param_grid = {\n",
    "\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "log_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "log_reg_best_estimator = log_reg_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "log_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
    "\n",
    "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
    "log_reg_predictions_prob_df = pd.DataFrame()\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
    "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_f1_score = f1_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_precision = precision_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_recall = recall_score(y_test, log_reg_predictions.iloc[:,0])\n",
    "log_reg_roc_auc_score = roc_auc_score(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
    "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
    "                                  ['log_reg','precision', log_reg_precision],\n",
    "                                  ['log_reg','recall', log_reg_recall],\n",
    "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
    "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "log_reg_roc_auc_plot, log_reg_roc_auc_plot_ax = plt.subplots()\n",
    "log_reg_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "log_reg_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "log_reg_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "log_reg_roc_auc_plot_ax.set_title(f'log_reg ROC Curve')\n",
    "# Add legend\n",
    "log_reg_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(log_reg_performance_metrics[log_reg_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = log_reg_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "log_reg_lift_plot, log_reg_lift_plot_ax = plt.subplots()\n",
    "log_reg_lift_plot_ax.set_xlabel('Proportion')\n",
    "log_reg_lift_plot_ax.set_ylabel('Lift')\n",
    "log_reg_lift_plot_ax.set_title(f'log_reg Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "log_reg_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "log_reg_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(log_reg_performance_metrics)##### End of Model Pipeline for Logistic Regression #####\n",
    "##### Model Pipeline for Random Forest Classifier #####\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "random_forest_classifier_param_grid = {\n",
    "\"random_forest_classifier__n_estimators\": np.arange(10, 100, 20),\n",
    "\"random_forest_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"random_forest_classifier__min_samples_split\": np.arange(0.5, 1.0, 0.1),\n",
    "\"random_forest_classifier__min_samples_leaf\": np.arange(1, 10, 2),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "random_forest_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest_classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "random_forest_classifier_grid_search = GridSearchCV(estimator=random_forest_classifier_pipe, param_grid=random_forest_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "random_forest_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "random_forest_classifier_best_estimator = random_forest_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "random_forest_classifier_search_results = pd.DataFrame(random_forest_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "random_forest_classifier_predictions = pd.DataFrame(random_forest_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "random_forest_classifier_predictions_prob = random_forest_classifier_best_estimator.predict_proba(X_test)\n",
    "random_forest_classifier_predictions_prob_df = pd.DataFrame()\n",
    "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[0]] = random_forest_classifier_predictions_prob[:,0]\n",
    "random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]] = random_forest_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "random_forest_classifier_accuracy = accuracy_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_f1_score = f1_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_precision = precision_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_recall = recall_score(y_test, random_forest_classifier_predictions.iloc[:,0])\n",
    "random_forest_classifier_roc_auc_score = roc_auc_score(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
    "random_forest_classifier_performance_metrics = [['random_forest_classifier','accuracy',random_forest_classifier_accuracy], \n",
    "                                  ['random_forest_classifier','f1_score',random_forest_classifier_f1_score],\n",
    "                                  ['random_forest_classifier','precision', random_forest_classifier_precision],\n",
    "                                  ['random_forest_classifier','recall', random_forest_classifier_recall],\n",
    "                                  ['random_forest_classifier','roc_auc_score', random_forest_classifier_roc_auc_score]]\n",
    "random_forest_classifier_performance_metrics = pd.DataFrame(random_forest_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, random_forest_classifier_predictions_prob_df[random_forest_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "random_forest_classifier_roc_auc_plot, random_forest_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "random_forest_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "random_forest_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "random_forest_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "random_forest_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "random_forest_classifier_roc_auc_plot_ax.set_title(f'random_forest_classifier ROC Curve')\n",
    "# Add legend\n",
    "random_forest_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(random_forest_classifier_performance_metrics[random_forest_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = random_forest_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "random_forest_classifier_lift_plot, random_forest_classifier_lift_plot_ax = plt.subplots()\n",
    "random_forest_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "random_forest_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "random_forest_classifier_lift_plot_ax.set_title(f'random_forest_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "random_forest_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "random_forest_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(random_forest_classifier_performance_metrics)##### End of Model Pipeline for Random Forest Classifier #####\n",
    "##### Model Pipeline for XGBoost Classifier #####\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "xgboost_classifier_param_grid = {\n",
    "\"xgboost_classifier__learning_rate\": np.arange(0.1, 1.0, 0.25),\n",
    "\"xgboost_classifier__n_estimators\": np.arange(100, 500, 100),\n",
    "\"xgboost_classifier__max_depth\": np.arange(2, 10, 2),\n",
    "\"xgboost_classifier__gamma\": np.arange(0.0, 0.5, 0.25),\n",
    "\"xgboost_classifier__subsample\": np.arange(0.1, 1.0, 0.25),\n",
    "\"xgboost_classifier__colsample_bytree\": np.arange(0.5, 1.0, 0.25),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "xgboost_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgboost_classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "xgboost_classifier_grid_search = GridSearchCV(estimator=xgboost_classifier_pipe, param_grid=xgboost_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "xgboost_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "xgboost_classifier_best_estimator = xgboost_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "xgboost_classifier_search_results = pd.DataFrame(xgboost_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "xgboost_classifier_predictions = pd.DataFrame(xgboost_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "xgboost_classifier_predictions_prob = xgboost_classifier_best_estimator.predict_proba(X_test)\n",
    "xgboost_classifier_predictions_prob_df = pd.DataFrame()\n",
    "xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[0]] = xgboost_classifier_predictions_prob[:,0]\n",
    "xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]] = xgboost_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "xgboost_classifier_accuracy = accuracy_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_f1_score = f1_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_precision = precision_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_recall = recall_score(y_test, xgboost_classifier_predictions.iloc[:,0])\n",
    "xgboost_classifier_roc_auc_score = roc_auc_score(y_test, xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]])\n",
    "xgboost_classifier_performance_metrics = [['xgboost_classifier','accuracy',xgboost_classifier_accuracy], \n",
    "                                  ['xgboost_classifier','f1_score',xgboost_classifier_f1_score],\n",
    "                                  ['xgboost_classifier','precision', xgboost_classifier_precision],\n",
    "                                  ['xgboost_classifier','recall', xgboost_classifier_recall],\n",
    "                                  ['xgboost_classifier','roc_auc_score', xgboost_classifier_roc_auc_score]]\n",
    "xgboost_classifier_performance_metrics = pd.DataFrame(xgboost_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgboost_classifier_predictions_prob_df[xgboost_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "xgboost_classifier_roc_auc_plot, xgboost_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "xgboost_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "xgboost_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "xgboost_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "xgboost_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "xgboost_classifier_roc_auc_plot_ax.set_title(f'xgboost_classifier ROC Curve')\n",
    "# Add legend\n",
    "xgboost_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(xgboost_classifier_performance_metrics[xgboost_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = xgboost_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "xgboost_classifier_lift_plot, xgboost_classifier_lift_plot_ax = plt.subplots()\n",
    "xgboost_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "xgboost_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "xgboost_classifier_lift_plot_ax.set_title(f'xgboost_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "xgboost_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "xgboost_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(xgboost_classifier_performance_metrics)##### End of Model Pipeline for XGBoost Classifier #####\n",
    "##### Model Pipeline for GBT Classifier #####\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "gbt_classifier_param_grid = {\n",
    "\"gbt_classifier__learning_rate\": np.arange(0.0, 1.0, 0.2),\n",
    "\"gbt_classifier__n_estimators\": np.arange(100, 10000, 1000),\n",
    "\"gbt_classifier__subsample\": np.arange(0.1, 1.0, 0.2),\n",
    "\"gbt_classifier__max_depth\": np.arange(1, 10000, 1000),\n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "gbt_classifier_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gbt_classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "gbt_classifier_grid_search = GridSearchCV(estimator=gbt_classifier_pipe, param_grid=gbt_classifier_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=3)\n",
    "gbt_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "gbt_classifier_best_estimator = gbt_classifier_grid_search.best_estimator_\n",
    "\n",
    "# Store results as a dataframe  \n",
    "gbt_classifier_search_results = pd.DataFrame(gbt_classifier_grid_search.cv_results_)\n",
    "\n",
    "# Model metrics\n",
    "\n",
    "# Generate Predictions\n",
    "gbt_classifier_predictions = pd.DataFrame(gbt_classifier_best_estimator.predict(X_test))\n",
    "\n",
    "gbt_classifier_predictions_prob = gbt_classifier_best_estimator.predict_proba(X_test)\n",
    "gbt_classifier_predictions_prob_df = pd.DataFrame()\n",
    "gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[0]] = gbt_classifier_predictions_prob[:,0]\n",
    "gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]] = gbt_classifier_predictions_prob[:,1] \n",
    "\n",
    "\n",
    "# Generate Model Metrics\n",
    "gbt_classifier_accuracy = accuracy_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_f1_score = f1_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_precision = precision_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_recall = recall_score(y_test, gbt_classifier_predictions.iloc[:,0])\n",
    "gbt_classifier_roc_auc_score = roc_auc_score(y_test, gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]])\n",
    "gbt_classifier_performance_metrics = [['gbt_classifier','accuracy',gbt_classifier_accuracy], \n",
    "                                  ['gbt_classifier','f1_score',gbt_classifier_f1_score],\n",
    "                                  ['gbt_classifier','precision', gbt_classifier_precision],\n",
    "                                  ['gbt_classifier','recall', gbt_classifier_recall],\n",
    "                                  ['gbt_classifier','roc_auc_score', gbt_classifier_roc_auc_score]]\n",
    "gbt_classifier_performance_metrics = pd.DataFrame(gbt_classifier_performance_metrics, columns=['model','metric', 'value'])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gbt_classifier_predictions_prob_df[gbt_classifier_grid_search.classes_[1]])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve plot\n",
    "gbt_classifier_roc_auc_plot, gbt_classifier_roc_auc_plot_ax = plt.subplots()\n",
    "gbt_classifier_roc_auc_plot_ax.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "gbt_classifier_roc_auc_plot_ax.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "# Set axis labels and title\n",
    "gbt_classifier_roc_auc_plot_ax.set_xlabel('False Positive Rate')\n",
    "gbt_classifier_roc_auc_plot_ax.set_ylabel('True Positive Rate')\n",
    "gbt_classifier_roc_auc_plot_ax.set_title(f'gbt_classifier ROC Curve')\n",
    "# Add legend\n",
    "gbt_classifier_roc_auc_plot_ax.legend()\n",
    "\n",
    "\n",
    "print(gbt_classifier_performance_metrics[gbt_classifier_performance_metrics['metric'] == 'roc_auc_score'])\n",
    "\n",
    "# Lift Chart\n",
    "aux_df = pd.DataFrame()\n",
    "aux_df['y_real'] = y_test\n",
    "aux_df['y_proba'] = gbt_classifier_predictions_prob_df.iloc[:,1].values\n",
    "\n",
    "# Sort by predicted probability\n",
    "aux_df = aux_df.sort_values('y_proba', ascending=False)\n",
    "\n",
    "# Find the total positive ratio of the whole dataset\n",
    "total_positive_ratio = sum(aux_df['y_real'] == 1) / aux_df.shape[0]\n",
    "\n",
    "# For each line of data, get the ratio of positives of the given subset and calculate the lift\n",
    "lift_values = []\n",
    "for i in aux_df.index:\n",
    "    threshold = aux_df.loc[i]['y_proba']\n",
    "    subset = aux_df[aux_df['y_proba'] >= threshold]\n",
    "    subset_positive_ratio = sum(subset['y_real'] == 1) / subset.shape[0]\n",
    "    lift = subset_positive_ratio / total_positive_ratio\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Plot the lift curve\n",
    "gbt_classifier_lift_plot, gbt_classifier_lift_plot_ax = plt.subplots()\n",
    "gbt_classifier_lift_plot_ax.set_xlabel('Proportion')\n",
    "gbt_classifier_lift_plot_ax.set_ylabel('Lift')\n",
    "gbt_classifier_lift_plot_ax.set_title(f'gbt_classifier Lift Curve')\n",
    "\n",
    "# plot the lift curve\n",
    "x_vals = np.linspace(0, 1, num=len(lift_values))\n",
    "gbt_classifier_lift_plot_ax.plot(x_vals, lift_values, color='b')\n",
    "\n",
    "# add dashed horizontal line at lift of 1\n",
    "gbt_classifier_lift_plot_ax.axhline(y=1, color='gray', linestyle='--', linewidth=3)\n",
    "\n",
    "\n",
    "model_comparison_list.append(gbt_classifier_performance_metrics)##### End of Model Pipeline for GBT Classifier #####\n",
    "##### Model Comparison #####\n",
    "\n",
    "table = pd.concat(model_comparison_list)\n",
    "table = table.sort_values(by=['value'], ascending=False)\n",
    "table = table[table['metric'] == 'roc_auc_score']\n",
    "print(table)\n",
    "print(f\"The best model is {table['model'].iloc[0]} with {table['value'].iloc[0]} as {table['metric'].iloc[0]}\")\n",
    "\n",
    "\n",
    "# Predict test data using the best model\n",
    "test_predictions = eval(table['model'].iloc[0]+\"_best_estimator\").predict(prediction_df)\n",
    "print('Predictions from best model are stored in test_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOMALY DETECTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE ANOMALY DETECTION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypelines.anomaly_detection_pipeline as pipe \n",
    "from pypelines import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vehicle_claims = pd.read_csv(\"pypelines/datasets/anomaly_detection/vehicle_claims.csv\")\n",
    "\n",
    "ad_pypelines_all = pipe.AnomalyDetectionPipeline(data = vehicle_claims,\n",
    "                                                 predictions_data= vehicle_claims\n",
    "                                                ,models = ['LUNAR']\n",
    "                                               , nfolds = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numerical': [{'search': False, 'val_size': 'contamination', 'min': 0, 'max': 1, 'step': 0.1}], 'categorical': [{'search': False, 'name': 'model_type', 'selected': ['WEIGHT'], 'values': ['WEIGHT', 'SCORE']}]}\n"
     ]
    }
   ],
   "source": [
    "print(ad_pypelines_all.model_grid_search_settings(model_name='LUNAR'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from pyod.models import *\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import make_scorer\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.preprocessing import MinMaxScaler,OrdinalEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "\n",
      "# target dataframe: vehicle_claims\n",
      "# target = \"\"\n",
      "features = list(vehicle_claims.columns)\n",
      "feature_df = vehicle_claims[features]\n",
      "\n",
      "prediction_df = vehicle_claims\n",
      "\n",
      "# get numerical and categorical columns\n",
      "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
      "vehicle_claims[bool_cols] = feature_df[bool_cols].astype(int)\n",
      "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
      "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
      "\n",
      "print(numerical_cols)\n",
      "print(categorical_cols)\n",
      "\n",
      "# define numeric transformer steps\n",
      "numeric_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
      "        (\"scaler\", MinMaxScaler())]\n",
      ")\n",
      "\n",
      "# define categorical transformer steps\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
      "        (\"encoder\", OrdinalEncoder())\n",
      "    ]\n",
      ")\n",
      "\n",
      "# create the preprocessing pipelines for both numeric and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "        transformers=[('num', numeric_transformer , numerical_cols),\n",
      "        ('cat', categorical_transformer, categorical_cols)]\n",
      ")\n",
      "\n",
      "# train test split\n",
      "x_train = vehicle_claims[features]\n",
      "\n",
      "x_train_preprocessed = preprocessor.fit_transform(x_train)\n",
      "y_train_preprocessed = preprocessor.fit_transform(prediction_df)\n",
      "\n",
      "model_comparison_list = []\n",
      "\n",
      "##### End of Data Processing Pipeline #####\n",
      "\n",
      "\n",
      "\n",
      "##### Model Pipeline for LUNAR #####\n",
      "\n",
      "from pyod.models.lunar import LUNAR\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
      "      return 1\n",
      "\n",
      "\n",
      "LUNAR_param_grid = {\n",
      "}\n",
      "\n",
      "\n",
      "LUNAR_model = LUNAR()\n",
      "\n",
      "# Perform grid search with cross-validation\n",
      "LUNAR_grid_search = GridSearchCV(LUNAR_model,\n",
      "                                      LUNAR_param_grid, \n",
      "                                      scoring=scorer_f, \n",
      "                                      verbose=3)\n",
      "LUNAR_grid_search.fit(x_train_preprocessed)\n",
      "\n",
      "# Get the best hyperparameters\n",
      "LUNAR_best_estimator = LUNAR_grid_search.best_estimator_\n",
      "\n",
      "# get the prediction labels and outlier scores of the training data\n",
      "LUNAR_y_train_pred = LUNAR_best_estimator.labels_ \n",
      "LUNAR_y_train_scores = LUNAR_best_estimator.decision_scores_\n",
      "\n",
      "# get the prediction on the test data\n",
      "LUNAR_y_test_pred = LUNAR_best_estimator.predict(y_train_preprocessed)\n",
      "LUNAR_y_test_pred_proba = LUNAR_best_estimator.predict_proba(y_train_preprocessed) \n",
      "LUNAR_y_test_scores = LUNAR_best_estimator.decision_function(y_train_preprocessed) \n",
      "##### Model Metrics LUNAR #####\n",
      "\n",
      "##### End of Model Pipeline for LUNAR #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hyperparameter = {\n",
    "    'numerical': [\n",
    "        {'search': False, 'val_size': 'contamination', 'min': 0, 'max': 1, 'step': 0.1},\n",
    "    ],\n",
    "    'categorical': [\n",
    "        {'search': False, 'name': 'model_type', 'selected': ['WEIGHT'], 'values': ['WEIGHT', 'SCORE']}\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "print(ad_pypelines_all.set_model_grid_search_settings(hyperparam_dict=hyperparameter, model_name='LUNAR'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for single anomaly detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyod.models import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler,OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: vehicle_claims\n",
    "# target = \"\"\n",
    "features = list(vehicle_claims.columns)\n",
    "feature_df = vehicle_claims[features]\n",
    "\n",
    "prediction_df = vehicle_claims\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "vehicle_claims[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "x_train = vehicle_claims[features]\n",
    "\n",
    "x_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "y_train_preprocessed = preprocessor.fit_transform(prediction_df)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for LUNAR #####\n",
    "\n",
    "from pyod.models.lunar import LUNAR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "LUNAR_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "LUNAR_model = LUNAR()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "LUNAR_grid_search = GridSearchCV(LUNAR_model,\n",
    "                                      LUNAR_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "LUNAR_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "LUNAR_best_estimator = LUNAR_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "LUNAR_y_train_pred = LUNAR_best_estimator.labels_ \n",
    "LUNAR_y_train_scores = LUNAR_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "LUNAR_y_test_pred = LUNAR_best_estimator.predict(y_train_preprocessed)\n",
    "LUNAR_y_test_pred_proba = LUNAR_best_estimator.predict_proba(y_train_preprocessed) \n",
    "LUNAR_y_test_scores = LUNAR_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for LUNAR #####\n",
    "##### Model Comparison #####\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE ANOMALY DETECTION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pypelines_all = pipe.AnomalyDetectionPipeline(data = vehicle_claims,\n",
    "                                                 predictions_data= vehicle_claims\n",
    "                                              , models = ['R Graph','Principal Component Analysis']\n",
    "                                               , nfolds = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code for multiple anomaly detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyod.models import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler,OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: vehicle_claims\n",
    "# target = \"\"\n",
    "features = list(vehicle_claims.columns)\n",
    "feature_df = vehicle_claims[features]\n",
    "\n",
    "prediction_df = vehicle_claims\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "vehicle_claims[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "x_train = vehicle_claims[features]\n",
    "\n",
    "x_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "y_train_preprocessed = preprocessor.fit_transform(prediction_df)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for Principal Component Analysis #####\n",
    "\n",
    "from pyod.models.pca import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "PCA_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"svd_solver\": ['auto'],\n",
    "}\n",
    "\n",
    "\n",
    "PCA_model = PCA()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "PCA_grid_search = GridSearchCV(PCA_model,\n",
    "                                      PCA_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "PCA_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "PCA_best_estimator = PCA_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "PCA_y_train_pred = PCA_best_estimator.labels_ \n",
    "PCA_y_train_scores = PCA_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "PCA_y_test_pred = PCA_best_estimator.predict(y_train_preprocessed)\n",
    "PCA_y_test_pred_proba = PCA_best_estimator.predict_proba(y_train_preprocessed) \n",
    "PCA_y_test_scores = PCA_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Principal Component Analysis #####\n",
    "##### Model Pipeline for R Graph #####\n",
    "\n",
    "from pyod.models.rgraph import RGraph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "RGraph_param_grid = {\n",
    "\"maxiter_lasso\": np.arange(100, 1000, 1000),\n",
    "\"transition_steps\": np.arange(10, 20, 15),\n",
    "\"verbose\": [1],\n",
    "}\n",
    "\n",
    "\n",
    "RGraph_model = RGraph()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "RGraph_grid_search = GridSearchCV(RGraph_model,\n",
    "                                      RGraph_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "RGraph_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "RGraph_best_estimator = RGraph_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "RGraph_y_train_pred = RGraph_best_estimator.labels_ \n",
    "RGraph_y_train_scores = RGraph_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "RGraph_y_test_pred = RGraph_best_estimator.predict(y_train_preprocessed)\n",
    "RGraph_y_test_pred_proba = RGraph_best_estimator.predict_proba(y_train_preprocessed) \n",
    "RGraph_y_test_scores = RGraph_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for R Graph #####\n",
    "##### Model Comparison #####\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOMALY DETECTION - DEFAULT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pypelines_all = pipe.AnomalyDetectionPipeline(data = vehicle_claims,\n",
    "                                                 predictions_data= vehicle_claims,\n",
    "                                               nfolds = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pypelines_all.code_to_clipboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tranining code generation for anomaly detection default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyod.models import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler,OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# target dataframe: vehicle_claims\n",
    "# target = \"\"\n",
    "features = list(vehicle_claims.columns)\n",
    "feature_df = vehicle_claims[features]\n",
    "\n",
    "prediction_df = vehicle_claims\n",
    "\n",
    "# get numerical and categorical columns\n",
    "bool_cols = feature_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "vehicle_claims[bool_cols] = feature_df[bool_cols].astype(int)\n",
    "numerical_cols = feature_df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "categorical_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)\n",
    "\n",
    "# define numeric transformer steps\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# define categorical transformer steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the preprocessing pipelines for both numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer , numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "x_train = vehicle_claims[features]\n",
    "\n",
    "x_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "y_train_preprocessed = preprocessor.fit_transform(prediction_df)\n",
    "\n",
    "model_comparison_list = []\n",
    "\n",
    "##### End of Data Processing Pipeline #####\n",
    "\n",
    "\n",
    "##### Model Pipeline for ABOD Anomaly Detection #####\n",
    "\n",
    "from pyod.models.abod import ABOD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "ABOD_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "ABOD_model = ABOD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "ABOD_grid_search = GridSearchCV(ABOD_model,\n",
    "                                      ABOD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "ABOD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ABOD_best_estimator = ABOD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "ABOD_y_train_pred = ABOD_best_estimator.labels_ \n",
    "ABOD_y_train_scores = ABOD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "ABOD_y_test_pred = ABOD_best_estimator.predict(y_train_preprocessed)\n",
    "ABOD_y_test_pred_proba = ABOD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "ABOD_y_test_scores = ABOD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for ABOD Anomaly Detection #####\n",
    "##### Model Pipeline for ALAD Anomaly Detection #####\n",
    "\n",
    "from pyod.models.alad import ALAD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "ALAD_param_grid = {\n",
    "\"epochs\": np.arange(100, 1000, 100),\n",
    "\"batch_size\": np.arange(100, 1000, 100),\n",
    "\"dropout_rate\": np.arange(0.01, 1.0, 0.2),\n",
    "\"activation_hidden_gen\": ['relu'],\n",
    "\"activation_hidden_disc\": ['relu'],\n",
    "}\n",
    "\n",
    "\n",
    "ALAD_model = ALAD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "ALAD_grid_search = GridSearchCV(ALAD_model,\n",
    "                                      ALAD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "ALAD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ALAD_best_estimator = ALAD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "ALAD_y_train_pred = ALAD_best_estimator.labels_ \n",
    "ALAD_y_train_scores = ALAD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "ALAD_y_test_pred = ALAD_best_estimator.predict(y_train_preprocessed)\n",
    "ALAD_y_test_pred_proba = ALAD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "ALAD_y_test_scores = ALAD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for ALAD Anomaly Detection #####\n",
    "##### Model Pipeline for AnoGAN Anomaly Detection #####\n",
    "\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "AnoGAN_param_grid = {\n",
    "\"epochs\": np.arange(100, 1000, 100),\n",
    "\"batch_size\": np.arange(100, 1000, 100),\n",
    "\"dropout_rate\": np.arange(0.01, 1.0, 0.2),\n",
    "\"output_activation\": ['relu'],\n",
    "\"activation_hidden\": ['tanh'],\n",
    "}\n",
    "\n",
    "\n",
    "AnoGAN_model = AnoGAN()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "AnoGAN_grid_search = GridSearchCV(AnoGAN_model,\n",
    "                                      AnoGAN_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "AnoGAN_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "AnoGAN_best_estimator = AnoGAN_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "AnoGAN_y_train_pred = AnoGAN_best_estimator.labels_ \n",
    "AnoGAN_y_train_scores = AnoGAN_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "AnoGAN_y_test_pred = AnoGAN_best_estimator.predict(y_train_preprocessed)\n",
    "AnoGAN_y_test_pred_proba = AnoGAN_best_estimator.predict_proba(y_train_preprocessed) \n",
    "AnoGAN_y_test_scores = AnoGAN_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for AnoGAN Anomaly Detection #####\n",
    "##### Model Pipeline for Clustering Based Local Outlier Factor #####\n",
    "\n",
    "from pyod.models.cblof import CBLOF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "CBLOF_param_grid = {\n",
    "\"n_clusters\": np.arange(2, 20, 2),\n",
    "\"alpha\": np.arange(0.5, 1.0, 0.1),\n",
    "\"beta\": np.arange(1.1, 100.0, 20.0),\n",
    "}\n",
    "\n",
    "\n",
    "CBLOF_model = CBLOF()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "CBLOF_grid_search = GridSearchCV(CBLOF_model,\n",
    "                                      CBLOF_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "CBLOF_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "CBLOF_best_estimator = CBLOF_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "CBLOF_y_train_pred = CBLOF_best_estimator.labels_ \n",
    "CBLOF_y_train_scores = CBLOF_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "CBLOF_y_test_pred = CBLOF_best_estimator.predict(y_train_preprocessed)\n",
    "CBLOF_y_test_pred_proba = CBLOF_best_estimator.predict_proba(y_train_preprocessed) \n",
    "CBLOF_y_test_scores = CBLOF_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Clustering Based Local Outlier Factor #####\n",
    "##### Model Pipeline for Connectivity-Based Outlier Factor #####\n",
    "\n",
    "from pyod.models.cof import COF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "COF_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "COF_model = COF()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "COF_grid_search = GridSearchCV(COF_model,\n",
    "                                      COF_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "COF_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "COF_best_estimator = COF_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "COF_y_train_pred = COF_best_estimator.labels_ \n",
    "COF_y_train_scores = COF_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "COF_y_test_pred = COF_best_estimator.predict(y_train_preprocessed)\n",
    "COF_y_test_pred_proba = COF_best_estimator.predict_proba(y_train_preprocessed) \n",
    "COF_y_test_scores = COF_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Connectivity-Based Outlier Factor #####\n",
    "##### Model Pipeline for Cooks Distance Outlier Factor #####\n",
    "\n",
    "from pyod.models.cd import CD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "CD_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "CD_model = CD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "CD_grid_search = GridSearchCV(CD_model,\n",
    "                                      CD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "CD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "CD_best_estimator = CD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "CD_y_train_pred = CD_best_estimator.labels_ \n",
    "CD_y_train_scores = CD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "CD_y_test_pred = CD_best_estimator.predict(y_train_preprocessed)\n",
    "CD_y_test_pred_proba = CD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "CD_y_test_scores = CD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Cooks Distance Outlier Factor #####\n",
    "##### Model Pipeline for Copula Based Outlier Detector #####\n",
    "\n",
    "from pyod.models.copod import COPOD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "COPOD_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "COPOD_model = COPOD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "COPOD_grid_search = GridSearchCV(COPOD_model,\n",
    "                                      COPOD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "COPOD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "COPOD_best_estimator = COPOD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "COPOD_y_train_pred = COPOD_best_estimator.labels_ \n",
    "COPOD_y_train_scores = COPOD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "COPOD_y_test_pred = COPOD_best_estimator.predict(y_train_preprocessed)\n",
    "COPOD_y_test_pred_proba = COPOD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "COPOD_y_test_scores = COPOD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Copula Based Outlier Detector #####\n",
    "##### Model Pipeline for Deep One-Class Classification #####\n",
    "\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "DeepSVDD_param_grid = {\n",
    "\"epochs\": np.arange(100, 1000, 100),\n",
    "\"batch_size\": np.arange(100, 1000, 100),\n",
    "\"dropout_rate\": np.arange(0.01, 1.0, 0.2),\n",
    "\"hidden_activation\": ['relu'],\n",
    "\"output_activation\": ['sigmoid'],\n",
    "}\n",
    "\n",
    "\n",
    "DeepSVDD_model = DeepSVDD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "DeepSVDD_grid_search = GridSearchCV(DeepSVDD_model,\n",
    "                                      DeepSVDD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "DeepSVDD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "DeepSVDD_best_estimator = DeepSVDD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "DeepSVDD_y_train_pred = DeepSVDD_best_estimator.labels_ \n",
    "DeepSVDD_y_train_scores = DeepSVDD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "DeepSVDD_y_test_pred = DeepSVDD_best_estimator.predict(y_train_preprocessed)\n",
    "DeepSVDD_y_test_pred_proba = DeepSVDD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "DeepSVDD_y_test_scores = DeepSVDD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Deep One-Class Classification #####\n",
    "##### Model Pipeline for Empirical Cumulative Distribution #####\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "ECOD_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "ECOD_model = ECOD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "ECOD_grid_search = GridSearchCV(ECOD_model,\n",
    "                                      ECOD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "ECOD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ECOD_best_estimator = ECOD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "ECOD_y_train_pred = ECOD_best_estimator.labels_ \n",
    "ECOD_y_train_scores = ECOD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "ECOD_y_test_pred = ECOD_best_estimator.predict(y_train_preprocessed)\n",
    "ECOD_y_test_pred_proba = ECOD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "ECOD_y_test_scores = ECOD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Empirical Cumulative Distribution #####\n",
    "##### Model Pipeline for Gaussian Mixture Model #####\n",
    "\n",
    "from pyod.models.gmm import GMM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "GMM_param_grid = {\n",
    "\"covariance_type\": ['full'],\n",
    "}\n",
    "\n",
    "\n",
    "GMM_model = GMM()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "GMM_grid_search = GridSearchCV(GMM_model,\n",
    "                                      GMM_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "GMM_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "GMM_best_estimator = GMM_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "GMM_y_train_pred = GMM_best_estimator.labels_ \n",
    "GMM_y_train_scores = GMM_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "GMM_y_test_pred = GMM_best_estimator.predict(y_train_preprocessed)\n",
    "GMM_y_test_pred_proba = GMM_best_estimator.predict_proba(y_train_preprocessed) \n",
    "GMM_y_test_scores = GMM_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Gaussian Mixture Model #####\n",
    "##### Model Pipeline for Principal Component Analysis #####\n",
    "\n",
    "from pyod.models.pca import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "PCA_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"svd_solver\": ['auto'],\n",
    "}\n",
    "\n",
    "\n",
    "PCA_model = PCA()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "PCA_grid_search = GridSearchCV(PCA_model,\n",
    "                                      PCA_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "PCA_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "PCA_best_estimator = PCA_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "PCA_y_train_pred = PCA_best_estimator.labels_ \n",
    "PCA_y_train_scores = PCA_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "PCA_y_test_pred = PCA_best_estimator.predict(y_train_preprocessed)\n",
    "PCA_y_test_pred_proba = PCA_best_estimator.predict_proba(y_train_preprocessed) \n",
    "PCA_y_test_scores = PCA_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Principal Component Analysis #####\n",
    "##### Model Pipeline for R Graph #####\n",
    "\n",
    "from pyod.models.rgraph import RGraph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "RGraph_param_grid = {\n",
    "\"maxiter_lasso\": np.arange(100, 1000, 1000),\n",
    "\"transition_steps\": np.arange(10, 20, 15),\n",
    "\"verbose\": [1],\n",
    "}\n",
    "\n",
    "\n",
    "RGraph_model = RGraph()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "RGraph_grid_search = GridSearchCV(RGraph_model,\n",
    "                                      RGraph_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "RGraph_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "RGraph_best_estimator = RGraph_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "RGraph_y_train_pred = RGraph_best_estimator.labels_ \n",
    "RGraph_y_train_scores = RGraph_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "RGraph_y_test_pred = RGraph_best_estimator.predict(y_train_preprocessed)\n",
    "RGraph_y_test_pred_proba = RGraph_best_estimator.predict_proba(y_train_preprocessed) \n",
    "RGraph_y_test_scores = RGraph_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for R Graph #####\n",
    "##### Model Pipeline for Outlier detection based on Sampling #####\n",
    "\n",
    "from pyod.models.sampling import Sampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "Sampling_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"subset_size\": np.arange(10, 20, 15),\n",
    "\"metric\": ['minkowski'],\n",
    "}\n",
    "\n",
    "\n",
    "Sampling_model = Sampling()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "Sampling_grid_search = GridSearchCV(Sampling_model,\n",
    "                                      Sampling_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "Sampling_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "Sampling_best_estimator = Sampling_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "Sampling_y_train_pred = Sampling_best_estimator.labels_ \n",
    "Sampling_y_train_scores = Sampling_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "Sampling_y_test_pred = Sampling_best_estimator.predict(y_train_preprocessed)\n",
    "Sampling_y_test_pred_proba = Sampling_best_estimator.predict_proba(y_train_preprocessed) \n",
    "Sampling_y_test_scores = Sampling_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Outlier detection based on Sampling #####\n",
    "##### Model Pipeline for SUOD #####\n",
    "\n",
    "from pyod.models.suod import SUOD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "SUOD_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"target_dim_frac\": np.arange(0.0, 1.0, 0.5),\n",
    "\"jl_method\": ['basic'],\n",
    "}\n",
    "\n",
    "\n",
    "SUOD_model = SUOD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "SUOD_grid_search = GridSearchCV(SUOD_model,\n",
    "                                      SUOD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "SUOD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "SUOD_best_estimator = SUOD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "SUOD_y_train_pred = SUOD_best_estimator.labels_ \n",
    "SUOD_y_train_scores = SUOD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "SUOD_y_test_pred = SUOD_best_estimator.predict(y_train_preprocessed)\n",
    "SUOD_y_test_pred_proba = SUOD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "SUOD_y_test_scores = SUOD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for SUOD #####\n",
    "##### Model Pipeline for Rotation-based Outlier Detector #####\n",
    "\n",
    "from pyod.models.rod import ROD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "ROD_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "ROD_model = ROD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "ROD_grid_search = GridSearchCV(ROD_model,\n",
    "                                      ROD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "ROD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "ROD_best_estimator = ROD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "ROD_y_train_pred = ROD_best_estimator.labels_ \n",
    "ROD_y_train_scores = ROD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "ROD_y_test_pred = ROD_best_estimator.predict(y_train_preprocessed)\n",
    "ROD_y_test_pred_proba = ROD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "ROD_y_test_scores = ROD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Rotation-based Outlier Detector #####\n",
    "##### Model Pipeline for Stochastic Outlier Selection #####\n",
    "\n",
    "from pyod.models.sos import SOS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "SOS_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"metric\": ['euclidean'],\n",
    "}\n",
    "\n",
    "\n",
    "SOS_model = SOS()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "SOS_grid_search = GridSearchCV(SOS_model,\n",
    "                                      SOS_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "SOS_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "SOS_best_estimator = SOS_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "SOS_y_train_pred = SOS_best_estimator.labels_ \n",
    "SOS_y_train_scores = SOS_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "SOS_y_test_pred = SOS_best_estimator.predict(y_train_preprocessed)\n",
    "SOS_y_test_pred_proba = SOS_best_estimator.predict_proba(y_train_preprocessed) \n",
    "SOS_y_test_scores = SOS_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Stochastic Outlier Selection #####\n",
    "##### Model Pipeline for Quasi-Monte Carlo Discrepancy outlier detection #####\n",
    "\n",
    "from pyod.models.qmcd import QMCD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "QMCD_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "}\n",
    "\n",
    "\n",
    "QMCD_model = QMCD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "QMCD_grid_search = GridSearchCV(QMCD_model,\n",
    "                                      QMCD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "QMCD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "QMCD_best_estimator = QMCD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "QMCD_y_train_pred = QMCD_best_estimator.labels_ \n",
    "QMCD_y_train_scores = QMCD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "QMCD_y_test_pred = QMCD_best_estimator.predict(y_train_preprocessed)\n",
    "QMCD_y_test_pred_proba = QMCD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "QMCD_y_test_scores = QMCD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Quasi-Monte Carlo Discrepancy outlier detection #####\n",
    "##### Model Pipeline for Single-Objective Generative Adversarial Active Learning #####\n",
    "\n",
    "from pyod.models.so_gaal import SO_GAAL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "SO_GAAL_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"stop_epochs\": np.arange(10, 20, 15),\n",
    "}\n",
    "\n",
    "\n",
    "SO_GAAL_model = SO_GAAL()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "SO_GAAL_grid_search = GridSearchCV(SO_GAAL_model,\n",
    "                                      SO_GAAL_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "SO_GAAL_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "SO_GAAL_best_estimator = SO_GAAL_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "SO_GAAL_y_train_pred = SO_GAAL_best_estimator.labels_ \n",
    "SO_GAAL_y_train_scores = SO_GAAL_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "SO_GAAL_y_test_pred = SO_GAAL_best_estimator.predict(y_train_preprocessed)\n",
    "SO_GAAL_y_test_pred_proba = SO_GAAL_best_estimator.predict_proba(y_train_preprocessed) \n",
    "SO_GAAL_y_test_scores = SO_GAAL_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for Single-Objective Generative Adversarial Active Learning #####\n",
    "##### Model Pipeline for One-class SVM detector #####\n",
    "\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "OCSVM_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"kernel\": ['rbf'],\n",
    "}\n",
    "\n",
    "\n",
    "OCSVM_model = OCSVM()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "OCSVM_grid_search = GridSearchCV(OCSVM_model,\n",
    "                                      OCSVM_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "OCSVM_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "OCSVM_best_estimator = OCSVM_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "OCSVM_y_train_pred = OCSVM_best_estimator.labels_ \n",
    "OCSVM_y_train_scores = OCSVM_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "OCSVM_y_test_pred = OCSVM_best_estimator.predict(y_train_preprocessed)\n",
    "OCSVM_y_test_pred_proba = OCSVM_best_estimator.predict_proba(y_train_preprocessed) \n",
    "OCSVM_y_test_scores = OCSVM_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for One-class SVM detector #####\n",
    "##### Model Pipeline for MO_GAAL #####\n",
    "\n",
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "MO_GAAL_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "}\n",
    "\n",
    "\n",
    "MO_GAAL_model = MO_GAAL()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "MO_GAAL_grid_search = GridSearchCV(MO_GAAL_model,\n",
    "                                      MO_GAAL_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "MO_GAAL_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "MO_GAAL_best_estimator = MO_GAAL_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "MO_GAAL_y_train_pred = MO_GAAL_best_estimator.labels_ \n",
    "MO_GAAL_y_train_scores = MO_GAAL_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "MO_GAAL_y_test_pred = MO_GAAL_best_estimator.predict(y_train_preprocessed)\n",
    "MO_GAAL_y_test_pred_proba = MO_GAAL_best_estimator.predict_proba(y_train_preprocessed) \n",
    "MO_GAAL_y_test_scores = MO_GAAL_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for MO_GAAL #####\n",
    "##### Model Pipeline for MCD #####\n",
    "\n",
    "from pyod.models.mcd import MCD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "MCD_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"store_precision\": [True],\n",
    "}\n",
    "\n",
    "\n",
    "MCD_model = MCD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "MCD_grid_search = GridSearchCV(MCD_model,\n",
    "                                      MCD_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "MCD_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "MCD_best_estimator = MCD_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "MCD_y_train_pred = MCD_best_estimator.labels_ \n",
    "MCD_y_train_scores = MCD_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "MCD_y_test_pred = MCD_best_estimator.predict(y_train_preprocessed)\n",
    "MCD_y_test_pred_proba = MCD_best_estimator.predict_proba(y_train_preprocessed) \n",
    "MCD_y_test_scores = MCD_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for MCD #####\n",
    "##### Model Pipeline for LUNAR #####\n",
    "\n",
    "from pyod.models.lunar import LUNAR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "LUNAR_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "LUNAR_model = LUNAR()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "LUNAR_grid_search = GridSearchCV(LUNAR_model,\n",
    "                                      LUNAR_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "LUNAR_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "LUNAR_best_estimator = LUNAR_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "LUNAR_y_train_pred = LUNAR_best_estimator.labels_ \n",
    "LUNAR_y_train_scores = LUNAR_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "LUNAR_y_test_pred = LUNAR_best_estimator.predict(y_train_preprocessed)\n",
    "LUNAR_y_test_pred_proba = LUNAR_best_estimator.predict_proba(y_train_preprocessed) \n",
    "LUNAR_y_test_scores = LUNAR_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for LUNAR #####\n",
    "##### Model Pipeline for LOF #####\n",
    "\n",
    "from pyod.models.lof import LOF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "LOF_param_grid = {\n",
    "\"contamination\": np.arange(0.1, 0.5, 0.05),\n",
    "\"algorithm\": ['auto'],\n",
    "}\n",
    "\n",
    "\n",
    "LOF_model = LOF()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "LOF_grid_search = GridSearchCV(LOF_model,\n",
    "                                      LOF_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "LOF_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "LOF_best_estimator = LOF_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "LOF_y_train_pred = LOF_best_estimator.labels_ \n",
    "LOF_y_train_scores = LOF_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "LOF_y_test_pred = LOF_best_estimator.predict(y_train_preprocessed)\n",
    "LOF_y_test_pred_proba = LOF_best_estimator.predict_proba(y_train_preprocessed) \n",
    "LOF_y_test_scores = LOF_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for LOF #####\n",
    "##### Model Pipeline for LODA #####\n",
    "\n",
    "from pyod.models.loda import LODA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scorer_f(estimator, X_test):   #dummy scorer for unsupervised model gridsearch\n",
    "      return 1\n",
    "\n",
    "\n",
    "LODA_param_grid = {\n",
    "}\n",
    "\n",
    "\n",
    "LODA_model = LODA()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "LODA_grid_search = GridSearchCV(LODA_model,\n",
    "                                      LODA_param_grid, \n",
    "                                      scoring=scorer_f, \n",
    "                                      verbose=3)\n",
    "LODA_grid_search.fit(x_train_preprocessed)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "LODA_best_estimator = LODA_grid_search.best_estimator_\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "LODA_y_train_pred = LODA_best_estimator.labels_ \n",
    "LODA_y_train_scores = LODA_best_estimator.decision_scores_\n",
    "\n",
    "# get the prediction on the test data\n",
    "LODA_y_test_pred = LODA_best_estimator.predict(y_train_preprocessed)\n",
    "LODA_y_test_pred_proba = LODA_best_estimator.predict_proba(y_train_preprocessed) \n",
    "LODA_y_test_scores = LODA_best_estimator.decision_function(y_train_preprocessed) \n",
    "##### End of Model Pipeline for LODA #####\n",
    "##### Model Comparison #####\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canvas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
