{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'train.csv', 'target_column': 'target', 'model_type': 'classification', 'cross_validation': 5, 'selected_models': ['Logistic Regression'], 'completed_models': [], 'classification': {'Logistic Regression': {'numerical': [{'checked': True, 'name': 'C', 'min': 0.1, 'max': 1, 'step': 0.1}], 'categorical': [{'checked': True, 'name': 'penalty', 'selected': ['l2'], 'values': ['l2', 'elasticnet', 'none']}]}, 'Random Forest': {'numerical': [{'checked': True, 'name': 'n_estimators', 'min': 10, 'max': 100, 'step': 20}, {'checked': True, 'name': 'max_depth', 'min': 2, 'max': 10, 'step': 2}, {'checked': True, 'name': 'min_samples_split', 'min': 0.5, 'max': 1, 'step': 0.1}, {'checked': True, 'name': 'min_samples_leaf', 'min': 1, 'max': 10, 'step': 2}], 'categorical': [{'checked': True, 'name': 'criterion', 'selected': ['gini'], 'values': ['gini', 'entropy']}, {'checked': True, 'name': 'max_features', 'selected': ['auto'], 'values': ['auto', 'sqrt', 'log2']}, {'checked': True, 'name': 'bootstrap', 'selected': [True], 'values': [True, False]}, {'checked': False, 'name': 'oob_score', 'selected': [], 'values': [True, False]}, {'checked': False, 'name': 'warm_start', 'selected': [], 'values': [True, False]}, {'checked': False, 'name': 'class_weight', 'selected': [], 'values': ['balanced', 'balanced_subsample']}]}, 'SVC': {'numerical': [{'checked': True, 'name': 'C', 'min': 0.1, 'max': 1, 'step': 0.1}, {'checked': True, 'name': 'degree', 'min': 2, 'max': 5, 'step': 1}], 'categorical': [{'checked': True, 'name': 'kernel', 'selected': ['linear', 'poly'], 'values': ['linear', 'rbf', 'sigmoid']}, {'checked': False, 'name': 'gamma', 'selected': [], 'values': ['scale', 'auto']}]}, 'MLP': {'numerical': [{'checked': True, 'name': 'hidden_layer_sizes', 'min': 10, 'max': 100, 'step': 10}, {'checked': True, 'name': 'batch_size', 'min': 64, 'max': 512, 'step': 128}, {'checked': True, 'name': 'learning_rate', 'min': 0.001, 'max': 0.1, 'step': 0.05}], 'categorical': [{'checked': True, 'name': 'learning_rate_init', 'selected': ['constant', 'adaptive'], 'value': ['constant', 'invscaling', 'adaptive']}, {'checked': True, 'name': 'solver', 'selected': ['adam'], 'value': ['lbfgs', 'sgd', 'adam']}, {'checked': True, 'name': 'activation', 'selected': ['relu', 'tanh'], 'value': ['relu', 'identity', 'logistic', 'tanh']}, {'checked': True, 'name': 'shuffle', 'selected': [True], 'value': [True, False]}]}, 'Ridge Classifier': {'numerical': [{'checked': True, 'name': 'alpha', 'min': 10, 'max': 100, 'step': 10}, {'checked': True, 'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}], 'categorical': [{'checked': True, 'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}, {'checked': True, 'name': 'positive', 'selected': [True], 'values': [True, False]}]}}, 'regression': {'Elastic Net': {'numerical': [{'checked': True, 'name': 'alpha', 'min': 0.1, 'max': 1, 'step': 0.5}, {'checked': True, 'name': 'l1_ratio', 'min': 0.0, 'max': 1.0, 'step': 0.1}, {'checked': True, 'name': 'max_iter', 'min': 500, 'max': 1000, 'step': 100}], 'categorical': [{'checked': True, 'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}, {'checked': True, 'name': 'precompute', 'selected': [False], 'values': [True, False]}, {'checked': True, 'name': 'selection', 'selected': ['cyclic'], 'values': ['cyclic', 'random']}]}, 'Linear Regression': {'numerical': [{'checked': True, 'name': 'n_jobs', 'min': 1, 'max': 10, 'step': 1}], 'categorical': [{'checked': True, 'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}, {'checked': False, 'name': 'normalize', 'selected': [True], 'values': [True, False]}]}, 'Lasso': {'numerical': [{'checked': True, 'name': 'alpha', 'min': 10, 'max': 100, 'step': 10}, {'checked': True, 'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}], 'categorical': [{'checked': True, 'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}, {'checked': True, 'name': 'precompute', 'selected': [False], 'values': [True, False]}, {'checked': True, 'name': 'positive', 'selected': [True], 'values': [True, False]}, {'checked': True, 'name': 'selection', 'selected': ['cyclic'], 'values': ['cyclic', 'random']}]}, 'Ridge': {'numerical': [{'checked': True, 'name': 'alpha', 'min': 10, 'max': 100, 'step': 10}, {'checked': True, 'name': 'max_iter', 'min': 100, 'max': 1000, 'step': 100}], 'categorical': [{'checked': True, 'name': 'fit_intercept', 'selected': [True], 'values': [True, False]}, {'checked': True, 'name': 'positive', 'selected': [True], 'values': [True, False]}]}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from pypelines.sklearn_pypeline import SklearnPipeline\n",
    "from default_content import default_content\n",
    "\n",
    "config = default_content = default_content(5)\n",
    "\n",
    "config = json.loads(config) \n",
    "\n",
    "#settings = SklearnPipeline().get_settings(config)\n",
    "\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'prefix': 'log_reg', 'model': 'LogisticRegression()', 'imports': 'from sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\\nimport plotly.express as px', 'model_type': 'Classification'}\n",
      "{'prefix': 'log_reg', 'model': 'LogisticRegression()', 'imports': 'from sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\\nimport plotly.express as px', 'model_type': 'Classification'}\n",
      "{'prefix': 'log_reg', 'model': 'LogisticRegression()', 'imports': 'from sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\\nimport plotly.express as px', 'model_type': 'Classification'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blocks, edges, requirements, imports  = SklearnPipeline().run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '\\n\\n# target dataframe: train.csv\\ntarget = \"target\"\\nfeatures = list(train.csv.columns.drop(\"target\"))\\nfeature_df = train.csv[features]\\n\\n# get numerical and categorical columns\\nbool_cols = feature_df.select_dtypes(include=[\\'bool\\']).columns.tolist()\\ntrain.csv[bool_cols] = feature_df[bool_cols].astype(int)\\nnumerical_cols = feature_df.select_dtypes(include=[\\'int\\', \\'float\\']).columns.tolist()\\ncategorical_cols = feature_df.select_dtypes(include=[\\'object\\']).columns.tolist()\\ntext_cols = feature_df.select_dtypes(include=[\\'string\\']).columns.tolist()\\n# check categorical columns for high cardinality\\n\\nsample_size = np.min([10000, train.csv.shape[0]])\\nunique_theshold = np.min([100, sample_size/10])\\n\\n# check categorical columns for high cardinality\\nfor col in categorical_cols:\\n    if train.csv[col].sample(sample_size).nunique() > unique_theshold:\\n        categorical_cols.remove(col)\\n\\n# check text columns for low cardinality\\nfor col in text_cols:\\n    if train.csv[col].sample(sample_size).nunique() < unique_theshold:\\n        categorical_cols.append(col)\\n        text_cols.remove(col)\\n\\n# define numeric transformer steps\\nnumeric_transformer = Pipeline(\\n    steps=[\\n        (\"imputer\", SimpleImputer(strategy=\"median\")), \\n        (\"scaler\", StandardScaler())]\\n)\\n\\n# define categorical transformer steps\\ncategorical_transformer = Pipeline(\\n    steps=[\\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\\n    ]\\n)\\n\\n# define text transformer steps\\ntext_transformer = Pipeline(\\n    steps=[\\n        (\\'text\\', TfidfVectorizer())\\n    ]\\n)\\n\\n# create the preprocessing pipelines for both numeric and categorical data\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'num\\', numeric_transformer , numerical_cols),\\n        (\\'cat\\', categorical_transformer, categorical_cols),\\n        (\\'text\\', text_transformer, text_cols),\\n    ]\\n)\\n\\n# train test split\\nX = train.csv[features]\\ny = train.csv[target]\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)',\n",
       "  'id': 1},\n",
       " {'content': '\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\\nimport plotly.express as px\\nlog_reg_param_grid = {\\n\"log_reg__C\": np.arange(0.1, 1.0, 0.1),\\n\"log_reg__penalty\": [\\'l2\\'],\\n}\\n\\n\\n# Create the pipeline\\nlog_reg_pipe = Pipeline([\\n    (\\'preprocessor\\', preprocessor),\\n    (\\'log_reg\\', LogisticRegression())\\n])\\n\\n# Create the grid search\\nlog_reg_grid_search = GridSearchCV(estimator=log_reg_pipe, param_grid=log_reg_param_grid, cv=5, scoring=make_scorer(accuracy_score), verbose=1)\\nlog_reg_grid_search.fit(X_train, y_train)\\n\\n# Get the best hyperparameters\\nlog_reg_best_estimator = log_reg_grid_search.best_estimator_\\n\\n# Store results as a dataframe  \\nlog_reg_search_results = pd.DataFrame(log_reg_grid_search.cv_results_)\\n\\n',\n",
       "  'id': 2},\n",
       " {'content': '# Model metrics\\n\\nlog_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\\nlog_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\\nlog_reg_predictions_prob_df = pd.DataFrame()\\nlog_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\\nlog_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \\nlog_reg_accuracy = accuracy_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\\nlog_reg_f1_score = f1_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\\nlog_reg_precision = precision_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\\nlog_reg_recall = recall_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\\nlog_reg_roc_auc_score = roc_auc_score(y_test.iloc[:,0], log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\\nlog_reg_performance_metrics = [[\\'log_reg\\',\\'accuracy\\',log_reg_accuracy], \\n                                  [\\'log_reg\\',\\'f1_score\\',log_reg_f1_score],\\n                                  [\\'log_reg\\',\\'precision\\', log_reg_precision],\\n                                  [\\'log_reg\\',\\'recall\\', log_reg_recall],\\n                                  [\\'log_reg\\',\\'roc_auc_score\\', log_reg_roc_auc_score]]\\nlog_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=[\\'model\\',\\'metric\\', \\'value\\'])\\nfpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\\nlog_reg_roc_auc_plot = px.area(\\n    x=fpr, y=tpr,\\n    title=f\\'ROC Curve (AUC={auc(fpr, tpr):.4f})\\',\\n    labels=dict(x=\\'False Positive Rate\\', y=\\'True Positive Rate\\'),\\n    width=700, height=500\\n)\\nlog_reg_roc_auc_plot.add_shape(\\n    type=\\'line\\', line=dict(dash=\\'dash\\'),\\n    x0=0, x1=1, y0=0, y1=1\\n)\\nlog_reg_roc_auc_plot.update_yaxes(scaleanchor=\"x\", scaleratio=1)\\nlog_reg_roc_auc_plot.update_xaxes(constrain=\\'domain\\')\\nlog_reg_roc_auc_plot.show()\\ndel df, target, features, feature_df, bool_cols, numerical_cols, categorical_cols, text_cols, col, numeric_transformer, categorical_transformer,text_transformer, preprocessor,X, X_train, X_test, y, y_train, y_test,accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\\n\\n\\n',\n",
       "  'id': 3},\n",
       " {'content': '\\n#comparison metrics - log_reg\\n\\nlog_reg_performance_metrics\\nlog_reg_roc_auc_plot.show()\\n',\n",
       "  'id': 4}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model metrics\n",
      "\n",
      "log_reg_predictions = pd.DataFrame(log_reg_best_estimator.predict(X_test))\n",
      "log_reg_predictions_prob = log_reg_best_estimator.predict_proba(X_test)\n",
      "log_reg_predictions_prob_df = pd.DataFrame()\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[0]] = log_reg_predictions_prob[:,0]\n",
      "log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]] = log_reg_predictions_prob[:,1] \n",
      "log_reg_accuracy = accuracy_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\n",
      "log_reg_f1_score = f1_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\n",
      "log_reg_precision = precision_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\n",
      "log_reg_recall = recall_score(y_test.iloc[:,0], log_reg_predictions.iloc[:,0])\n",
      "log_reg_roc_auc_score = roc_auc_score(y_test.iloc[:,0], log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "log_reg_performance_metrics = [['log_reg','accuracy',log_reg_accuracy], \n",
      "                                  ['log_reg','f1_score',log_reg_f1_score],\n",
      "                                  ['log_reg','precision', log_reg_precision],\n",
      "                                  ['log_reg','recall', log_reg_recall],\n",
      "                                  ['log_reg','roc_auc_score', log_reg_roc_auc_score]]\n",
      "log_reg_performance_metrics = pd.DataFrame(log_reg_performance_metrics, columns=['model','metric', 'value'])\n",
      "fpr, tpr, thresholds = roc_curve(y_test, log_reg_predictions_prob_df[log_reg_grid_search.classes_[1]])\n",
      "log_reg_roc_auc_plot = px.area(\n",
      "    x=fpr, y=tpr,\n",
      "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
      "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
      "    width=700, height=500\n",
      ")\n",
      "log_reg_roc_auc_plot.add_shape(\n",
      "    type='line', line=dict(dash='dash'),\n",
      "    x0=0, x1=1, y0=0, y1=1\n",
      ")\n",
      "log_reg_roc_auc_plot.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
      "log_reg_roc_auc_plot.update_xaxes(constrain='domain')\n",
      "log_reg_roc_auc_plot.show()\n",
      "del df, target, features, feature_df, bool_cols, numerical_cols, categorical_cols, text_cols, col, numeric_transformer, categorical_transformer,text_transformer, preprocessor,X, X_train, X_test, y, y_train, y_test,accuracy_score,make_scorer,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(blocks[2]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
